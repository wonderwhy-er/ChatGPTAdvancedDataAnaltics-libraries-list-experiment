[{"name": "oscrypto", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noscrypto\nSupported Operating Systems\nFeatures\nWhy Another Python Crypto Library?\nRelated Crypto Libraries\nCurrent Release\nDependencies\nInstallation\nLicense\nDocumentation\nContinuous Integration\nTesting\nGit Repository\nBackend Options\nInternet Tests\nPyPi Source Distribution\nTest Options\nForce OpenSSL Shared Library Paths\nForce Use of ctypes\nForce Use of Legacy Windows Crypto APIs\nSkip Tests Requiring an Internet Connection\nPackage\nDevelopment\nCI Tasks\n\n\n\n\n\nreadme.md\n\n\n\n\noscrypto\nA compilation-free, always up-to-date encryption library for Python that works\non Windows, OS X, Linux and BSD. Supports the following versions of Python:\n2.6, 2.7, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 3.10 and pypy.\n\nSupported Operating Systems\nFeatures\nWhy Another Python Crypto Library?\nRelated Crypto Libraries\nCurrent Release\nDependencies\nInstallation\nLicense\nDocumentation\nContinuous Integration\nTesting\nDevelopment\nCI Tasks\n\n\n\n\nSupported Operating Systems\nThe library integrates with the encryption library that is part of the operating\nsystem. This means that a compiler is never needed, and OS security updates take\ncare of patching vulnerabilities. Supported operating systems include:\n\nWindows XP or newer\n\nUses:\n\nCryptography API: Next Generation (CNG)\nSecure Channel for TLS\nCryptoAPI for trust lists and XP support\n\n\nTested on:\n\nWindows XP (no SNI)\nWindows 7\nWindows 8.1\nWindows Server 2012\nWindows 10\n\n\n\n\nOS X 10.7 or newer\n\nUses:\n\nSecurity.framework\nSecure Transport for TLS\nCommonCrypto for PBKDF2\nOpenSSL (or LibreSSL on macOS 10.13) for the PKCS #12 KDF\n\n\nTested on:\n\nOS X 10.7\nOS X 10.8\nOS X 10.9\nOS X 10.10\nOS X 10.11\nOS X 10.11 with OpenSSL 1.1.0\nmacOS 10.12\nmacOS 10.13 with LibreSSL 2.2.7\nmacOS 10.14\nmacOS 10.15\nmacOS 10.15 with OpenSSL 3.0\nmacOS 11\nmacOS 12\n\n\n\n\nLinux or BSD\n\nUses one of:\n\nOpenSSL 0.9.8\nOpenSSL 1.0.x\nOpenSSL 1.1.0\nOpenSSL 3.0\nLibreSSL\n\n\nTested on:\n\nArch Linux with OpenSSL 1.0.2\nOpenBSD 5.7 with LibreSSL\nUbuntu 10.04 with OpenSSL 0.9.8\nUbuntu 12.04 with OpenSSL 1.0.1\nUbuntu 15.04 with OpenSSL 1.0.1\nUbuntu 16.04 with OpenSSL 1.0.2 on Raspberry Pi 3 (armhf)\nUbuntu 18.04 with OpenSSL 1.1.x (amd64, arm64, ppc64el)\nUbuntu 22.04 with OpenSSL 3.0 (amd64)\n\n\n\n\n\nOS X 10.6 will not be supported due to a lack of available\ncryptographic primitives and due to lack of vendor support.\nFeatures\nCurrently the following features are implemented. Many of these should only be\nused for integration with existing/legacy systems. If you don't know which you\nshould, or should not use, please see Learning.\n\nTLSv1.x socket wrappers\n\nCertificate verification performed by OS trust roots\nCustom CA certificate support\nSNI support (except Windows XP)\nSession reuse via IDs/tickets\nModern cipher suites (RC4, DES, anon and NULL ciphers disabled)\nWeak DH parameters and certificate signatures rejected\nSSLv3 disabled by default, SSLv2 unimplemented\nCRL/OCSP revocation checks consistenty disabled\n\n\nExporting OS trust roots\n\nPEM-formatted CA certs from the OS for OpenSSL-based code\n\n\nEncryption/decryption\n\nAES (128, 192, 256), CBC mode, PKCS7 padding\nAES (128, 192, 256), CBC mode, no padding\nTripleDES 3-key, CBC mode, PKCS5 padding\nTripleDes 2-key, CBC mode, PKCS5 padding\nDES, CBC mode, PKCS5 padding\nRC2 (40-128), CBC mode, PKCS5 padding\nRC4 (40-128)\nRSA PKCSv1.5\nRSA OAEP (SHA1 only)\n\n\nGenerating public/private key pairs\n\nRSA (1024, 2048, 3072, 4096 bit)\nDSA (1024 bit on all platforms - 2048, 3072 bit with OpenSSL 1.x or\nWindows 8)\nEC (secp256r1, secp384r1, secp521r1 curves)\n\n\nGenerating DH parameters\nSigning and verification\n\nRSA PKCSv1.5\nRSA PSS\nDSA\nEC\n\n\nLoading and normalizing DER and PEM formatted keys\n\nRSA public and private keys\nDSA public and private keys\nEC public and private keys\nX.509 Certificates\nPKCS#12 archives (.pfx/.p12)\n\n\nKey derivation\n\nPBKDF2\nPBKDF1\nPKCS#12 KDF\n\n\nRandom byte generation\n\nThe feature set was largely driven by the technologies used related to\ngenerating and validating X.509 certificates. The various CBC encryption schemes\nand KDFs are used to load encrypted private keys, and the various RSA padding\nschemes are part of X.509 signatures.\nFor modern cryptography not tied to an existing system, please see the\nModern Cryptography section of the docs.\nPlease note that this library does not include modern block modes such as CTR\nand GCM due to lack of support from both OS X and OpenSSL 0.9.8.\nWhy Another Python Crypto Library?\nIn short, the existing cryptography libraries for Python didn't fit the needs of\na couple of projects I was working on. Primarily these are applications\ndistributed to end-users who aren't programmers, that need to handle TLS and\nvarious technologies related to X.509 certificates.\nIf your system is not tied to AES, TLS, X.509, or related technologies, you\nprobably want more modern cryptography.\nDepending on your needs, the cryptography package may\nbe a good (or better) fit.\nSome things that make oscrypto unique:\n\nNo compiler needed, ever. No need to pre-compile shared libraries. Just\ndistribute the Python source files, any way you want.\nUses the operating system's crypto library - does not require OpenSSL on\nWindows or OS X.\nRelies on the operating system for security patching. You don't need to\nrebuild all of your apps every time there is a new TLS vulnerability.\nIntentionally limited in scope to crypto primitives. Other libraries\nbuilt upon it deal with certificate path validation, creating certificates\nand CSRs, constructing CMS structures.\nBuilt on top of a fast, pure-Python ASN.1 parser,\nasn1crypto.\nTLS functionality uses the operating system's trust list/CA certs and is\npre-configured with sane defaults\nPublic APIs are simple and use strict type checks to avoid errors\n\nSome downsides include:\n\nDoes not currently implement:\n\nstandalone DH key exchange\nvarious encryption modes such as GCM, CCM, CTR, CFB, OFB, ECB\nkey wrapping\nCMAC\nHKDF\n\n\nNon-TLS functionality is architected for dealing with data that fits in\nmemory and is available all at once\nDeveloped by a single developer\n\nRelated Crypto Libraries\noscrypto is part of the modularcrypto family of Python packages:\n\nasn1crypto\noscrypto\ncsrbuilder\ncertbuilder\ncrlbuilder\nocspbuilder\ncertvalidator\n\nCurrent Release\n1.3.0 - changelog\nDependencies\n\nasn1crypto\nPython 2.6, 2.7, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 3.10, 3.11 or pypy\nOpenSSL/LibreSSL if on Linux\u00b9\n\n\u00b9 On Linux, ctypes.util.find_library() is used to located OpenSSL. Alpine Linux does not have an appropriate install by default for find_library() to work properly. Instead, oscrypto.use_openssl() must be called with the path to the OpenSSL shared libraries.\nInstallation\npip install oscrypto\nLicense\noscrypto is licensed under the terms of the MIT license. See the\nLICENSE file for the exact license text.\nDocumentation\noscrypto documentation\nContinuous Integration\nVarious combinations of platforms and versions of Python are tested via:\n\nmacOS, Linux, Windows via GitHub Actions\narm64 via CircleCI\n\nTesting\nTests are written using unittest and require no third-party packages.\nDepending on what type of source is available for the package, the following\ncommands can be used to run the test suite.\nGit Repository\nWhen working within a Git working copy, or an archive of the Git repository,\nthe full test suite is run via:\npython run.py tests\nTo run only some tests, pass a regular expression as a parameter to tests.\npython run.py tests aes\nTo run tests multiple times, in order to catch edge-case bugs, pass an integer\nto tests. If combined with a regular expression for filtering, pass the\nrepeat count after the regular expression.\npython run.py tests 20\npython run.py tests aes 20\nBackend Options\nTo run tests using a custom build of OpenSSL, or to use OpenSSL on Windows or\nMac, add use_openssl after run.py, like:\npython run.py use_openssl=/path/to/libcrypto.so,/path/to/libssl.so tests\nTo run tests forcing the use of ctypes, even if cffi is installed, add\nuse_ctypes after run.py:\npython run.py use_ctypes=true tests\nTo run tests using the legacy Windows crypto functions on Windows 7+, add\nuse_winlegacy after run.py:\npython run.py use_winlegacy=true tests\nInternet Tests\nTo skip tests that require an internet connection, add skip_internet after\nrun.py:\npython run.py skip_internet=true tests\nPyPi Source Distribution\nWhen working within an extracted source distribution (aka .tar.gz) from\nPyPi, the full test suite is run via:\npython setup.py test\nTest Options\nThe following env vars can control aspects of running tests:\nForce OpenSSL Shared Library Paths\nSetting the env var OSCRYPTO_USE_OPENSSL to a string in the form:\n/path/to/libcrypto.so,/path/to/libssl.so\n\nwill force use of specific OpenSSL shared libraries.\nThis also works on Mac and Windows to force use of OpenSSL instead of using\nnative crypto libraries.\nForce Use of ctypes\nBy default, oscrypto will use the cffi module for FFI if it is installed.\nTo use the slightly slower, but more widely-tested, ctypes FFI layer, set\nthe env var OSCRYPTO_USE_CTYPES=true.\nForce Use of Legacy Windows Crypto APIs\nOn Windows 7 and newer, oscrypto will use the CNG backend by default.\nTo force use of the older CryptoAPI, set the env var\nOSCRYPTO_USE_WINLEGACY=true.\nSkip Tests Requiring an Internet Connection\nSome of the TLS tests require an active internet connection to ensure that\nvarious \"bad\" server certificates are rejected.\nTo skip tests requiring an internet connection, set the env var\nOSCRYPTO_SKIP_INTERNET_TESTS=true.\nPackage\nWhen the package has been installed via pip (or another method), the package\noscrypto_tests may be installed and invoked to run the full test suite:\npip install oscrypto_tests\npython -m oscrypto_tests\nDevelopment\nTo install the package used for linting, execute:\npip install --user -r requires/lint\nThe following command will run the linter:\npython run.py lint\nSupport for code coverage can be installed via:\npip install --user -r requires/coverage\nCoverage is measured by running:\npython run.py coverage\nTo install the packages requires to generate the API documentation, run:\npip install --user -r requires/api_docs\nThe documentation can then be generated by running:\npython run.py api_docs\nTo install the necessary packages for releasing a new version on PyPI, run:\npip install --user -r requires/release\nReleases are created by:\n\n\nMaking a git tag in semver format\n\n\nRunning the command:\npython run.py release\n\n\nExisting releases can be found at https://pypi.python.org/pypi/oscrypto.\nCI Tasks\nA task named deps exists to download and stage all necessary testing\ndependencies. On posix platforms, curl is used for downloads and on Windows\nPowerShell with Net.WebClient is used. This configuration sidesteps issues\nrelated to getting pip to work properly and messing with site-packages for\nthe version of Python being used.\nThe ci task runs lint (if flake8 is available for the version of Python) and\ncoverage (or tests if coverage is not available for the version of Python).\nIf the current directory is a clean git working copy, the coverage data is\nsubmitted to codecov.io.\npython run.py deps\npython run.py ci\n\n\n"}, {"name": "orjson", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norjson\nUsage\nInstall\nQuickstart\nMigrating\nSerialize\ndefault\noption\nOPT_APPEND_NEWLINE\nOPT_INDENT_2\nOPT_NAIVE_UTC\nOPT_NON_STR_KEYS\nOPT_OMIT_MICROSECONDS\nOPT_PASSTHROUGH_DATACLASS\nOPT_PASSTHROUGH_DATETIME\nOPT_PASSTHROUGH_SUBCLASS\nOPT_SERIALIZE_DATACLASS\nOPT_SERIALIZE_NUMPY\nOPT_SERIALIZE_UUID\nOPT_SORT_KEYS\nOPT_STRICT_INTEGER\nOPT_UTC_Z\nFragment\nDeserialize\nTypes\ndataclass\ndatetime\nenum\nfloat\nint\nnumpy\nstr\nuuid\nTesting\nPerformance\nLatency\ntwitter.json serialization\ntwitter.json deserialization\ngithub.json serialization\ngithub.json deserialization\ncitm_catalog.json serialization\ncitm_catalog.json deserialization\ncanada.json serialization\ncanada.json deserialization\nMemory\ntwitter.json\ngithub.json\ncitm_catalog.json\ncanada.json\nReproducing\nQuestions\nWhy can't I install it from PyPI?\n\"Cargo, the Rust package manager, is not installed or is not on PATH.\"\nWill it deserialize to dataclasses, UUIDs, decimals, etc or support object_hook?\nWill it serialize to str?\nWill it support PyPy?\nPackaging\nLicense\n\n\n\n\n\nREADME.md\n\n\n\n\norjson\norjson is a fast, correct JSON library for Python. It\nbenchmarks as the fastest Python\nlibrary for JSON and is more correct than the standard json library or other\nthird-party libraries. It serializes\ndataclass,\ndatetime,\nnumpy, and\nUUID instances natively.\nIts features and drawbacks compared to other Python JSON libraries:\n\nserializes dataclass instances 40-50x as fast as other libraries\nserializes datetime, date, and time instances to RFC 3339 format,\ne.g., \"1970-01-01T00:00:00+00:00\"\nserializes numpy.ndarray instances 4-12x as fast with 0.3x the memory\nusage of other libraries\npretty prints 10x to 20x as fast as the standard library\nserializes to bytes rather than str, i.e., is not a drop-in replacement\nserializes str without escaping unicode to ASCII, e.g., \"\u597d\" rather than\n\"\\\\u597d\"\nserializes float 10x as fast and deserializes twice as fast as other\nlibraries\nserializes subclasses of str, int, list, and dict natively,\nrequiring default to specify how to serialize others\nserializes arbitrary types using a default hook\nhas strict UTF-8 conformance, more correct than the standard library\nhas strict JSON conformance in not supporting Nan/Infinity/-Infinity\nhas an option for strict JSON conformance on 53-bit integers with default\nsupport for 64-bit\ndoes not provide load() or dump() functions for reading from/writing to\nfile-like objects\n\norjson supports CPython 3.7, 3.8, 3.9, 3.10, 3.11, and 3.12. It distributes\namd64/x86_64, aarch64/armv8, arm7, POWER/ppc64le, and s390x wheels for Linux,\namd64 and aarch64 wheels for macOS, and amd64 and i686/x86 wheels for Windows.\norjson  does not support PyPy. Releases follow semantic versioning and\nserializing a new object type without an opt-in flag is considered a\nbreaking change.\norjson is licensed under both the Apache 2.0 and MIT licenses. The\nrepository and issue tracker is\ngithub.com/ijl/orjson, and patches may be\nsubmitted there. There is a\nCHANGELOG\navailable in the repository.\n\nUsage\n\nInstall\nQuickstart\nMigrating\nSerialize\n\ndefault\noption\nFragment\n\n\nDeserialize\n\n\nTypes\n\ndataclass\ndatetime\nenum\nfloat\nint\nnumpy\nstr\nuuid\n\n\nTesting\nPerformance\n\nLatency\nMemory\nReproducing\n\n\nQuestions\nPackaging\nLicense\n\nUsage\nInstall\nTo install a wheel from PyPI:\npip install --upgrade \"pip>=20.3\" # manylinux_x_y, universal2 wheel support\npip install --upgrade orjson\nTo build a wheel, see packaging.\nQuickstart\nThis is an example of serializing, with options specified, and deserializing:\n>>> import orjson, datetime, numpy\n>>> data = {\n    \"type\": \"job\",\n    \"created_at\": datetime.datetime(1970, 1, 1),\n    \"status\": \"\ud83c\udd97\",\n    \"payload\": numpy.array([[1, 2], [3, 4]]),\n}\n>>> orjson.dumps(data, option=orjson.OPT_NAIVE_UTC | orjson.OPT_SERIALIZE_NUMPY)\nb'{\"type\":\"job\",\"created_at\":\"1970-01-01T00:00:00+00:00\",\"status\":\"\\xf0\\x9f\\x86\\x97\",\"payload\":[[1,2],[3,4]]}'\n>>> orjson.loads(_)\n{'type': 'job', 'created_at': '1970-01-01T00:00:00+00:00', 'status': '\ud83c\udd97', 'payload': [[1, 2], [3, 4]]}\nMigrating\norjson version 3 serializes more types than version 2. Subclasses of str,\nint, dict, and list are now serialized. This is faster and more similar\nto the standard library. It can be disabled with\norjson.OPT_PASSTHROUGH_SUBCLASS.dataclasses.dataclass instances\nare now serialized by default and cannot be customized in a\ndefault function unless option=orjson.OPT_PASSTHROUGH_DATACLASS is\nspecified. uuid.UUID instances are serialized by default.\nFor any type that is now serialized,\nimplementations in a default function and options enabling them can be\nremoved but do not need to be. There was no change in deserialization.\nTo migrate from the standard library, the largest difference is that\norjson.dumps returns bytes and json.dumps returns a str. Users with\ndict objects using non-str keys should specify\noption=orjson.OPT_NON_STR_KEYS. sort_keys is replaced by\noption=orjson.OPT_SORT_KEYS. indent is replaced by\noption=orjson.OPT_INDENT_2 and other levels of indentation are not\nsupported.\nSerialize\ndef dumps(\n    __obj: Any,\n    default: Optional[Callable[[Any], Any]] = ...,\n    option: Optional[int] = ...,\n) -> bytes: ...\ndumps() serializes Python objects to JSON.\nIt natively serializes\nstr, dict, list, tuple, int, float, bool, None,\ndataclasses.dataclass, typing.TypedDict, datetime.datetime,\ndatetime.date, datetime.time, uuid.UUID, numpy.ndarray, and\norjson.Fragment instances. It supports arbitrary types through default. It\nserializes subclasses of str, int, dict, list,\ndataclasses.dataclass, and enum.Enum. It does not serialize subclasses\nof tuple to avoid serializing namedtuple objects as arrays. To avoid\nserializing subclasses, specify the option orjson.OPT_PASSTHROUGH_SUBCLASS.\nThe output is a bytes object containing UTF-8.\nThe global interpreter lock (GIL) is held for the duration of the call.\nIt raises JSONEncodeError on an unsupported type. This exception message\ndescribes the invalid object with the error message\nType is not JSON serializable: .... To fix this, specify\ndefault.\nIt raises JSONEncodeError on a str that contains invalid UTF-8.\nIt raises JSONEncodeError on an integer that exceeds 64 bits by default or,\nwith OPT_STRICT_INTEGER, 53 bits.\nIt raises JSONEncodeError if a dict has a key of a type other than str,\nunless OPT_NON_STR_KEYS is specified.\nIt raises JSONEncodeError if the output of default recurses to handling by\ndefault more than 254 levels deep.\nIt raises JSONEncodeError on circular references.\nIt raises JSONEncodeError  if a tzinfo on a datetime object is\nunsupported.\nJSONEncodeError is a subclass of TypeError. This is for compatibility\nwith the standard library.\nIf the failure was caused by an exception in default then\nJSONEncodeError chains the original exception as __cause__.\ndefault\nTo serialize a subclass or arbitrary types, specify default as a\ncallable that returns a supported type. default may be a function,\nlambda, or callable class instance. To specify that a type was not\nhandled by default, raise an exception such as TypeError.\n>>> import orjson, decimal\n>>>\ndef default(obj):\n    if isinstance(obj, decimal.Decimal):\n        return str(obj)\n    raise TypeError\n\n>>> orjson.dumps(decimal.Decimal(\"0.0842389659712649442845\"))\nJSONEncodeError: Type is not JSON serializable: decimal.Decimal\n>>> orjson.dumps(decimal.Decimal(\"0.0842389659712649442845\"), default=default)\nb'\"0.0842389659712649442845\"'\n>>> orjson.dumps({1, 2}, default=default)\norjson.JSONEncodeError: Type is not JSON serializable: set\nThe default callable may return an object that itself\nmust be handled by default up to 254 times before an exception\nis raised.\nIt is important that default raise an exception if a type cannot be handled.\nPython otherwise implicitly returns None, which appears to the caller\nlike a legitimate value and is serialized:\n>>> import orjson, json, rapidjson\n>>>\ndef default(obj):\n    if isinstance(obj, decimal.Decimal):\n        return str(obj)\n\n>>> orjson.dumps({\"set\":{1, 2}}, default=default)\nb'{\"set\":null}'\n>>> json.dumps({\"set\":{1, 2}}, default=default)\n'{\"set\":null}'\n>>> rapidjson.dumps({\"set\":{1, 2}}, default=default)\n'{\"set\":null}'\noption\nTo modify how data is serialized, specify option. Each option is an integer\nconstant in orjson. To specify multiple options, mask them together, e.g.,\noption=orjson.OPT_STRICT_INTEGER | orjson.OPT_NAIVE_UTC.\nOPT_APPEND_NEWLINE\nAppend \\n to the output. This is a convenience and optimization for the\npattern of dumps(...) + \"\\n\". bytes objects are immutable and this\npattern copies the original contents.\n>>> import orjson\n>>> orjson.dumps([])\nb\"[]\"\n>>> orjson.dumps([], option=orjson.OPT_APPEND_NEWLINE)\nb\"[]\\n\"\nOPT_INDENT_2\nPretty-print output with an indent of two spaces. This is equivalent to\nindent=2 in the standard library. Pretty printing is slower and the output\nlarger. orjson is the fastest compared library at pretty printing and has\nmuch less of a slowdown to pretty print than the standard library does. This\noption is compatible with all other options.\n>>> import orjson\n>>> orjson.dumps({\"a\": \"b\", \"c\": {\"d\": True}, \"e\": [1, 2]})\nb'{\"a\":\"b\",\"c\":{\"d\":true},\"e\":[1,2]}'\n>>> orjson.dumps(\n    {\"a\": \"b\", \"c\": {\"d\": True}, \"e\": [1, 2]},\n    option=orjson.OPT_INDENT_2\n)\nb'{\\n  \"a\": \"b\",\\n  \"c\": {\\n    \"d\": true\\n  },\\n  \"e\": [\\n    1,\\n    2\\n  ]\\n}'\nIf displayed, the indentation and linebreaks appear like this:\n{\n  \"a\": \"b\",\n  \"c\": {\n    \"d\": true\n  },\n  \"e\": [\n    1,\n    2\n  ]\n}\nThis measures serializing the github.json fixture as compact (52KiB) or\npretty (64KiB):\n\n\n\nLibrary\ncompact (ms)\npretty (ms)\nvs. orjson\n\n\n\n\norjson\n0.03\n0.04\n1\n\n\nujson\n0.18\n0.19\n4.6\n\n\nrapidjson\n0.1\n0.12\n2.9\n\n\nsimplejson\n0.25\n0.89\n21.4\n\n\njson\n0.18\n0.71\n17\n\n\n\nThis measures serializing the citm_catalog.json fixture, more of a worst\ncase due to the amount of nesting and newlines, as compact (489KiB) or\npretty (1.1MiB):\n\n\n\nLibrary\ncompact (ms)\npretty (ms)\nvs. orjson\n\n\n\n\norjson\n0.59\n0.71\n1\n\n\nujson\n2.9\n3.59\n5\n\n\nrapidjson\n1.81\n2.8\n3.9\n\n\nsimplejson\n10.43\n42.13\n59.1\n\n\njson\n4.16\n33.42\n46.9\n\n\n\nThis can be reproduced using the pyindent script.\nOPT_NAIVE_UTC\nSerialize datetime.datetime objects without a tzinfo as UTC. This\nhas no effect on datetime.datetime objects that have tzinfo set.\n>>> import orjson, datetime\n>>> orjson.dumps(\n        datetime.datetime(1970, 1, 1, 0, 0, 0),\n    )\nb'\"1970-01-01T00:00:00\"'\n>>> orjson.dumps(\n        datetime.datetime(1970, 1, 1, 0, 0, 0),\n        option=orjson.OPT_NAIVE_UTC,\n    )\nb'\"1970-01-01T00:00:00+00:00\"'\nOPT_NON_STR_KEYS\nSerialize dict keys of type other than str. This allows dict keys\nto be one of str, int, float, bool, None, datetime.datetime,\ndatetime.date, datetime.time, enum.Enum, and uuid.UUID. For comparison,\nthe standard library serializes str, int, float, bool or None by\ndefault. orjson benchmarks as being faster at serializing non-str keys\nthan other libraries. This option is slower for str keys than the default.\n>>> import orjson, datetime, uuid\n>>> orjson.dumps(\n        {uuid.UUID(\"7202d115-7ff3-4c81-a7c1-2a1f067b1ece\"): [1, 2, 3]},\n        option=orjson.OPT_NON_STR_KEYS,\n    )\nb'{\"7202d115-7ff3-4c81-a7c1-2a1f067b1ece\":[1,2,3]}'\n>>> orjson.dumps(\n        {datetime.datetime(1970, 1, 1, 0, 0, 0): [1, 2, 3]},\n        option=orjson.OPT_NON_STR_KEYS | orjson.OPT_NAIVE_UTC,\n    )\nb'{\"1970-01-01T00:00:00+00:00\":[1,2,3]}'\nThese types are generally serialized how they would be as\nvalues, e.g., datetime.datetime is still an RFC 3339 string and respects\noptions affecting it. The exception is that int serialization does not\nrespect OPT_STRICT_INTEGER.\nThis option has the risk of creating duplicate keys. This is because non-str\nobjects may serialize to the same str as an existing key, e.g.,\n{\"1\": true, 1: false}. The last key to be inserted to the dict will be\nserialized last and a JSON deserializer will presumably take the last\noccurrence of a key (in the above, false). The first value will be lost.\nThis option is compatible with orjson.OPT_SORT_KEYS. If sorting is used,\nnote the sort is unstable and will be unpredictable for duplicate keys.\n>>> import orjson, datetime\n>>> orjson.dumps(\n    {\"other\": 1, datetime.date(1970, 1, 5): 2, datetime.date(1970, 1, 3): 3},\n    option=orjson.OPT_NON_STR_KEYS | orjson.OPT_SORT_KEYS\n)\nb'{\"1970-01-03\":3,\"1970-01-05\":2,\"other\":1}'\nThis measures serializing 589KiB of JSON comprising a list of 100 dict\nin which each dict has both 365 randomly-sorted int keys representing epoch\ntimestamps as well as one str key and the value for each key is a\nsingle integer. In \"str keys\", the keys were converted to str before\nserialization, and orjson still specifes option=orjson.OPT_NON_STR_KEYS\n(which is always somewhat slower).\n\n\n\nLibrary\nstr keys (ms)\nint keys (ms)\nint keys sorted (ms)\n\n\n\n\norjson\n1.53\n2.16\n4.29\n\n\nujson\n3.07\n5.65\n\n\n\nrapidjson\n4.29\n\n\n\n\nsimplejson\n11.24\n14.50\n21.86\n\n\njson\n7.17\n8.49\n\n\n\n\nujson is blank for sorting because it segfaults. json is blank because it\nraises TypeError on attempting to sort before converting all keys to str.\nrapidjson is blank because it does not support non-str keys. This can\nbe reproduced using the pynonstr script.\nOPT_OMIT_MICROSECONDS\nDo not serialize the microsecond field on datetime.datetime and\ndatetime.time instances.\n>>> import orjson, datetime\n>>> orjson.dumps(\n        datetime.datetime(1970, 1, 1, 0, 0, 0, 1),\n    )\nb'\"1970-01-01T00:00:00.000001\"'\n>>> orjson.dumps(\n        datetime.datetime(1970, 1, 1, 0, 0, 0, 1),\n        option=orjson.OPT_OMIT_MICROSECONDS,\n    )\nb'\"1970-01-01T00:00:00\"'\nOPT_PASSTHROUGH_DATACLASS\nPassthrough dataclasses.dataclass instances to default. This allows\ncustomizing their output but is much slower.\n>>> import orjson, dataclasses\n>>>\n@dataclasses.dataclass\nclass User:\n    id: str\n    name: str\n    password: str\n\ndef default(obj):\n    if isinstance(obj, User):\n        return {\"id\": obj.id, \"name\": obj.name}\n    raise TypeError\n\n>>> orjson.dumps(User(\"3b1\", \"asd\", \"zxc\"))\nb'{\"id\":\"3b1\",\"name\":\"asd\",\"password\":\"zxc\"}'\n>>> orjson.dumps(User(\"3b1\", \"asd\", \"zxc\"), option=orjson.OPT_PASSTHROUGH_DATACLASS)\nTypeError: Type is not JSON serializable: User\n>>> orjson.dumps(\n        User(\"3b1\", \"asd\", \"zxc\"),\n        option=orjson.OPT_PASSTHROUGH_DATACLASS,\n        default=default,\n    )\nb'{\"id\":\"3b1\",\"name\":\"asd\"}'\nOPT_PASSTHROUGH_DATETIME\nPassthrough datetime.datetime, datetime.date, and datetime.time instances\nto default. This allows serializing datetimes to a custom format, e.g.,\nHTTP dates:\n>>> import orjson, datetime\n>>>\ndef default(obj):\n    if isinstance(obj, datetime.datetime):\n        return obj.strftime(\"%a, %d %b %Y %H:%M:%S GMT\")\n    raise TypeError\n\n>>> orjson.dumps({\"created_at\": datetime.datetime(1970, 1, 1)})\nb'{\"created_at\":\"1970-01-01T00:00:00\"}'\n>>> orjson.dumps({\"created_at\": datetime.datetime(1970, 1, 1)}, option=orjson.OPT_PASSTHROUGH_DATETIME)\nTypeError: Type is not JSON serializable: datetime.datetime\n>>> orjson.dumps(\n        {\"created_at\": datetime.datetime(1970, 1, 1)},\n        option=orjson.OPT_PASSTHROUGH_DATETIME,\n        default=default,\n    )\nb'{\"created_at\":\"Thu, 01 Jan 1970 00:00:00 GMT\"}'\nThis does not affect datetimes in dict keys if using OPT_NON_STR_KEYS.\nOPT_PASSTHROUGH_SUBCLASS\nPassthrough subclasses of builtin types to default.\n>>> import orjson\n>>>\nclass Secret(str):\n    pass\n\ndef default(obj):\n    if isinstance(obj, Secret):\n        return \"******\"\n    raise TypeError\n\n>>> orjson.dumps(Secret(\"zxc\"))\nb'\"zxc\"'\n>>> orjson.dumps(Secret(\"zxc\"), option=orjson.OPT_PASSTHROUGH_SUBCLASS)\nTypeError: Type is not JSON serializable: Secret\n>>> orjson.dumps(Secret(\"zxc\"), option=orjson.OPT_PASSTHROUGH_SUBCLASS, default=default)\nb'\"******\"'\nThis does not affect serializing subclasses as dict keys if using\nOPT_NON_STR_KEYS.\nOPT_SERIALIZE_DATACLASS\nThis is deprecated and has no effect in version 3. In version 2 this was\nrequired to serialize  dataclasses.dataclass instances. For more, see\ndataclass.\nOPT_SERIALIZE_NUMPY\nSerialize numpy.ndarray instances. For more, see\nnumpy.\nOPT_SERIALIZE_UUID\nThis is deprecated and has no effect in version 3. In version 2 this was\nrequired to serialize uuid.UUID instances. For more, see\nUUID.\nOPT_SORT_KEYS\nSerialize dict keys in sorted order. The default is to serialize in an\nunspecified order. This is equivalent to sort_keys=True in the standard\nlibrary.\nThis can be used to ensure the order is deterministic for hashing or tests.\nIt has a substantial performance penalty and is not recommended in general.\n>>> import orjson\n>>> orjson.dumps({\"b\": 1, \"c\": 2, \"a\": 3})\nb'{\"b\":1,\"c\":2,\"a\":3}'\n>>> orjson.dumps({\"b\": 1, \"c\": 2, \"a\": 3}, option=orjson.OPT_SORT_KEYS)\nb'{\"a\":3,\"b\":1,\"c\":2}'\nThis measures serializing the twitter.json fixture unsorted and sorted:\n\n\n\nLibrary\nunsorted (ms)\nsorted (ms)\nvs. orjson\n\n\n\n\norjson\n0.32\n0.54\n1\n\n\nujson\n1.6\n2.07\n3.8\n\n\nrapidjson\n1.12\n1.65\n3.1\n\n\nsimplejson\n2.25\n3.13\n5.8\n\n\njson\n1.78\n2.32\n4.3\n\n\n\nThe benchmark can be reproduced using the pysort script.\nThe sorting is not collation/locale-aware:\n>>> import orjson\n>>> orjson.dumps({\"a\": 1, \"\u00e4\": 2, \"A\": 3}, option=orjson.OPT_SORT_KEYS)\nb'{\"A\":3,\"a\":1,\"\\xc3\\xa4\":2}'\nThis is the same sorting behavior as the standard library, rapidjson,\nsimplejson, and ujson.\ndataclass also serialize as maps but this has no effect on them.\nOPT_STRICT_INTEGER\nEnforce 53-bit limit on integers. The limit is otherwise 64 bits, the same as\nthe Python standard library. For more, see int.\nOPT_UTC_Z\nSerialize a UTC timezone on datetime.datetime instances as Z instead\nof +00:00.\n>>> import orjson, datetime, zoneinfo\n>>> orjson.dumps(\n        datetime.datetime(1970, 1, 1, 0, 0, 0, tzinfo=zoneinfo.ZoneInfo(\"UTC\")),\n    )\nb'\"1970-01-01T00:00:00+00:00\"'\n>>> orjson.dumps(\n        datetime.datetime(1970, 1, 1, 0, 0, 0, tzinfo=zoneinfo.ZoneInfo(\"UTC\")),\n        option=orjson.OPT_UTC_Z\n    )\nb'\"1970-01-01T00:00:00Z\"'\nFragment\norjson.Fragment includes already-serialized JSON in a document. This is an\nefficient way to include JSON blobs from a cache, JSONB field, or separately\nserialized object without first deserializing to Python objects via loads().\n>>> import orjson\n>>> orjson.dumps({\"key\": \"zxc\", \"data\": orjson.Fragment(b'{\"a\": \"b\", \"c\": 1}')})\nb'{\"key\":\"zxc\",\"data\":{\"a\": \"b\", \"c\": 1}}'\nIt does no reformatting: orjson.OPT_INDENT_2 will not affect a\ncompact blob nor will a pretty-printed JSON blob be rewritten as compact.\nThe input must be bytes or str and given as a positional argument.\nThis raises orjson.JSONEncodeError if a str is given and the input is\nnot valid UTF-8. It otherwise does no validation and it is possible to\nwrite invalid JSON. This does not escape characters. The implementation is\ntested to not crash if given invalid strings or invalid JSON.\nThis is similar to RawJSON in rapidjson.\nDeserialize\ndef loads(__obj: Union[bytes, bytearray, memoryview, str]) -> Any: ...\nloads() deserializes JSON to Python objects. It deserializes to dict,\nlist, int, float, str, bool, and None objects.\nbytes, bytearray, memoryview, and str input are accepted. If the input\nexists as a memoryview, bytearray, or bytes object, it is recommended to\npass these directly rather than creating an unnecessary str object. That is,\norjson.loads(b\"{}\") instead of orjson.loads(b\"{}\".decode(\"utf-8\")). This\nhas lower memory usage and lower latency.\nThe input must be valid UTF-8.\norjson maintains a cache of map keys for the duration of the process. This\ncauses a net reduction in memory usage by avoiding duplicate strings. The\nkeys must be at most 64 bytes to be cached and 1024 entries are stored.\nThe global interpreter lock (GIL) is held for the duration of the call.\nIt raises JSONDecodeError if given an invalid type or invalid\nJSON. This includes if the input contains NaN, Infinity, or -Infinity,\nwhich the standard library allows, but is not valid JSON.\nJSONDecodeError is a subclass of json.JSONDecodeError and ValueError.\nThis is for compatibility with the standard library.\nTypes\ndataclass\norjson serializes instances of dataclasses.dataclass natively. It serializes\ninstances 40-50x as fast as other libraries and avoids a severe slowdown seen\nin other libraries compared to serializing dict.\nIt is supported to pass all variants of dataclasses, including dataclasses\nusing __slots__, frozen dataclasses, those with optional or default\nattributes, and subclasses. There is a performance benefit to not\nusing __slots__.\n\n\n\nLibrary\ndict (ms)\ndataclass (ms)\nvs. orjson\n\n\n\n\norjson\n1.40\n1.60\n1\n\n\nujson\n\n\n\n\n\nrapidjson\n3.64\n68.48\n42\n\n\nsimplejson\n14.21\n92.18\n57\n\n\njson\n13.28\n94.90\n59\n\n\n\nThis measures serializing 555KiB of JSON, orjson natively and other libraries\nusing default to serialize the output of dataclasses.asdict(). This can be\nreproduced using the pydataclass script.\nDataclasses are serialized as maps, with every attribute serialized and in\nthe order given on class definition:\n>>> import dataclasses, orjson, typing\n\n@dataclasses.dataclass\nclass Member:\n    id: int\n    active: bool = dataclasses.field(default=False)\n\n@dataclasses.dataclass\nclass Object:\n    id: int\n    name: str\n    members: typing.List[Member]\n\n>>> orjson.dumps(Object(1, \"a\", [Member(1, True), Member(2)]))\nb'{\"id\":1,\"name\":\"a\",\"members\":[{\"id\":1,\"active\":true},{\"id\":2,\"active\":false}]}'\ndatetime\norjson serializes datetime.datetime objects to\nRFC 3339 format,\ne.g., \"1970-01-01T00:00:00+00:00\". This is a subset of ISO 8601 and is\ncompatible with isoformat() in the standard library.\n>>> import orjson, datetime, zoneinfo\n>>> orjson.dumps(\n    datetime.datetime(2018, 12, 1, 2, 3, 4, 9, tzinfo=zoneinfo.ZoneInfo(\"Australia/Adelaide\"))\n)\nb'\"2018-12-01T02:03:04.000009+10:30\"'\n>>> orjson.dumps(\n    datetime.datetime(2100, 9, 1, 21, 55, 2).replace(tzinfo=zoneinfo.ZoneInfo(\"UTC\"))\n)\nb'\"2100-09-01T21:55:02+00:00\"'\n>>> orjson.dumps(\n    datetime.datetime(2100, 9, 1, 21, 55, 2)\n)\nb'\"2100-09-01T21:55:02\"'\ndatetime.datetime supports instances with a tzinfo that is None,\ndatetime.timezone.utc, a timezone instance from the python3.9+ zoneinfo\nmodule, or a timezone instance from the third-party pendulum, pytz, or\ndateutil/arrow libraries.\nIt is fastest to use the standard library's zoneinfo.ZoneInfo for timezones.\ndatetime.time objects must not have a tzinfo.\n>>> import orjson, datetime\n>>> orjson.dumps(datetime.time(12, 0, 15, 290))\nb'\"12:00:15.000290\"'\ndatetime.date objects will always serialize.\n>>> import orjson, datetime\n>>> orjson.dumps(datetime.date(1900, 1, 2))\nb'\"1900-01-02\"'\nErrors with tzinfo result in JSONEncodeError being raised.\nTo disable serialization of datetime objects specify the option\norjson.OPT_PASSTHROUGH_DATETIME.\nTo use \"Z\" suffix instead of \"+00:00\" to indicate UTC (\"Zulu\") time, use the option\norjson.OPT_UTC_Z.\nTo assume datetimes without timezone are UTC, use the option orjson.OPT_NAIVE_UTC.\nenum\norjson serializes enums natively. Options apply to their values.\n>>> import enum, datetime, orjson\n>>>\nclass DatetimeEnum(enum.Enum):\n    EPOCH = datetime.datetime(1970, 1, 1, 0, 0, 0)\n>>> orjson.dumps(DatetimeEnum.EPOCH)\nb'\"1970-01-01T00:00:00\"'\n>>> orjson.dumps(DatetimeEnum.EPOCH, option=orjson.OPT_NAIVE_UTC)\nb'\"1970-01-01T00:00:00+00:00\"'\nEnums with members that are not supported types can be serialized using\ndefault:\n>>> import enum, orjson\n>>>\nclass Custom:\n    def __init__(self, val):\n        self.val = val\n\ndef default(obj):\n    if isinstance(obj, Custom):\n        return obj.val\n    raise TypeError\n\nclass CustomEnum(enum.Enum):\n    ONE = Custom(1)\n\n>>> orjson.dumps(CustomEnum.ONE, default=default)\nb'1'\nfloat\norjson serializes and deserializes double precision floats with no loss of\nprecision and consistent rounding.\norjson.dumps() serializes Nan, Infinity, and -Infinity, which are not\ncompliant JSON, as null:\n>>> import orjson, ujson, rapidjson, json\n>>> orjson.dumps([float(\"NaN\"), float(\"Infinity\"), float(\"-Infinity\")])\nb'[null,null,null]'\n>>> ujson.dumps([float(\"NaN\"), float(\"Infinity\"), float(\"-Infinity\")])\nOverflowError: Invalid Inf value when encoding double\n>>> rapidjson.dumps([float(\"NaN\"), float(\"Infinity\"), float(\"-Infinity\")])\n'[NaN,Infinity,-Infinity]'\n>>> json.dumps([float(\"NaN\"), float(\"Infinity\"), float(\"-Infinity\")])\n'[NaN, Infinity, -Infinity]'\nint\norjson serializes and deserializes 64-bit integers by default. The range\nsupported is a signed 64-bit integer's minimum (-9223372036854775807) to\nan unsigned 64-bit integer's maximum (18446744073709551615). This\nis widely compatible, but there are implementations\nthat only support 53-bits for integers, e.g.,\nweb browsers. For those implementations, dumps() can be configured to\nraise a JSONEncodeError on values exceeding the 53-bit range.\n>>> import orjson\n>>> orjson.dumps(9007199254740992)\nb'9007199254740992'\n>>> orjson.dumps(9007199254740992, option=orjson.OPT_STRICT_INTEGER)\nJSONEncodeError: Integer exceeds 53-bit range\n>>> orjson.dumps(-9007199254740992, option=orjson.OPT_STRICT_INTEGER)\nJSONEncodeError: Integer exceeds 53-bit range\nnumpy\norjson natively serializes numpy.ndarray and individual\nnumpy.float64, numpy.float32,\nnumpy.int64, numpy.int32, numpy.int16, numpy.int8,\nnumpy.uint64, numpy.uint32, numpy.uint16, numpy.uint8,\nnumpy.uintp, numpy.intp, numpy.datetime64, and numpy.bool\ninstances.\norjson is faster than all compared libraries at serializing\nnumpy instances. Serializing numpy data requires specifying\noption=orjson.OPT_SERIALIZE_NUMPY.\n>>> import orjson, numpy\n>>> orjson.dumps(\n        numpy.array([[1, 2, 3], [4, 5, 6]]),\n        option=orjson.OPT_SERIALIZE_NUMPY,\n)\nb'[[1,2,3],[4,5,6]]'\nThe array must be a contiguous C array (C_CONTIGUOUS) and one of the\nsupported datatypes.\nNote a difference between serializing numpy.float32 using ndarray.tolist()\nor orjson.dumps(..., option=orjson.OPT_SERIALIZE_NUMPY): tolist() converts\nto a double before serializing and orjson's native path does not. This\ncan result in different rounding.\nnumpy.datetime64 instances are serialized as RFC 3339 strings and\ndatetime options affect them.\n>>> import orjson, numpy\n>>> orjson.dumps(\n        numpy.datetime64(\"2021-01-01T00:00:00.172\"),\n        option=orjson.OPT_SERIALIZE_NUMPY,\n)\nb'\"2021-01-01T00:00:00.172000\"'\n>>> orjson.dumps(\n        numpy.datetime64(\"2021-01-01T00:00:00.172\"),\n        option=(\n            orjson.OPT_SERIALIZE_NUMPY |\n            orjson.OPT_NAIVE_UTC |\n            orjson.OPT_OMIT_MICROSECONDS\n        ),\n)\nb'\"2021-01-01T00:00:00+00:00\"'\nIf an array is not a contiguous C array, contains an unsupported datatype,\nor contains a numpy.datetime64 using an unsupported representation\n(e.g., picoseconds), orjson falls through to default. In default,\nobj.tolist() can be specified. If an array is malformed, which\nis not expected, orjson.JSONEncodeError is raised.\nThis measures serializing 92MiB of JSON from an numpy.ndarray with\ndimensions of (50000, 100) and numpy.float64 values:\n\n\n\nLibrary\nLatency (ms)\nRSS diff (MiB)\nvs. orjson\n\n\n\n\norjson\n194\n99\n1.0\n\n\nujson\n\n\n\n\n\nrapidjson\n3,048\n309\n15.7\n\n\nsimplejson\n3,023\n297\n15.6\n\n\njson\n3,133\n297\n16.1\n\n\n\nThis measures serializing 100MiB of JSON from an numpy.ndarray with\ndimensions of (100000, 100) and numpy.int32 values:\n\n\n\nLibrary\nLatency (ms)\nRSS diff (MiB)\nvs. orjson\n\n\n\n\norjson\n178\n115\n1.0\n\n\nujson\n\n\n\n\n\nrapidjson\n1,512\n551\n8.5\n\n\nsimplejson\n1,606\n504\n9.0\n\n\njson\n1,506\n503\n8.4\n\n\n\nThis measures serializing 105MiB of JSON from an numpy.ndarray with\ndimensions of (100000, 200) and numpy.bool values:\n\n\n\nLibrary\nLatency (ms)\nRSS diff (MiB)\nvs. orjson\n\n\n\n\norjson\n157\n120\n1.0\n\n\nujson\n\n\n\n\n\nrapidjson\n710\n327\n4.5\n\n\nsimplejson\n931\n398\n5.9\n\n\njson\n996\n400\n6.3\n\n\n\nIn these benchmarks, orjson serializes natively, ujson is blank because it\ndoes not support a default parameter, and the other libraries serialize\nndarray.tolist() via default. The RSS column measures peak memory\nusage during serialization. This can be reproduced using the pynumpy script.\norjson does not have an installation or compilation dependency on numpy. The\nimplementation is independent, reading numpy.ndarray using\nPyArrayInterface.\nstr\norjson is strict about UTF-8 conformance. This is stricter than the standard\nlibrary's json module, which will serialize and deserialize UTF-16 surrogates,\ne.g., \"\\ud800\", that are invalid UTF-8.\nIf orjson.dumps() is given a str that does not contain valid UTF-8,\norjson.JSONEncodeError is raised. If loads() receives invalid UTF-8,\norjson.JSONDecodeError is raised.\norjson and rapidjson are the only compared JSON libraries to consistently\nerror on bad input.\n>>> import orjson, ujson, rapidjson, json\n>>> orjson.dumps('\\ud800')\nJSONEncodeError: str is not valid UTF-8: surrogates not allowed\n>>> ujson.dumps('\\ud800')\nUnicodeEncodeError: 'utf-8' codec ...\n>>> rapidjson.dumps('\\ud800')\nUnicodeEncodeError: 'utf-8' codec ...\n>>> json.dumps('\\ud800')\n'\"\\\\ud800\"'\n>>> orjson.loads('\"\\\\ud800\"')\nJSONDecodeError: unexpected end of hex escape at line 1 column 8: line 1 column 1 (char 0)\n>>> ujson.loads('\"\\\\ud800\"')\n''\n>>> rapidjson.loads('\"\\\\ud800\"')\nValueError: Parse error at offset 1: The surrogate pair in string is invalid.\n>>> json.loads('\"\\\\ud800\"')\n'\\ud800'\nTo make a best effort at deserializing bad input, first decode bytes using\nthe replace or lossy argument for errors:\n>>> import orjson\n>>> orjson.loads(b'\"\\xed\\xa0\\x80\"')\nJSONDecodeError: str is not valid UTF-8: surrogates not allowed\n>>> orjson.loads(b'\"\\xed\\xa0\\x80\"'.decode(\"utf-8\", \"replace\"))\n'\ufffd\ufffd\ufffd'\nuuid\norjson serializes uuid.UUID instances to\nRFC 4122 format, e.g.,\n\"f81d4fae-7dec-11d0-a765-00a0c91e6bf6\".\n>>> import orjson, uuid\n>>> orjson.dumps(uuid.UUID('f81d4fae-7dec-11d0-a765-00a0c91e6bf6'))\nb'\"f81d4fae-7dec-11d0-a765-00a0c91e6bf6\"'\n>>> orjson.dumps(uuid.uuid5(uuid.NAMESPACE_DNS, \"python.org\"))\nb'\"886313e1-3b8a-5372-9b90-0c9aee199e5d\"'\nTesting\nThe library has comprehensive tests. There are tests against fixtures in the\nJSONTestSuite and\nnativejson-benchmark\nrepositories. It is tested to not crash against the\nBig List of Naughty Strings.\nIt is tested to not leak memory. It is tested to not crash\nagainst and not accept invalid UTF-8. There are integration tests\nexercising the library's use in web servers (gunicorn using multiprocess/forked\nworkers) and when\nmultithreaded. It also uses some tests from the ultrajson library.\norjson is the most correct of the compared libraries. This graph shows how each\nlibrary handles a combined 342 JSON fixtures from the\nJSONTestSuite and\nnativejson-benchmark tests:\n\n\n\nLibrary\nInvalid JSON documents not rejected\nValid JSON documents not deserialized\n\n\n\n\norjson\n0\n0\n\n\nujson\n38\n0\n\n\nrapidjson\n6\n0\n\n\nsimplejson\n13\n0\n\n\njson\n17\n0\n\n\n\nThis shows that all libraries deserialize valid JSON but only orjson\ncorrectly rejects the given invalid JSON fixtures. Errors are largely due to\naccepting invalid strings and numbers.\nThe graph above can be reproduced using the pycorrectness script.\nPerformance\nSerialization and deserialization performance of orjson is better than\nultrajson, rapidjson, simplejson, or json. The benchmarks are done on\nfixtures of real data:\n\n\ntwitter.json, 631.5KiB, results of a search on Twitter for \"\u4e00\", containing\nCJK strings, dictionaries of strings and arrays of dictionaries, indented.\n\n\ngithub.json, 55.8KiB, a GitHub activity feed, containing dictionaries of\nstrings and arrays of dictionaries, not indented.\n\n\ncitm_catalog.json, 1.7MiB, concert data, containing nested dictionaries of\nstrings and arrays of integers, indented.\n\n\ncanada.json, 2.2MiB, coordinates of the Canadian border in GeoJSON\nformat, containing floats and arrays, indented.\n\n\nLatency\ntwitter.json serialization\n\n\n\nLibrary\nMedian latency (milliseconds)\nOperations per second\nRelative (latency)\n\n\n\n\norjson\n0.33\n3069.4\n1\n\n\nujson\n1.68\n592.8\n5.15\n\n\nrapidjson\n1.12\n891\n3.45\n\n\nsimplejson\n2.29\n436.2\n7.03\n\n\njson\n1.8\n556.6\n5.52\n\n\n\ntwitter.json deserialization\n\n\n\nLibrary\nMedian latency (milliseconds)\nOperations per second\nRelative (latency)\n\n\n\n\norjson\n0.81\n1237.6\n1\n\n\nujson\n1.87\n533.9\n2.32\n\n\nrapidjson\n2.97\n335.8\n3.67\n\n\nsimplejson\n2.15\n463.8\n2.66\n\n\njson\n2.45\n408.2\n3.03\n\n\n\ngithub.json serialization\n\n\n\nLibrary\nMedian latency (milliseconds)\nOperations per second\nRelative (latency)\n\n\n\n\norjson\n0.03\n28817.3\n1\n\n\nujson\n0.18\n5478.2\n5.26\n\n\nrapidjson\n0.1\n9686.4\n2.98\n\n\nsimplejson\n0.26\n3901.3\n7.39\n\n\njson\n0.18\n5437\n5.27\n\n\n\ngithub.json deserialization\n\n\n\nLibrary\nMedian latency (milliseconds)\nOperations per second\nRelative (latency)\n\n\n\n\norjson\n0.07\n15270\n1\n\n\nujson\n0.19\n5374.8\n2.84\n\n\nrapidjson\n0.17\n5854.9\n2.59\n\n\nsimplejson\n0.15\n6707.4\n2.27\n\n\njson\n0.16\n6397.3\n2.39\n\n\n\ncitm_catalog.json serialization\n\n\n\nLibrary\nMedian latency (milliseconds)\nOperations per second\nRelative (latency)\n\n\n\n\norjson\n0.58\n1722.5\n1\n\n\nujson\n2.89\n345.6\n4.99\n\n\nrapidjson\n1.83\n546.4\n3.15\n\n\nsimplejson\n10.39\n95.9\n17.89\n\n\njson\n3.93\n254.6\n6.77\n\n\n\ncitm_catalog.json deserialization\n\n\n\nLibrary\nMedian latency (milliseconds)\nOperations per second\nRelative (latency)\n\n\n\n\norjson\n1.76\n569.2\n1\n\n\nujson\n3.5\n284.3\n1.99\n\n\nrapidjson\n5.77\n173.2\n3.28\n\n\nsimplejson\n5.13\n194.7\n2.92\n\n\njson\n4.99\n200.5\n2.84\n\n\n\ncanada.json serialization\n\n\n\nLibrary\nMedian latency (milliseconds)\nOperations per second\nRelative (latency)\n\n\n\n\norjson\n3.62\n276.3\n1\n\n\nujson\n14.16\n70.6\n3.91\n\n\nrapidjson\n33.64\n29.7\n9.29\n\n\nsimplejson\n57.46\n17.4\n15.88\n\n\njson\n35.7\n28\n9.86\n\n\n\ncanada.json deserialization\n\n\n\nLibrary\nMedian latency (milliseconds)\nOperations per second\nRelative (latency)\n\n\n\n\norjson\n3.89\n256.6\n1\n\n\nujson\n8.73\n114.3\n2.24\n\n\nrapidjson\n23.33\n42.8\n5.99\n\n\nsimplejson\n23.99\n41.7\n6.16\n\n\njson\n21.1\n47.4\n5.42\n\n\n\nMemory\norjson as of 3.7.0 has higher baseline memory usage than other libraries\ndue to a persistent buffer used for parsing. Incremental memory usage when\ndeserializing is similar to the standard library and other third-party\nlibraries.\nThis measures, in the first column, RSS after importing a library and reading\nthe fixture, and in the second column, increases in RSS after repeatedly\ncalling loads() on the fixture.\ntwitter.json\n\n\n\nLibrary\nimport, read() RSS (MiB)\nloads() increase in RSS (MiB)\n\n\n\n\norjson\n21.8\n2.8\n\n\nujson\n14.3\n4.8\n\n\nrapidjson\n14.9\n4.6\n\n\nsimplejson\n13.4\n2.4\n\n\njson\n13.1\n2.3\n\n\n\ngithub.json\n\n\n\nLibrary\nimport, read() RSS (MiB)\nloads() increase in RSS (MiB)\n\n\n\n\norjson\n21.2\n0.5\n\n\nujson\n13.6\n0.6\n\n\nrapidjson\n14.1\n0.5\n\n\nsimplejson\n12.5\n0.3\n\n\njson\n12.4\n0.3\n\n\n\ncitm_catalog.json\n\n\n\nLibrary\nimport, read() RSS (MiB)\nloads() increase in RSS (MiB)\n\n\n\n\norjson\n23\n10.6\n\n\nujson\n15.2\n11.2\n\n\nrapidjson\n15.8\n29.7\n\n\nsimplejson\n14.4\n24.7\n\n\njson\n13.9\n24.7\n\n\n\ncanada.json\n\n\n\nLibrary\nimport, read() RSS (MiB)\nloads() increase in RSS (MiB)\n\n\n\n\norjson\n23.2\n21.3\n\n\nujson\n15.6\n19.2\n\n\nrapidjson\n16.3\n23.4\n\n\nsimplejson\n15\n21.1\n\n\njson\n14.3\n20.9\n\n\n\nReproducing\nThe above was measured using Python 3.10.5 on Linux (amd64) with\norjson 3.7.9, ujson 5.4.0, python-rapidson 1.8, and simplejson 3.17.6.\nThe latency results can be reproduced using the pybench and graph\nscripts. The memory results can be reproduced using the pymem script.\nQuestions\nWhy can't I install it from PyPI?\nProbably pip needs to be upgraded to version 20.3 or later to support\nthe latest manylinux_x_y or universal2 wheel formats.\n\"Cargo, the Rust package manager, is not installed or is not on PATH.\"\nThis happens when there are no binary wheels (like manylinux) for your\nplatform on PyPI. You can install Rust through\nrustup or a package manager and then it will compile.\nWill it deserialize to dataclasses, UUIDs, decimals, etc or support object_hook?\nNo. This requires a schema specifying what types are expected and how to\nhandle errors etc. This is addressed by data validation libraries a\nlevel above this.\nWill it serialize to str?\nNo. bytes is the correct type for a serialized blob.\nWill it support PyPy?\nProbably not.\nPackaging\nTo package orjson requires at least Rust 1.60\nand the maturin build tool. The recommended\nbuild command is:\nmaturin build --release --strip\nIt benefits from also having a C build environment to compile a faster\ndeserialization backend. See this project's manylinux_2_28 builds for an\nexample using clang and LTO.\nThe project's own CI tests against nightly-2023-06-30 and stable 1.60. It\nis prudent to pin the nightly version because that channel can introduce\nbreaking changes.\norjson is tested for amd64, aarch64, arm7, ppc64le, and s390x on Linux. It\nis tested for amd64 on macOS and cross-compiles for aarch64. For Windows\nit is tested on amd64 and i686.\nThere are no runtime dependencies other than libc.\nThe source distribution on PyPI contains all dependencies' source and can be\nbuilt without network access. The file can be downloaded from\nhttps://files.pythonhosted.org/packages/source/o/orjson/orjson-${version}.tar.gz.\norjson's tests are included in the source distribution on PyPI. The\nrequirements to run the tests are specified in test/requirements.txt. The\ntests should be run as part of the build. It can be run with\npytest -q test.\nLicense\norjson was written by ijl <ijl@mailbox.org>, copyright 2018 - 2023, licensed\nunder both the Apache 2 and MIT licenses.\n\n\n"}, {"name": "opt-einsum", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nOptimized Einsum\nOptimized Einsum: A tensor contraction order optimizer\nExample usage\nFeatures\nInstallation\nCitation\nContributing\n\n\n\n\n\nREADME.md\n\n\n\n\nOptimized Einsum\n\n\n\n\n\n\n\nOptimized Einsum: A tensor contraction order optimizer\nOptimized einsum can significantly reduce the overall execution time of einsum-like expressions (e.g.,\nnp.einsum,\ndask.array.einsum,\npytorch.einsum,\ntensorflow.einsum,\n)\nby optimizing the expression's contraction order and dispatching many\noperations to canonical BLAS, cuBLAS, or other specialized routines.\nOptimized\neinsum is agnostic to the backend and can handle NumPy, Dask, PyTorch,\nTensorflow, CuPy, Sparse, Theano, JAX, and Autograd arrays as well as potentially\nany library which conforms to a standard API. See the\ndocumentation for more\ninformation.\nExample usage\nThe opt_einsum.contract\nfunction can often act as a drop-in replacement for einsum\nfunctions without further changes to the code while providing superior performance.\nHere, a tensor contraction is performed with and without optimization:\nimport numpy as np\nfrom opt_einsum import contract\n\nN = 10\nC = np.random.rand(N, N)\nI = np.random.rand(N, N, N, N)\n\n%timeit np.einsum('pi,qj,ijkl,rk,sl->pqrs', C, C, I, C, C)\n1 loops, best of 3: 934 ms per loop\n\n%timeit contract('pi,qj,ijkl,rk,sl->pqrs', C, C, I, C, C)\n1000 loops, best of 3: 324 us per loop\nIn this particular example, we see a ~3000x performance improvement which is\nnot uncommon when compared against unoptimized contractions. See the backend\nexamples\nfor more information on using other backends.\nFeatures\nThe algorithms found in this repository often power the einsum optimizations\nin many of the above projects. For example, the optimization of np.einsum\nhas been passed upstream and most of the same features that can be found in\nthis repository can be enabled with np.einsum(..., optimize=True). However,\nthis repository often has more up to date algorithms for complex contractions.\nThe following capabilities are enabled by opt_einsum:\n\nInspect detailed information about the path chosen.\nPerform contractions with numerous backends, including on the GPU and with libraries such as TensorFlow and PyTorch.\nGenerate reusable expressions, potentially with constant tensors, that can be compiled for greater performance.\nUse an arbitrary number of indices to find contractions for hundreds or even thousands of tensors.\nShare intermediate computations among multiple contractions.\nCompute gradients of tensor contractions using autograd or jax\n\nPlease see the documentation for more features!\nInstallation\nopt_einsum can either be installed via pip install opt_einsum or from conda conda install opt_einsum -c conda-forge.\nSee the installation documentation for further methods.\nCitation\nIf this code has benefited your research, please support us by citing:\nDaniel G. A. Smith and Johnnie Gray, opt_einsum - A Python package for optimizing contraction order for einsum-like expressions. Journal of Open Source Software, 2018, 3(26), 753\nDOI: https://doi.org/10.21105/joss.00753\nContributing\nAll contributions, bug reports, bug fixes, documentation improvements, enhancements, and ideas are welcome.\nA detailed overview on how to contribute can be found in the contributing guide.\n\n\n"}, {"name": "nest-asyncio", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\nInstallation\nUsage\n\n\n\n\n\nREADME.rst\n\n\n\n\n \n\n \n\n\nIntroduction\nBy design asyncio does not allow\nits event loop to be nested. This presents a practical problem:\nWhen in an environment where the event loop is\nalready running it's impossible to run tasks and wait\nfor the result. Trying to do so will give the error\n\"RuntimeError: This event loop is already running\".\nThe issue pops up in various environments, such as web servers,\nGUI applications and in Jupyter notebooks.\nThis module patches asyncio to allow nested use of asyncio.run and\nloop.run_until_complete.\n\nInstallation\npip3 install nest_asyncio\n\nPython 3.5 or higher is required.\n\nUsage\nimport nest_asyncio\nnest_asyncio.apply()\nOptionally the specific loop that needs patching can be given\nas argument to apply, otherwise the current event loop is used.\nAn event loop can be patched whether it is already running\nor not. Only event loops from asyncio can be patched;\nLoops from other projects, such as uvloop or quamash,\ngenerally can't be patched.\n\n\n"}, {"name": "ipykernel", "readme": "\nIPython Kernel for Jupyter\n\n\nThis package provides the IPython kernel for Jupyter.\nInstallation from source\n\ngit clone\ncd ipykernel\npip install -e \".[test]\"\n\nAfter that, all normal ipython commands will use this newly-installed version of the kernel.\nRunning tests\nFollow the instructions from Installation from source.\nand then from the root directory\npytest ipykernel\n\nRunning tests with coverage\nFollow the instructions from Installation from source.\nand then from the root directory\npytest ipykernel -vv -s --cov ipykernel --cov-branch --cov-report term-missing:skip-covered --durations 10\n\nAbout the IPython Development Team\nThe IPython Development Team is the set of all contributors to the IPython project.\nThis includes all of the IPython subprojects.\nThe core team that coordinates development on GitHub can be found here:\nhttps://github.com/ipython/.\nOur Copyright Policy\nIPython uses a shared copyright model. Each contributor maintains copyright\nover their contributions to IPython. But, it is important to note that these\ncontributions are typically only changes to the repositories. Thus, the IPython\nsource code, in its entirety is not the copyright of any single person or\ninstitution. Instead, it is the collective copyright of the entire IPython\nDevelopment Team. If individual contributors want to maintain a record of what\nchanges/contributions they have specific copyright on, they should indicate\ntheir copyright in the commit message of the change, when they commit the\nchange to one of the IPython repositories.\nWith this in mind, the following banner should be used in any source code file\nto indicate the copyright and license terms:\n# Copyright (c) IPython Development Team.\n# Distributed under the terms of the Modified BSD License.\n\n"}, {"name": "basemap", "readme": "\nbasemap\nPlot on map projections (with coastlines and political boundaries) using\nmatplotlib.\nThis package depends on the support package basemap-data with the\nbasic basemap data assets, and optionally on the support package\nbasemap-data-hires with high-resolution data assets.\nInstallation\nPrecompiled binary wheels for Windows and GNU/Linux are available in\nPyPI (architectures x86 and x64, Python 2.7 and 3.5+) and can be\ninstalled with pip:\npython -m pip install basemap\n\nIf you need to install from source, please visit the\nGitHub repository for a\nstep-by-step description.\nLicense\nThe library is licensed under the terms of the MIT license (see\nLICENSE). The GEOS dynamic library bundled with the package wheels\nis provided under the terms of the LGPLv2.1 license as given in\nLICENSE.geos.\n"}, {"name": "basemap-data", "readme": "\nbasemap-data\nPlot on map projections (with coastlines and political boundaries) using\nmatplotlib.\nThis is a support package for basemap with the basic data assets\nrequired by basemap to work.\nInstallation\nThe package is available in PyPI and can be installed with pip:\npython -m pip install basemap-data\n\nLicense\nThe land-sea mask, coastline, lake, river and political boundary data\nare extracted from the GSHHG datasets (version 2.3.6) using GMT\n(5.x series) and are included under the terms of the LGPLv3+ license\n(see COPYING and COPYING.LESSER).\nThe other files are included under the terms of the MIT license. See\nLICENSE.epsg for the EPSG file (taken from the PROJ.4 package) and\nLICENSE.mit for the rest.\n"}, {"name": "backports.zoneinfo", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nbackports.zoneinfo: Backport of the standard library module zoneinfo\nInstallation and depending on this library\nUse\nContributing\n\n\n\n\n\nREADME.md\n\n\n\n\nbackports.zoneinfo: Backport of the standard library module zoneinfo\nThis package was originally the reference implementation for PEP 615, which proposes support for the IANA time zone database in the standard library, and now serves as a backport to Python 3.6+ (including PyPy).\nThis exposes the backports.zoneinfo module, which is a backport of the zoneinfo module. The backport's documentation can be found on readthedocs.\nThe module uses the system time zone data if available, and falls back to the tzdata package (available on PyPI) if installed.\nInstallation and depending on this library\nThis module is called backports.zoneinfo on PyPI. To install it in your local environment, use:\npip install backports.zoneinfo\n\nOr (particularly on Windows), you can also use the tzdata extra (which basically just declares a dependency on tzdata, so this doesn't actually save you any typing \ud83d\ude05):\npip install backports.zoneinfo[tzdata]\n\nIf you want to use this in your application, it is best to use PEP 508 environment markers to declare a dependency conditional on the Python version:\nbackports.zoneinfo;python_version<\"3.9\"\n\nSupport for backports.zoneinfo in Python 3.9+ is currently minimal, since it is expected that you would use the standard library zoneinfo module instead.\nUse\nThe backports.zoneinfo module should be a drop-in replacement for the Python 3.9 standard library module zoneinfo. If you do not support anything earlier than Python 3.9, you do not need this library; if you are supporting Python 3.6+, you may want to use this idiom to \"fall back\" to backports.zoneinfo:\ntry:\n    import zoneinfo\nexcept ImportError:\n    from backports import zoneinfo\nTo get access to time zones with this module, construct a ZoneInfo object and attach it to your datetime:\n>>> from backports.zoneinfo import ZoneInfo\n>>> from datetime import datetime, timedelta, timezone\n>>> dt = datetime(1992, 3, 1, tzinfo=ZoneInfo(\"Europe/Minsk\"))\n>>> print(dt)\n1992-03-01 00:00:00+02:00\n>>> print(dt.utcoffset())\n2:00:00\n>>> print(dt.tzname())\nEET\nArithmetic works as expected without the need for a \"normalization\" step:\n>>> dt += timedelta(days=90)\n>>> print(dt)\n1992-05-30 00:00:00+03:00\n>>> dt.utcoffset()\ndatetime.timedelta(seconds=10800)\n>>> dt.tzname()\n'EEST'\nAmbiguous and imaginary times are handled using the fold attribute added in PEP 495:\n>>> dt = datetime(2020, 11, 1, 1, tzinfo=ZoneInfo(\"America/Chicago\"))\n>>> print(dt)\n2020-11-01 01:00:00-05:00\n>>> print(dt.replace(fold=1))\n2020-11-01 01:00:00-06:00\n\n>>> UTC = timezone.utc\n>>> print(dt.astimezone(UTC))\n2020-11-01 06:00:00+00:00\n>>> print(dt.replace(fold=1).astimezone(UTC))\n2020-11-01 07:00:00+00:00\nContributing\nCurrently we are not accepting contributions to this repository because we have not put the CLA in place and we would like to avoid complicating the process of adoption into the standard library. Contributions to CPython will eventually be backported to this repository \u2014 see the Python developer's guide for more information on how to contribute to CPython.\n\n\n"}]