[{"name": "matplotlib-inline", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nMatplotlib Inline Back-end for IPython and Jupyter\nInstallation\nUsage\nLicense\n\n\n\n\n\nREADME.md\n\n\n\n\nMatplotlib Inline Back-end for IPython and Jupyter\nThis package provides support for matplotlib to display figures directly inline in the Jupyter notebook and related clients, as shown below.\nInstallation\nWith conda:\nconda install -c conda-forge matplotlib-inline\nWith pip:\npip install matplotlib-inline\nUsage\nNote that in current versions of JupyterLab and Jupyter Notebook, the explicit use of the %matplotlib inline directive is not needed anymore, though other third-party clients may still require it.\nThis will produce a figure immediately below:\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0, 3*np.pi, 500)\nplt.plot(x, np.sin(x**2))\nplt.title('A simple chirp');\nLicense\nLicensed under the terms of the BSD 3-Clause License, by the IPython Development Team (see LICENSE file).\n\n\n"}, {"name": "MarkupSafe", "readme": "\nMarkupSafe implements a text object that escapes characters so it is\nsafe to use in HTML and XML. Characters that have special meanings are\nreplaced so that they display as the actual characters. This mitigates\ninjection attacks, meaning untrusted user input can safely be displayed\non a page.\n\nInstalling\nInstall and update using pip:\npip install -U MarkupSafe\n\n\nExamples\n>>> from markupsafe import Markup, escape\n\n>>> # escape replaces special characters and wraps in Markup\n>>> escape(\"<script>alert(document.cookie);</script>\")\nMarkup('&lt;script&gt;alert(document.cookie);&lt;/script&gt;')\n\n>>> # wrap in Markup to mark text \"safe\" and prevent escaping\n>>> Markup(\"<strong>Hello</strong>\")\nMarkup('<strong>hello</strong>')\n\n>>> escape(Markup(\"<strong>Hello</strong>\"))\nMarkup('<strong>hello</strong>')\n\n>>> # Markup is a str subclass\n>>> # methods and operators escape their arguments\n>>> template = Markup(\"Hello <em>{name}</em>\")\n>>> template.format(name='\"World\"')\nMarkup('Hello <em>&#34;World&#34;</em>')\n\n\nDonate\nThe Pallets organization develops and supports MarkupSafe and other\npopular packages. In order to grow the community of contributors and\nusers, and allow the maintainers to devote more time to the projects,\nplease donate today.\n\n\nLinks\n\nDocumentation: https://markupsafe.palletsprojects.com/\nChanges: https://markupsafe.palletsprojects.com/changes/\nPyPI Releases: https://pypi.org/project/MarkupSafe/\nSource Code: https://github.com/pallets/markupsafe/\nIssue Tracker: https://github.com/pallets/markupsafe/issues/\nChat: https://discord.gg/pallets\n\n\n"}, {"name": "korean-lunar-calendar", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nkorean_lunar_calendar\nOverview\nDocs\nInstall\nImport\nExample\nValidation\nOther languages\n\n\n\n\n\nREADME.md\n\n\n\n\nkorean_lunar_calendar\n\nLibrary to convert Korean lunar-calendar to Gregorian calendar.\n\nOverview\nKorean calendar and Chinese calendar is same lunar calendar but have different date.\nThis follow the KARI(Korea Astronomy and Space Science Institute)\n\ud55c\uad6d \uc591\uc74c\ub825 \ubcc0\ud658 (\ud55c\uad6d\ucc9c\ubb38\uc5f0\uad6c\uc6d0 \uae30\uc900) - \ub124\ud2b8\uc6cc\ud06c \uc5f0\uacb0 \ubd88\ud544\uc694\n\uc74c\ub825 \uc9c0\uc6d0 \ubc94\uc704 (1000\ub144 01\uc6d4 01\uc77c ~ 2050\ub144 11\uc6d4 18\uc77c)\nKorean Lunar Calendar (1000-01-01 ~ 2050-11-18)\n\n\uc591\ub825 \uc9c0\uc6d0 \ubc94\uc704 (1000\ub144 02\uc6d4 13\uc77c ~ 2050\ub144 12\uc6d4 31\uc77c)\nGregorian Calendar (1000-02-13 ~ 2050-12-31)\n\nExample Site\nDocs\n\nInstall\nImport\nExample\nValidation\nOther languages\n\nInstall\npip install korean_lunar_calendar\nImport\nfrom korean_lunar_calendar import KoreanLunarCalendar\nExample\nKorean Solar Date -> Korean Lunar Date (\uc591\ub825 -> \uc74c\ub825)\ncalendar = KoreanLunarCalendar()\n\n# params : year(\ub144), month(\uc6d4), day(\uc77c)\ncalendar.setSolarDate(2017, 6, 24)\n\n# Lunar Date (ISO Format)\nprint(calendar.LunarIsoFormat())\n\n# Korean GapJa String\nprint(calendar.getGapJaString())\n\n# Chinese GapJa String\nprint(calendar.getChineseGapJaString())\n[Result]\n2017-05-01 Intercalation\n\uc815\uc720\ub144 \ubcd1\uc624\uc6d4 \uc784\uc624\uc77c (\uc724\uc6d4)\n\u4e01\u9149\u5e74 \u4e19\u5348\u6708 \u58ec\u5348\u65e5 (\u958f\u6708)\n\nKorean Lunar Date -> Korean Solar Date (\uc74c\ub825 -> \uc591\ub825)\ncalendar = KoreanLunarCalendar()\n\n# params : year(\ub144), month(\uc6d4), day(\uc77c), intercalation(\uc724\ub2ec\uc5ec\ubd80)\ncalendar.setLunarDate(1956, 1, 21, False)\n\n# Solar Date (ISO Format)\nprint(calendar.SolarIsoFormat())\n\n# Korean GapJa String\nprint(calendar.getGapJaString())\n\n# Chinese GapJa String\nprint(calendar.getChineseGapJaString())\n[Result]\n1956-03-03\n\ubcd1\uc2e0\ub144 \uacbd\uc778\uc6d4 \uae30\uc0ac\uc77c\n\u4e19\u7533\u5e74 \u5e9a\u5bc5\u6708 \u5df1\u5df3\u65e5\n\nValidation\nCheck for invalid date input\ncalendar = KoreanLunarCalendar()\n\n# invald date\ncalendar.setLunarDate(99, 1, 1, False) # => return False\ncalendar.setSolarDate(2051, 1, 1) # => return False\n\n# OK\ncalendar.setLunarDate(1000, 1, 1, False) # => return True\ncalendar.setSolarDate(2050, 12, 31) # => return True\nOther languages\n\nJava : https://github.com/usingsky/KoreanLunarCalendar\nPython : https://github.com/usingsky/korean_lunar_calendar_py\nJavascript : https://github.com/usingsky/korean_lunar_calendar_js\n\n\n\n"}, {"name": "jupyterlab-server", "readme": "\n\n\n\n\n\n\n\n\n\n\n\njupyterlab server\nMotivation\nInstall\nUsage\nExtending the Application\nContribution\n\n\n\n\n\nREADME.md\n\n\n\n\njupyterlab server\n\n\nMotivation\nJupyterLab Server sits between JupyterLab and Jupyter Server, and provides a\nset of REST API handlers and utilities that are used by JupyterLab. It is a separate project in order to\naccommodate creating JupyterLab-like applications from a more limited scope.\nInstall\npip install jupyterlab_server\nTo include optional openapi dependencies, use:\npip install jupyterlab_server[openapi]\nTo include optional pytest_plugin dependencies, use:\npip install jupyterlab_server[test]\nUsage\nSee the full documentation for API docs and REST endpoint descriptions.\nExtending the Application\nSubclass the LabServerApp and provide additional traits and handlers as appropriate for your application.\nContribution\nPlease see CONTRIBUTING.md for details.\n\n\n"}, {"name": "jupyterlab-pygments", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nJupyterLab Pygments Theme\nScreencast\nInstallation\nDependencies\nLimitations\nLicense\n\n\n\n\n\nREADME.md\n\n\n\n\nJupyterLab Pygments Theme\nThis package contains a syntax coloring theme for pygments making use of\nthe JupyterLab CSS variables.\nThe goal is to enable the use of JupyterLab's themes with pygments-generated HTML.\nScreencast\nIn the following screencast, we demonstrate how Pygments-highlighted code can make use of the JupyterLab theme.\n\nInstallation\njupyterlab_pygments can be installed with the conda package manager\nconda install -c conda-forge jupyterlab_pygments\n\nor from pypi\npip install jupyterlab_pygments\n\nDependencies\n\njupyterlab_pygments requires pygments version 2.4.1.\nThe CSS variables used by the theme correspond to the CodeMirror syntex coloring\ntheme defined in the NPM package @jupyterlab/codemirror. Supported versions for @jupyterlab/codemirror's CSS include 0.19.1, ^1.0, and, ^2.0.\n\nLimitations\nPygments-generated HTML and CSS classes are not granular enough to reproduce\nall of the details of codemirror (the JavaScript text editor used by JupyterLab).\nThis includes the ability to differentiate properties from general names.\nLicense\njupyterlab_pygments uses a shared copyright model that enables all contributors to maintain the\ncopyright on their contributions. All code is licensed under the terms of the revised BSD license.\n\n\n"}, {"name": "jupyter-server", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJupyter Server\nInstallation and Basic usage\nVersioning and Branches\nUsage - Running Jupyter Server\nRunning in a local installation\nTesting\nContributing\nTeam Meetings and Roadmap\nAbout the Jupyter Development Team\nOur Copyright Policy\n\n\n\n\n\nREADME.md\n\n\n\n\nJupyter Server\n\n\nThe Jupyter Server provides the backend (i.e. the core services, APIs, and REST endpoints) for Jupyter web applications like Jupyter notebook, JupyterLab, and Voila.\nFor more information, read our documentation here.\nInstallation and Basic usage\nTo install the latest release locally, make sure you have\npip installed and run:\npip install jupyter_server\n\nJupyter Server currently supports Python>=3.6 on Linux, OSX and Windows.\nVersioning and Branches\nIf Jupyter Server is a dependency of your project/application, it is important that you pin it to a version that works for your application. Currently, Jupyter Server only has minor and patch versions. Different minor versions likely include API-changes while patch versions do not change API.\nWhen a new minor version is released on PyPI, a branch for that version will be created in this repository, and the version of the main branch will be bumped to the next minor version number. That way, the main branch always reflects the latest un-released version.\nTo see the changes between releases, checkout the CHANGELOG.\nUsage - Running Jupyter Server\nRunning in a local installation\nLaunch with:\njupyter server\n\nTesting\nSee CONTRIBUTING.\nContributing\nIf you are interested in contributing to the project, see CONTRIBUTING.rst.\nTeam Meetings and Roadmap\n\nWhen: Thursdays 8:00am, Pacific time\nWhere: Jovyan Zoom\nWhat: Meeting notes\n\nSee our tentative roadmap here.\nAbout the Jupyter Development Team\nThe Jupyter Development Team is the set of all contributors to the Jupyter project.\nThis includes all of the Jupyter subprojects.\nThe core team that coordinates development on GitHub can be found here:\nhttps://github.com/jupyter/.\nOur Copyright Policy\nJupyter uses a shared copyright model. Each contributor maintains copyright\nover their contributions to Jupyter. But, it is important to note that these\ncontributions are typically only changes to the repositories. Thus, the Jupyter\nsource code, in its entirety is not the copyright of any single person or\ninstitution. Instead, it is the collective copyright of the entire Jupyter\nDevelopment Team. If individual contributors want to maintain a record of what\nchanges/contributions they have specific copyright on, they should indicate\ntheir copyright in the commit message of the change, when they commit the\nchange to one of the Jupyter repositories.\nWith this in mind, the following banner should be used in any source code file\nto indicate the copyright and license terms:\n# Copyright (c) Jupyter Development Team.\n# Distributed under the terms of the Modified BSD License.\n\n\n\n"}, {"name": "jupyter-core", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nJupyter Core\nDevelopment Setup\nCoding\nCode Styling\nDocumentation\nAbout the Jupyter Development Team\nOur Copyright Policy\n\n\n\n\n\nREADME.md\n\n\n\n\nJupyter Core\n\n\nCore common functionality of Jupyter projects.\nThis package contains base application classes and configuration inherited by other projects.\nIt doesn't do much on its own.\nDevelopment Setup\nThe Jupyter Contributor Guides provide extensive information on contributing code or documentation to Jupyter projects. The limited instructions below for setting up a development environment are for your convenience.\nCoding\nYou'll need Python and pip on the search path. Clone the Jupyter Core git repository to your computer, for example in /my/projects/jupyter_core.\nNow create an editable install\nand download the dependencies of code and test suite by executing:\ncd /my/projects/jupyter_core/\npip install -e \".[test]\"\npy.test\n\nThe last command runs the test suite to verify the setup. During development, you can pass filenames to py.test, and it will execute only those tests.\nCode Styling\njupyter_core has adopted automatic code formatting so you shouldn't\nneed to worry too much about your code style.\nAs long as your code is valid,\nthe pre-commit hook should take care of how it should look.\npre-commit and its associated hooks will automatically be installed when\nyou run pip install -e \".[test]\"\nTo install pre-commit manually, run the following:\n    pip install pre-commit\n    pre-commit install\nYou can invoke the pre-commit hook by hand at any time with:\n    pre-commit run\nwhich should run any autoformatting on your code\nand tell you about any errors it couldn't fix automatically.\nYou may also install black integration\ninto your text editor to format code automatically.\nIf you have already committed files before setting up the pre-commit\nhook with pre-commit install, you can fix everything up using\npre-commit run --all-files. You need to make the fixing commit\nyourself after that.\nDocumentation\nThe documentation of Jupyter Core is generated from the files in docs/ using Sphinx. Instructions for setting up Sphinx with a selection of optional modules are in the Documentation Guide. You'll also need the make command.\nFor a minimal Sphinx installation to process the Jupyter Core docs, execute:\npip install sphinx\n\nThe following commands build the documentation in HTML format and check for broken links:\ncd /my/projects/jupyter_core/docs/\nmake html linkcheck\n\nPoint your browser to the following URL to access the generated documentation:\nfile:///my/projects/jupyter_core/docs/_build/html/index.html\nAbout the Jupyter Development Team\nThe Jupyter Development Team is the set of all contributors to the Jupyter\nproject. This includes all of the Jupyter subprojects. A full list with\ndetails is kept in the documentation directory, in the file\nabout/credits.txt.\nThe core team that coordinates development on GitHub can be found here:\nhttps://github.com/ipython/.\nOur Copyright Policy\nJupyter uses a shared copyright model. Each contributor maintains copyright\nover their contributions to Jupyter. It is important to note that these\ncontributions are typically only changes to the repositories. Thus, the Jupyter\nsource code in its entirety is not the copyright of any single person or\ninstitution. Instead, it is the collective copyright of the entire Jupyter\nDevelopment Team. If individual contributors want to maintain a record of what\nchanges/contributions they have specific copyright on, they should indicate\ntheir copyright in the commit message of the change, when they commit the\nchange to one of the Jupyter repositories.\nWith this in mind, the following banner should be used in any source code file\nto indicate the copyright and license terms:\n# Copyright (c) Jupyter Development Team.\n# Distributed under the terms of the Modified BSD License.\n\n\n\n"}, {"name": "jupyter-client", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nJupyter Client\nDevelopment Setup\nCoding\nDocumentation\nContributing\nAbout the Jupyter Development Team\nOur Copyright Policy\n\n\n\n\n\nREADME.md\n\n\n\n\nJupyter Client\n\n\njupyter_client contains the reference implementation of the Jupyter protocol.\nIt also provides client and kernel management APIs for working with kernels.\nIt also provides the jupyter kernelspec entrypoint\nfor installing kernelspecs for use with Jupyter frontends.\nDevelopment Setup\nThe Jupyter Contributor Guides provide extensive information on contributing code or documentation to Jupyter projects. The limited instructions below for setting up a development environment are for your convenience.\nCoding\nYou'll need Python and pip on the search path. Clone the Jupyter Client git repository to your computer, for example in /my/project/jupyter_client\ncd /my/projects/\ngit clone git@github.com:jupyter/jupyter_client.git\nNow create an editable install\nand download the dependencies of code and test suite by executing:\ncd /my/projects/jupyter_client/\npip install -e \".[test]\"\npytest\nThe last command runs the test suite to verify the setup. During development, you can pass filenames to pytest, and it will execute only those tests.\nDocumentation\nThe documentation of Jupyter Client is generated from the files in docs/ using Sphinx. Instructions for setting up Sphinx with a selection of optional modules are in the Documentation Guide. You'll also need the make command.\nFor a minimal Sphinx installation to process the Jupyter Client docs, execute:\npip install \".[doc]\"\nThe following commands build the documentation in HTML format and check for broken links:\ncd /my/projects/jupyter_client/docs/\nmake html linkcheck\nPoint your browser to the following URL to access the generated documentation:\nfile:///my/projects/jupyter_client/docs/_build/html/index.html\nContributing\njupyter-client has adopted automatic code formatting so you shouldn't\nneed to worry too much about your code style.\nAs long as your code is valid,\nthe pre-commit hook should take care of how it should look.\nYou can invoke the pre-commit hook by hand at any time with:\npre-commit run\nwhich should run any autoformatting on your code\nand tell you about any errors it couldn't fix automatically.\nYou may also install black integration\ninto your text editor to format code automatically.\nIf you have already committed files before setting up the pre-commit\nhook with pre-commit install, you can fix everything up using\npre-commit run --all-files. You need to make the fixing commit\nyourself after that.\nSome of the hooks only run on CI by default, but you can invoke them by\nrunning with the --hook-stage manual argument.\nAbout the Jupyter Development Team\nThe Jupyter Development Team is the set of all contributors to the Jupyter project.\nThis includes all of the Jupyter subprojects.\nThe core team that coordinates development on GitHub can be found here:\nhttps://github.com/jupyter/.\nOur Copyright Policy\nJupyter uses a shared copyright model. Each contributor maintains copyright\nover their contributions to Jupyter. But, it is important to note that these\ncontributions are typically only changes to the repositories. Thus, the Jupyter\nsource code, in its entirety is not the copyright of any single person or\ninstitution. Instead, it is the collective copyright of the entire Jupyter\nDevelopment Team. If individual contributors want to maintain a record of what\nchanges/contributions they have specific copyright on, they should indicate\ntheir copyright in the commit message of the change, when they commit the\nchange to one of the Jupyter repositories.\nWith this in mind, the following banner should be used in any source code file\nto indicate the copyright and license terms:\n# Copyright (c) Jupyter Development Team.\n# Distributed under the terms of the Modified BSD License.\n\n\n\n"}, {"name": "jsonschema-specifications", "readme": "\n   \nJSON support files from the JSON Schema Specifications (metaschemas, vocabularies, etc.), packaged for runtime access from Python as a referencing-based Schema Registry.\n"}, {"name": "ipython-genutils", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nWarehouse\nGetting Started\nDiscussion\nTesting\nCode of Conduct\n\n\n\n\n\nREADME.rst\n\n\n\n\nWarehouse\nWarehouse is the software that powers PyPI.\nSee our development roadmap, documentation, and\narchitectural overview.\n\nGetting Started\nYou can run Warehouse locally in a development environment using\ndocker. See Getting started\ndocumentation for instructions on how to set it up.\nThe canonical deployment of Warehouse is in production at pypi.org.\n\nDiscussion\nYou can find help or get involved on:\n\nGithub issue tracker for reporting issues\nIRC: on Libera, channel #pypa for general packaging discussion\nand user support, and #pypa-dev for\ndiscussions about development of packaging tools\nThe PyPA Discord for live discussions\nThe Packaging category on Discourse for discussing\nnew ideas and community initiatives\n\n\nTesting\nRead the running tests and linters section of our documentation to\nlearn how to test your code.  For cross-browser testing, we use an\nopen source account from BrowserStack. If your pull request makes\nany change to the user interface, it will need to be tested to confirm\nit works in our supported browsers.\n\n\nCode of Conduct\nEveryone interacting in the Warehouse project's codebases, issue trackers, chat\nrooms, and mailing lists is expected to follow the PSF Code of Conduct.\n\n\n"}, {"name": "ipykernel", "readme": "\nIPython Kernel for Jupyter\n\n\nThis package provides the IPython kernel for Jupyter.\nInstallation from source\n\ngit clone\ncd ipykernel\npip install -e \".[test]\"\n\nAfter that, all normal ipython commands will use this newly-installed version of the kernel.\nRunning tests\nFollow the instructions from Installation from source.\nand then from the root directory\npytest ipykernel\n\nRunning tests with coverage\nFollow the instructions from Installation from source.\nand then from the root directory\npytest ipykernel -vv -s --cov ipykernel --cov-branch --cov-report term-missing:skip-covered --durations 10\n\nAbout the IPython Development Team\nThe IPython Development Team is the set of all contributors to the IPython project.\nThis includes all of the IPython subprojects.\nThe core team that coordinates development on GitHub can be found here:\nhttps://github.com/ipython/.\nOur Copyright Policy\nIPython uses a shared copyright model. Each contributor maintains copyright\nover their contributions to IPython. But, it is important to note that these\ncontributions are typically only changes to the repositories. Thus, the IPython\nsource code, in its entirety is not the copyright of any single person or\ninstitution. Instead, it is the collective copyright of the entire IPython\nDevelopment Team. If individual contributors want to maintain a record of what\nchanges/contributions they have specific copyright on, they should indicate\ntheir copyright in the commit message of the change, when they commit the\nchange to one of the IPython repositories.\nWith this in mind, the following banner should be used in any source code file\nto indicate the copyright and license terms:\n# Copyright (c) IPython Development Team.\n# Distributed under the terms of the Modified BSD License.\n\n"}, {"name": "importlib-resources", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nCompatibility\nFor Enterprise\n\n\n\n\n\nREADME.rst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimportlib_resources is a backport of Python standard library\nimportlib.resources\nmodule for older Pythons.\nThe key goal of this module is to replace parts of pkg_resources with a\nsolution in Python's stdlib that relies on well-defined APIs.  This makes\nreading resources included in packages easier, with more stable and consistent\nsemantics.\n\nCompatibility\nNew features are introduced in this third-party library and later merged\ninto CPython. The following table indicates which versions of this library\nwere contributed to different versions in the standard library:\n\n\nimportlib_resources\nstdlib\n\n\n\n6.0\n3.13\n\n5.12\n3.12\n\n5.7\n3.11\n\n5.0\n3.10\n\n1.3\n3.9\n\n0.5 (?)\n3.7\n\n\n\n\nFor Enterprise\nAvailable as part of the Tidelift Subscription.\nThis project and the maintainers of thousands of other packages are working with Tidelift to deliver one enterprise subscription that covers all of the open source you use.\nLearn more.\n\n\n"}, {"name": "importlib-metadata", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nCompatibility\nUsage\nCaveats\nProject details\nFor Enterprise\n\n\n\n\n\nREADME.rst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLibrary to access the metadata for a Python package.\nThis package supplies third-party access to the functionality of\nimportlib.metadata\nincluding improvements added to subsequent Python versions.\n\nCompatibility\nNew features are introduced in this third-party library and later merged\ninto CPython. The following table indicates which versions of this library\nwere contributed to different versions in the standard library:\n\n\nimportlib_metadata\nstdlib\n\n\n\n6.5\n3.12\n\n4.13\n3.11\n\n4.6\n3.10\n\n1.4\n3.8\n\n\n\n\nUsage\nSee the online documentation\nfor usage details.\nFinder authors can\nalso add support for custom package installers.  See the above documentation\nfor details.\n\nCaveats\nThis project primarily supports third-party packages installed by PyPA\ntools (or other conforming packages). It does not support:\n\nPackages in the stdlib.\nPackages installed without metadata.\n\n\nProject details\n\n\nProject home: https://github.com/python/importlib_metadata\nReport bugs at: https://github.com/python/importlib_metadata/issues\nCode hosting: https://github.com/python/importlib_metadata\nDocumentation: https://importlib-metadata.readthedocs.io/\n\n\n\nFor Enterprise\nAvailable as part of the Tidelift Subscription.\nThis project and the maintainers of thousands of other packages are working with Tidelift to deliver one enterprise subscription that covers all of the open source you use.\nLearn more.\n\n\n"}, {"name": "IMAPClient", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEssentials\nFeatures\nExample\nWhy IMAPClient?\nInstalling IMAPClient\nDocumentation\nCurrent Status\nDiscussions\nWorking on IMAPClient\nIMAP Servers\nInteractive Console\n\"Live\" Tests\n\n\n\n\n\nREADME.rst\n\n\n\n\n\nEssentials\nIMAPClient is an easy-to-use, Pythonic and complete IMAP client\nlibrary.\n\n\nCurrent version\n2.3.1\n\nSupported Python versions\n3.7 - 3.11\n\nLicense\nNew BSD\n\nProject home\nhttps://github.com/mjs/imapclient/\n\nPyPI\nhttps://pypi.python.org/pypi/IMAPClient\n\nDocumentation\nhttps://imapclient.readthedocs.io/\n\nDiscussions\nhttps://github.com/mjs/imapclient/discussions\n\nTest Status\n\n\n\n\n\n\nFeatures\n\nArguments and return values are natural Python types.\nIMAP server responses are fully parsed and readily usable.\nIMAP unique message IDs (UIDs) are handled transparently. There is\nno need to call different methods to use UIDs.\nEscaping for internationalised mailbox names is transparently\nhandled.  Unicode mailbox names may be passed as input wherever a\nfolder name is accepted.\nTime zones are transparently handled including when the server and\nclient are in different zones.\nConvenience methods are provided for commonly used functionality.\nExceptions are raised when errors occur.\n\n\nExample\nfrom imapclient import IMAPClient\n\n# context manager ensures the session is cleaned up\nwith IMAPClient(host=\"imap.host.org\") as client:\n    client.login('someone', 'secret')\n    client.select_folder('INBOX')\n\n    # search criteria are passed in a straightforward way\n    # (nesting is supported)\n    messages = client.search(['NOT', 'DELETED'])\n\n    # fetch selectors are passed as a simple list of strings.\n    response = client.fetch(messages, ['FLAGS', 'RFC822.SIZE'])\n\n    # `response` is keyed by message id and contains parsed,\n    # converted response items.\n    for message_id, data in response.items():\n        print('{id}: {size} bytes, flags={flags}'.format(\n            id=message_id,\n            size=data[b'RFC822.SIZE'],\n            flags=data[b'FLAGS']))\n\nWhy IMAPClient?\nYou may ask: \"why create another IMAP client library for Python?\nDoesn't the Python standard library already have imaplib?\".\nThe problem with imaplib is that it's very low-level. It expects\nstring values where lists or tuples would be more appropriate and\nreturns server responses almost unparsed. As IMAP server responses can\nbe quite complex this means everyone using imaplib ends up writing\ntheir own fragile parsing routines.\nAlso, imaplib doesn't make good use of exceptions. This means you need\nto check the return value of each call to imaplib to see if what you\njust did was successful.\nIMAPClient actually uses imaplib internally. This may change at some\npoint in the future.\n\nInstalling IMAPClient\nIMAPClient is listed on PyPI and can be installed with pip:\npip install imapclient\n\nMore installation methods are described in the documentation.\n\nDocumentation\nIMAPClient's manual is available at http://imapclient.readthedocs.io/.\nRelease notes can be found at\nhttp://imapclient.readthedocs.io/#release-history.\nSee the examples directory in the root of project source for\nexamples of how to use IMAPClient.\n\nCurrent Status\nYou should feel confident using IMAPClient for production purposes.\nIn order to clearly communicate version compatibility, IMAPClient\nwill strictly adhere to the Semantic Versioning\nscheme from version 1.0 onwards.\nThe project's home page is https://github.com/mjs/imapclient/ (this\ncurrently redirects to the IMAPClient Github site). Details about\nupcoming versions and planned features/fixes can be found in the issue\ntracker on Github. The maintainers also blog about IMAPClient\nnews. Those articles can be found here.\n\nDiscussions\nGithub Discussions can be used to ask questions, propose changes or praise\nthe project maintainers :)\n\nWorking on IMAPClient\nThe contributing documentation contains\ninformation for those interested in improving IMAPClient.\n\nIMAP Servers\nIMAPClient is heavily tested against Dovecot, Gmail, Fastmail.fm\n(who use a modified Cyrus implementation), Office365 and Yahoo. Access\nto accounts on other IMAP servers/services for testing would be\ngreatly appreciated.\n\nInteractive Console\nThis script connects an IMAPClient instance using the command line\nargs given and starts an interactive session. This is useful for\nexploring the IMAPClient API and testing things out, avoiding the\nsteps required to set up an IMAPClient instance.\nThe IPython shell is used if it is installed. Otherwise the\ncode.interact() function from the standard library is used.\nThe interactive console functionality can be accessed running the\ninteract.py script in the root of the source tree or by invoking the\ninteract module like this:\npython -m imapclient.interact ...\n\n\n\"Live\" Tests\nIMAPClient includes a series of live, functional tests which exercise\nit against a live IMAP account. These are useful for ensuring\ncompatibility with a given IMAP server implementation.\nThe livetest functionality are run from the root of the project source\nlike this:\npython livetest.py <livetest.ini> [ optional unittest arguments ]\n\nThe configuration file format is\ndescribed in the main documentation.\nWARNING: The operations used by livetest are destructive and could\ncause unintended loss of data. That said, as of version 0.9, livetest\nlimits its activity to a folder it creates and subfolders of that\nfolder. It should be safe to use with any IMAP account but please\ndon't run livetest against a truly important IMAP account.\nPlease include the output of livetest.py with an issue if it fails\nto run successfully against a particular IMAP server. Reports of\nsuccessful runs are also welcome.  Please include the type and version\nof the IMAP server, if known.\n\n\n"}, {"name": "imageio", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIMAGEIO\nExample\nAPI in a nutshell\nFeatures\nDependencies\nCiting imageio\nSecurity contact information\nImageIO for enterprise\nDetails\nContributing\n\n\n\n\n\nREADME.md\n\n\n\n\nIMAGEIO\n\n\n\n\n\n\n\n\nWebsite: https://imageio.readthedocs.io/\n\nImageio is a Python library that provides an easy interface to read and\nwrite a wide range of image data, including animated images, video,\nvolumetric data, and scientific formats. It is cross-platform, runs on\nPython 3.8+, and is easy to install.\n\n\n    Professional support is available via Tidelift.\n\nExample\nHere's a minimal example of how to use imageio. See the docs for\nmore examples.\nimport imageio.v3 as iio\nim = iio.imread('imageio:chelsea.png')  # read a standard image\nim.shape  # im is a NumPy array of shape (300, 451, 3)\niio.imwrite('chelsea.jpg', im)  # convert to jpg\nAPI in a nutshell\nAs a user, you just have to remember a handful of functions:\n\nimread() - for reading\nimwrite() - for writing\nimiter() - for iterating image series (animations/videos/OME-TIFF/...)\nimprops() - for standardized metadata\nimmeta() - for format-specific metadata\nimopen() - for advanced usage\n\nSee the API docs for more information.\nFeatures\n\nSimple interface via a concise set of functions\nEasy to install using Conda or pip\nFew dependencies (only NumPy and Pillow)\nPure Python, runs on Python 3.8+, and PyPy\nCross platform, runs on Windows, Linux, macOS\nMore than 295 supported formats\nRead/Write support for various resources (files, URLs, bytes, FileLike objects, ...)\nCode quality is maintained via continuous integration and continuous deployment\n\nDependencies\nMinimal requirements:\n\nPython 3.8+\nNumPy\nPillow >= 8.3.2\n\nOptional Python packages:\n\nimageio-ffmpeg (for working with video files)\npyav (for working with video files)\ntifffile (for working with TIFF files)\nitk or SimpleITK (for ITK plugin)\nastropy (for FITS plugin)\nimageio-flif (for working with FLIF image files)\n\nCiting imageio\n\nIf you use imageio for scientific work, we would appreciate a citation.\nWe have a DOI!\n\nSecurity contact information\nTo report a security vulnerability, please use the\nTidelift security contact.\nTidelift will coordinate the fix and disclosure.\nImageIO for enterprise\nAvailable as part of the Tidelift Subscription.\nThe maintainers of imageio and thousands of other packages are working with Tidelift to deliver commercial support and maintenance for the open source dependencies you use to build your applications. Save time, reduce risk, and improve code health, while paying the maintainers of the exact dependencies you use.\nLearn more.\nDetails\n\n    The core of ImageIO is a set of user-facing APIs combined with a plugin manager. API calls choose sensible defaults and then call the plugin manager, which deduces the correct plugin/backend to use for the given resource and file format. The plugin manager then adds sensible backend-specific defaults and then calls one of ImageIOs many backends to perform the actual loading. This allows ImageIO to take care of most of the gory details of loading images for you, while still allowing you to customize the behavior when and where you need to. You can find a more detailed explanation of this process in our documentation.\nContributing\nWe welcome contributions of any kind. Here are some suggestions on how you are able to contribute\n\nadd missing formats to the format list\nsuggest/implement support for new backends\nreport/fix any bugs you encounter while using ImageIO\n\nTo assist you in getting started with contributing code, take a look at the development section of the docs. You will find instructions on setting up the dev environment as well as examples on how to contribute code.\n\n\n"}, {"name": "imageio-ffmpeg", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimageio-ffmpeg\nPurpose\nInstallation\nExample usage\nHow it works\nimageio-ffmpeg for enterprise\nSecurity contact information\nEnvironment variables\nDevelopers\nAPI\n\n\n\n\n\nREADME.md\n\n\n\n\nimageio-ffmpeg\n\n\nFFMPEG wrapper for Python\nPurpose\nThe purpose of this project is to provide a simple and reliable ffmpeg\nwrapper for working with video files. It implements two simple generator\nfunctions for reading and writing data from/to ffmpeg, which reliably\nterminate the ffmpeg process when done. It also takes care of publishing\nplatform-specific wheels that include the binary ffmpeg executables.\nThis library is used as the basis for the\nimageio\nffmpeg plugin,\nbut it can also be used by itself. Imageio provides a higher level API,\nand adds support for e.g. cameras and seeking.\nInstallation\nThis library works with any version of Python 3.5+ (including Pypy).\nThere are no further dependencies. The wheels on Pypi include the ffmpeg\nexecutable for all common platforms (Windows 7+, Linux kernel 2.6.32+,\nOSX 10.9+). Install using:\n$ pip install --upgrade imageio-ffmpeg\n\n(On Linux you may want to first pip install -U pip, since pip 19 is needed to detect the manylinux2010 wheels.)\nIf you're using a Conda environment: the conda package does not include\nthe ffmpeg executable, but instead depends on the ffmpeg package from\nconda-forge. Install using:\n$ conda install imageio-ffmpeg -c conda-forge\n\nIf you don't want to install the included ffmpeg, you can use pip with\n--no-binary or conda with --no-deps. Then use the\nIMAGEIO_FFMPEG_EXE environment variable if needed.\nExample usage\nThe imageio_ffmpeg library provides low level functionality to read\nand write video data, using Python generators:\n# Read a video file\nreader = read_frames(path)\nmeta = reader.__next__()  # meta data, e.g. meta[\"size\"] -> (width, height)\nfor frame in reader:\n    ... # each frame is a bytes object\n\n# Write a video file\nwriter = write_frames(path, size)  # size is (width, height)\nwriter.send(None)  # seed the generator\nfor frame in frames:\n    writer.send(frame)\nwriter.close()  # don't forget this\n(Also see the API section further down.)\nHow it works\nThis library calls ffmpeg in a subprocess, and video frames are\ncommunicated over pipes. This is certainly not the fastest way to\nuse ffmpeg, but it makes it possible to wrap ffmpeg with pure Python,\nmaking distribution and installation much easier. And probably\nthe code itself too. In contrast, PyAV\nwraps ffmpeg at the C level.\nNote that because of how imageio-ffmpeg works, read_frames() and\nwrite_frames() only accept file names, and not file (like) objects.\nimageio-ffmpeg for enterprise\nAvailable as part of the Tidelift Subscription\nThe maintainers of imageio-ffmpeg and thousands of other packages are working with Tidelift to deliver commercial support and maintenance for the open source dependencies you use to build your applications. Save time, reduce risk, and improve code health, while paying the maintainers of the exact dependencies you use. Learn more.\nSecurity contact information\nTo report a security vulnerability, please use the\nTidelift security contact.\nTidelift will coordinate the fix and disclosure.\nEnvironment variables\nThe library can be configured at runtime by setting the following environment\nvariables:\n\nIMAGEIO_FFMPEG_EXE=[file name] -- override the ffmpeg executable;\nIMAGEIO_FFMPEG_NO_PREVENT_SIGINT=1 -- don't prevent propagation of SIGINT\nto the ffmpeg process.\n\nDevelopers\nDev deps:\npip install invoke black flake8\n\nWe use invoke:\ninvoke autoformat\ninvoke lint\ninvoke -l  # to get a list of all tasks\ninvoke update-readme  # after changes to the docstrings\n\nAPI\ndef read_frames(\n    path,\n    pix_fmt=\"rgb24\",\n    bpp=None,\n    input_params=None,\n    output_params=None,\n    bits_per_pixel=None,\n):\n    \"\"\"\n    Create a generator to iterate over the frames in a video file.\n\n    It first yields a small metadata dictionary that contains:\n\n    * ffmpeg_version: the ffmpeg version in use (as a string).\n    * codec: a hint about the codec used to encode the video, e.g. \"h264\".\n    * source_size: the width and height of the encoded video frames.\n    * size: the width and height of the frames that will be produced.\n    * fps: the frames per second. Can be zero if it could not be detected.\n    * duration: duration in seconds. Can be zero if it could not be detected.\n\n    After that, it yields frames until the end of the video is reached. Each\n    frame is a bytes object.\n\n    This function makes no assumptions about the number of frames in\n    the data. For one because this is hard to predict exactly, but also\n    because it may depend on the provided output_params. If you want\n    to know the number of frames in a video file, use count_frames_and_secs().\n    It is also possible to estimate the number of frames from the fps and\n    duration, but note that even if both numbers are present, the resulting\n    value is not always correct.\n\n    Example:\n\n        gen = read_frames(path)\n        meta = gen.__next__()\n        for frame in gen:\n            print(len(frame))\n\n    Parameters:\n        path (str): the filename of the file to read from.\n        pix_fmt (str): the pixel format of the frames to be read.\n            The default is \"rgb24\" (frames are uint8 RGB images).\n        input_params (list): Additional ffmpeg input command line parameters.\n        output_params (list): Additional ffmpeg output command line parameters.\n        bits_per_pixel (int): The number of bits per pixel in the output frames.\n            This depends on the given pix_fmt. Default is 24 (RGB)\n        bpp (int): DEPRECATED, USE bits_per_pixel INSTEAD. The number of bytes per pixel in the output frames.\n            This depends on the given pix_fmt. Some pixel formats like yuv420p have 12 bits per pixel\n            and cannot be set in bytes as integer. For this reason the bpp argument is deprecated.\n    \"\"\"\ndef write_frames(\n    path,\n    size,\n    pix_fmt_in=\"rgb24\",\n    pix_fmt_out=\"yuv420p\",\n    fps=16,\n    quality=5,\n    bitrate=None,\n    codec=None,\n    macro_block_size=16,\n    ffmpeg_log_level=\"warning\",\n    ffmpeg_timeout=None,\n    input_params=None,\n    output_params=None,\n    audio_path=None,\n    audio_codec=None,\n):\n    \"\"\"\n    Create a generator to write frames (bytes objects) into a video file.\n\n    The frames are written by using the generator's `send()` method. Frames\n    can be anything that can be written to a file. Typically these are\n    bytes objects, but c-contiguous Numpy arrays also work.\n\n    Example:\n\n        gen = write_frames(path, size)\n        gen.send(None)  # seed the generator\n        for frame in frames:\n            gen.send(frame)\n        gen.close()  # don't forget this\n\n    Parameters:\n        path (str): the filename to write to.\n        size (tuple): the width and height of the frames.\n        pix_fmt_in (str): the pixel format of incoming frames.\n            E.g. \"gray\", \"gray8a\", \"rgb24\", or \"rgba\". Default \"rgb24\".\n        pix_fmt_out (str): the pixel format to store frames. Default yuv420p\".\n        fps (float): The frames per second. Default 16.\n        quality (float): A measure for quality between 0 and 10. Default 5.\n            Ignored if bitrate is given.\n        bitrate (str): The bitrate, e.g. \"192k\". The defaults are pretty good.\n        codec (str): The codec. Default \"libx264\" for .mp4 (if available from\n            the ffmpeg executable) or \"msmpeg4\" for .wmv.\n        macro_block_size (int): You probably want to align the size of frames\n            to this value to avoid image resizing. Default 16. Can be set\n            to 1 to avoid block alignment, though this is not recommended.\n        ffmpeg_log_level (str): The ffmpeg logging level. Default \"warning\".\n        ffmpeg_timeout (float): Timeout in seconds to wait for ffmpeg process\n            to finish. Value of 0 or None will wait forever (default). The time that\n            ffmpeg needs depends on CPU speed, compression, and frame size.\n        input_params (list): Additional ffmpeg input command line parameters.\n        output_params (list): Additional ffmpeg output command line parameters.\n        audio_path (str): A input file path for encoding with an audio stream.\n            Default None, no audio.\n        audio_codec (str): The audio codec to use if audio_path is provided.\n            \"copy\" will try to use audio_path's audio codec without re-encoding.\n            Default None, but some formats must have certain codecs specified.\n    \"\"\"\ndef count_frames_and_secs(path):\n    \"\"\"\n    Get the number of frames and number of seconds for the given video\n    file. Note that this operation can be quite slow for large files.\n\n    Disclaimer: I've seen this produce different results from actually reading\n    the frames with older versions of ffmpeg (2.x). Therefore I cannot say\n    with 100% certainty that the returned values are always exact.\n    \"\"\"\ndef get_ffmpeg_exe():\n    \"\"\"\n    Get the ffmpeg executable file. This can be the binary defined by\n    the IMAGEIO_FFMPEG_EXE environment variable, the binary distributed\n    with imageio-ffmpeg, an ffmpeg binary installed with conda, or the\n    system ffmpeg (in that order). A RuntimeError is raised if no valid\n    ffmpeg could be found.\n    \"\"\"\ndef get_ffmpeg_version():\n    \"\"\"\n    Get the version of the used ffmpeg executable (as a string).\n    \"\"\"\n\n\n"}, {"name": "h2", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nh2: HTTP/2 Protocol Stack\nDocumentation\nContributing\nLicense\nAuthors\n\n\n\n\n\nREADME.rst\n\n\n\n\nh2: HTTP/2 Protocol Stack\n\n\n\n\n\n\n\n\nThis repository contains a pure-Python implementation of a HTTP/2 protocol\nstack. It's written from the ground up to be embeddable in whatever program you\nchoose to use, ensuring that you can speak HTTP/2 regardless of your\nprogramming paradigm.\nYou use it like this:\nimport h2.connection\nimport h2.config\n\nconfig = h2.config.H2Configuration()\nconn = h2.connection.H2Connection(config=config)\nconn.send_headers(stream_id=stream_id, headers=headers)\nconn.send_data(stream_id, data)\nsocket.sendall(conn.data_to_send())\nevents = conn.receive_data(socket_data)\nThis repository does not provide a parsing layer, a network layer, or any rules\nabout concurrency. Instead, it's a purely in-memory solution, defined in terms\nof data actions and HTTP/2 frames. This is one building block of a full Python\nHTTP implementation.\nTo install it, just run:\n$ python -m pip install h2\n\nDocumentation\nDocumentation is available at https://h2.readthedocs.io .\n\nContributing\nh2 welcomes contributions from anyone! Unlike many other projects we\nare happy to accept cosmetic contributions and small contributions, in addition\nto large feature requests and changes.\nBefore you contribute (either by opening an issue or filing a pull request),\nplease read the contribution guidelines.\n\nLicense\nh2 is made available under the MIT License. For more details, see the\nLICENSE file in the repository.\n\nAuthors\nh2 was authored by Cory Benfield and is maintained\nby the members of python-hyper.\n\n\n"}, {"name": "gTTS", "readme": "\ngTTS\ngTTS (Google Text-to-Speech), a Python library and CLI tool to interface with Google Translate's text-to-speech API.\nWrite spoken mp3 data to a file, a file-like object (bytestring) for further audio manipulation, or stdout.\nhttp://gtts.readthedocs.org/\n\n\n\n\n\n\n\nFeatures\n\nCustomizable speech-specific sentence tokenizer that allows for unlimited lengths of text to be read, all while keeping proper intonation, abbreviations, decimals and more;\nCustomizable text pre-processors which can, for example, provide pronunciation corrections;\n\nInstallation\n$ pip install gTTS\n\nQuickstart\nCommand Line:\n$ gtts-cli 'hello' --output hello.mp3\n\nModule:\n>>> from gtts import gTTS\n>>> tts = gTTS('hello')\n>>> tts.save('hello.mp3')\n\nSee http://gtts.readthedocs.org/ for documentation and examples.\nDisclaimer\nThis project is not affiliated with Google or Google Cloud. Breaking upstream changes can occur without notice. This project is leveraging the undocumented Google Translate speech functionality and is different from Google Cloud Text-to-Speech.\nProject\n\nQuestions & community\nChangelog\nContributing\n\nLicence\nThe MIT License (MIT) Copyright \u00a9 2014-2023 Pierre Nicolas Durette & Contributors\n"}, {"name": "Flask-Login", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nFlask-Login\nInstallation\nUsage\nContributing\n\n\n\n\n\nREADME.md\n\n\n\n\nFlask-Login\n\n\n\nFlask-Login provides user session management for Flask. It handles the common\ntasks of logging in, logging out, and remembering your users' sessions over\nextended periods of time.\nFlask-Login is not bound to any particular database system or permissions\nmodel. The only requirement is that your user objects implement a few methods,\nand that you provide a callback to the extension capable of loading users from\ntheir ID.\nInstallation\nInstall the extension with pip:\n$ pip install flask-login\nUsage\nOnce installed, the Flask-Login is easy to use. Let's walk through setting up\na basic application. Also please note that this is a very basic guide: we will\nbe taking shortcuts here that you should never take in a real application.\nTo begin we'll set up a Flask app:\nimport flask\n\napp = flask.Flask(__name__)\napp.secret_key = 'super secret string'  # Change this!\nFlask-Login works via a login manager. To kick things off, we'll set up the\nlogin manager by instantiating it and telling it about our Flask app:\nimport flask_login\n\nlogin_manager = flask_login.LoginManager()\n\nlogin_manager.init_app(app)\nTo keep things simple we're going to use a dictionary to represent a database\nof users. In a real application, this would be an actual persistence layer.\nHowever it's important to point out this is a feature of Flask-Login: it\ndoesn't care how your data is stored so long as you tell it how to retrieve it!\n# Our mock database.\nusers = {'foo@bar.tld': {'password': 'secret'}}\nWe also need to tell Flask-Login how to load a user from a Flask request and\nfrom its session. To do this we need to define our user object, a\nuser_loader callback, and a request_loader callback.\nclass User(flask_login.UserMixin):\n    pass\n\n\n@login_manager.user_loader\ndef user_loader(email):\n    if email not in users:\n        return\n\n    user = User()\n    user.id = email\n    return user\n\n\n@login_manager.request_loader\ndef request_loader(request):\n    email = request.form.get('email')\n    if email not in users:\n        return\n\n    user = User()\n    user.id = email\n    return user\nNow we're ready to define our views. We can start with a login view, which will\npopulate the session with authentication bits. After that we can define a view\nthat requires authentication.\n@app.route('/login', methods=['GET', 'POST'])\ndef login():\n    if flask.request.method == 'GET':\n        return '''\n               <form action='login' method='POST'>\n                <input type='text' name='email' id='email' placeholder='email'/>\n                <input type='password' name='password' id='password' placeholder='password'/>\n                <input type='submit' name='submit'/>\n               </form>\n               '''\n\n    email = flask.request.form['email']\n    if email in users and flask.request.form['password'] == users[email]['password']:\n        user = User()\n        user.id = email\n        flask_login.login_user(user)\n        return flask.redirect(flask.url_for('protected'))\n\n    return 'Bad login'\n\n\n@app.route('/protected')\n@flask_login.login_required\ndef protected():\n    return 'Logged in as: ' + flask_login.current_user.id\nFinally we can define a view to clear the session and log users out:\n@app.route('/logout')\ndef logout():\n    flask_login.logout_user()\n    return 'Logged out'\nWe now have a basic working application that makes use of session-based\nauthentication. To round things off, we should provide a callback for login\nfailures:\n@login_manager.unauthorized_handler\ndef unauthorized_handler():\n    return 'Unauthorized', 401\nDocumentation for Flask-Login is available on ReadTheDocs.\nFor complete understanding of available configuration, please refer to the source code.\nContributing\nWe welcome contributions! If you would like to hack on Flask-Login, please\nfollow these steps:\n\nFork this repository\nMake your changes\nInstall the dev requirements with pip install -r requirements/dev.txt\nSubmit a pull request after running tox (ensure it does not error!)\n\nPlease give us adequate time to review your submission. Thanks!\n\n\n"}, {"name": "Flask-Cors", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFlask-CORS\nInstallation\nUsage\nSimple Usage\nResource specific CORS\nRoute specific CORS via decorator\nDocumentation\nTroubleshooting\nTests\nContributing\nCredits\n\n\n\n\n\nREADME.rst\n\n\n\n\nFlask-CORS\n\n \n \n\n\n\nA Flask extension for handling Cross Origin Resource Sharing (CORS), making cross-origin AJAX possible.\nThis package has a simple philosophy: when you want to enable CORS, you wish to enable it for all use cases on a domain.\nThis means no mucking around with different allowed headers, methods, etc.\nBy default, submission of cookies across domains is disabled due to the security implications.\nPlease see the documentation for how to enable credential'ed requests, and please make sure you add some sort of CSRF protection before doing so!\n\nInstallation\nInstall the extension with using pip, or easy_install.\n$ pip install -U flask-cors\n\nUsage\nThis package exposes a Flask extension which by default enables CORS support on all routes, for all origins and methods.\nIt allows parameterization of all CORS headers on a per-resource level.\nThe package also contains a decorator, for those who prefer this approach.\n\nSimple Usage\nIn the simplest case, initialize the Flask-Cors extension with default arguments in order to allow CORS for all domains on all routes.\nSee the full list of options in the documentation.\nfrom flask import Flask\nfrom flask_cors import CORS\n\napp = Flask(__name__)\nCORS(app)\n\n@app.route(\"/\")\ndef helloWorld():\n  return \"Hello, cross-origin-world!\"\n\nResource specific CORS\nAlternatively, you can specify CORS options on a resource and origin level of granularity by passing a dictionary as the resources option, mapping paths to a set of options.\nSee the full list of options in the documentation.\napp = Flask(__name__)\ncors = CORS(app, resources={r\"/api/*\": {\"origins\": \"*\"}})\n\n@app.route(\"/api/v1/users\")\ndef list_users():\n  return \"user example\"\n\nRoute specific CORS via decorator\nThis extension also exposes a simple decorator to decorate flask routes with.\nSimply add @cross_origin() below a call to Flask's @app.route(..) to allow CORS on a given route.\nSee the full list of options in the decorator documentation.\n@app.route(\"/\")\n@cross_origin()\ndef helloWorld():\n  return \"Hello, cross-origin-world!\"\n\nDocumentation\nFor a full list of options, please see the full documentation\n\nTroubleshooting\nIf things aren't working as you expect, enable logging to help understand what is going on under the hood, and why.\nlogging.getLogger('flask_cors').level = logging.DEBUG\n\nTests\nA simple set of tests is included in test/.\nTo run, install nose, and simply invoke nosetests or python setup.py test to exercise the tests.\nIf nosetests does not work for you, due to it no longer working with newer python versions.\nYou can use pytest to run the tests instead.\n\nContributing\nQuestions, comments or improvements?\nPlease create an issue on Github, tweet at @corydolphin or send me an email.\nI do my best to include every contribution proposed in any way that I can.\n\nCredits\nThis Flask extension is based upon the Decorator for the HTTP Access Control written by Armin Ronacher.\n\n\n"}, {"name": "Flask-CacheBuster", "readme": "\n\nflask-cachebuster\nFlask-CacheBuster is a lightweight http://flask.pocoo.org/ extension that adds a hash to the URL query parameters of each static file. This lets you safely declare your static resources as indefinitely cacheable because they automatically get new URLs when their contents change.\n\n\nNotes:\nInspired by https://github.com/ChrisTM/Flask-CacheBust, and an updated version of https://github.com/daxlab/Flask-Cache-Buster to work with python 3.+\n\n\nInstallation\nUsing pip:\npip install flask-cachebuster\n\nUsage\nConfiguration:\nfrom flask_cachebuster import CacheBuster\n\nconfig = { 'extensions': ['.js', '.css', '.csv'], 'hash_size': 5 }\n\ncache_buster = CacheBuster(config=config)\n\ncache_buster.init_app(app)\n\n\nConfiguration\nConfiguration:\n* extensions - file extensions to bust\n* hash_size - looks something like this `/static/index.css%3Fq3` where [%3Fq3] is the hash size.\nThe http://flask.pocoo.org/docs/0.12/api/#flask.url_for function will now cache-bust your static files. For example, this template:\n<script src=\"{{ url_for('static', filename='js/main.js') }}\"></script>\nwill render like this:\n<script src=\"/static/js/main.js?%3Fq%3Dc5b5b2fa19\"></script>\n\n\n"}, {"name": "ffmpeg-python", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nffmpeg-python: Python bindings for FFmpeg\nOverview\nQuickstart\nAPI reference\nComplex filter graphs\nInstallation\nInstalling ffmpeg-python\nInstalling FFmpeg\nExamples\nCustom Filters\nFrequently asked questions\nContributing\nRunning tests\nSpecial thanks\nAdditional Resources\n\n\n\n\n\nREADME.md\n\n\n\n\nffmpeg-python: Python bindings for FFmpeg\n\n\nOverview\nThere are tons of Python FFmpeg wrappers out there but they seem to lack complex filter support.  ffmpeg-python works well for simple as well as complex signal graphs.\nQuickstart\nFlip a video horizontally:\nimport ffmpeg\nstream = ffmpeg.input('input.mp4')\nstream = ffmpeg.hflip(stream)\nstream = ffmpeg.output(stream, 'output.mp4')\nffmpeg.run(stream)\nOr if you prefer a fluent interface:\nimport ffmpeg\n(\n    ffmpeg\n    .input('input.mp4')\n    .hflip()\n    .output('output.mp4')\n    .run()\n)\nAPI reference\nComplex filter graphs\nFFmpeg is extremely powerful, but its command-line interface gets really complicated rather quickly - especially when working with signal graphs and doing anything more than trivial.\nTake for example a signal graph that looks like this:\n\nThe corresponding command-line arguments are pretty gnarly:\nffmpeg -i input.mp4 -i overlay.png -filter_complex \"[0]trim=start_frame=10:end_frame=20[v0];\\\n    [0]trim=start_frame=30:end_frame=40[v1];[v0][v1]concat=n=2[v2];[1]hflip[v3];\\\n    [v2][v3]overlay=eof_action=repeat[v4];[v4]drawbox=50:50:120:120:red:t=5[v5]\"\\\n    -map [v5] output.mp4\nMaybe this looks great to you, but if you're not an FFmpeg command-line expert, it probably looks alien.\nIf you're like me and find Python to be powerful and readable, it's easier with ffmpeg-python:\nimport ffmpeg\n\nin_file = ffmpeg.input('input.mp4')\noverlay_file = ffmpeg.input('overlay.png')\n(\n    ffmpeg\n    .concat(\n        in_file.trim(start_frame=10, end_frame=20),\n        in_file.trim(start_frame=30, end_frame=40),\n    )\n    .overlay(overlay_file.hflip())\n    .drawbox(50, 50, 120, 120, color='red', thickness=5)\n    .output('out.mp4')\n    .run()\n)\nffmpeg-python takes care of running ffmpeg with the command-line arguments that correspond to the above filter diagram, in familiar Python terms.\n\nReal-world signal graphs can get a heck of a lot more complex, but ffmpeg-python handles arbitrarily large (directed-acyclic) signal graphs.\nInstallation\nInstalling ffmpeg-python\nThe latest version of ffmpeg-python can be acquired via a typical pip install:\npip install ffmpeg-python\nOr the source can be cloned and installed from locally:\ngit clone git@github.com:kkroening/ffmpeg-python.git\npip install -e ./ffmpeg-python\n\nNote: ffmpeg-python makes no attempt to download/install FFmpeg, as ffmpeg-python is merely a pure-Python wrapper - whereas FFmpeg installation is platform-dependent/environment-specific, and is thus the responsibility of the user, as described below.\n\nInstalling FFmpeg\nBefore using ffmpeg-python, FFmpeg must be installed and accessible via the $PATH environment variable.\nThere are a variety of ways to install FFmpeg, such as the official download links, or using your package manager of choice (e.g. sudo apt install ffmpeg on Debian/Ubuntu, brew install ffmpeg on OS X, etc.).\nRegardless of how FFmpeg is installed, you can check if your environment path is set correctly by running the ffmpeg command from the terminal, in which case the version information should appear, as in the following example (truncated for brevity):\n$ ffmpeg\nffmpeg version 4.2.4-1ubuntu0.1 Copyright (c) 2000-2020 the FFmpeg developers\n  built with gcc 9 (Ubuntu 9.3.0-10ubuntu2)\n\n\nNote: The actual version information displayed here may vary from one system to another; but if a message such as ffmpeg: command not found appears instead of the version information, FFmpeg is not properly installed.\n\nExamples\nWhen in doubt, take a look at the examples to see if there's something that's close to whatever you're trying to do.\nHere are a few:\n\n\nConvert video to numpy array\n\n\nGenerate thumbnail for video\n\n\nRead raw PCM audio via pipe\n\n\nJupyterLab/Notebook stream editor\n\n\n\n\nTensorflow/DeepDream streaming\n\n\nSee the Examples README for additional examples.\nCustom Filters\nDon't see the filter you're looking for?  While ffmpeg-python includes shorthand notation for some of the most commonly used filters (such as concat), all filters can be referenced via the .filter operator:\nstream = ffmpeg.input('dummy.mp4')\nstream = ffmpeg.filter(stream, 'fps', fps=25, round='up')\nstream = ffmpeg.output(stream, 'dummy2.mp4')\nffmpeg.run(stream)\nOr fluently:\n(\n    ffmpeg\n    .input('dummy.mp4')\n    .filter('fps', fps=25, round='up')\n    .output('dummy2.mp4')\n    .run()\n)\nSpecial option names:\nArguments with special names such as -qscale:v (variable bitrate), -b:v (constant bitrate), etc. can be specified as a keyword-args dictionary as follows:\n(\n    ffmpeg\n    .input('in.mp4')\n    .output('out.mp4', **{'qscale:v': 3})\n    .run()\n)\nMultiple inputs:\nFilters that take multiple input streams can be used by passing the input streams as an array to ffmpeg.filter:\nmain = ffmpeg.input('main.mp4')\nlogo = ffmpeg.input('logo.png')\n(\n    ffmpeg\n    .filter([main, logo], 'overlay', 10, 10)\n    .output('out.mp4')\n    .run()\n)\nMultiple outputs:\nFilters that produce multiple outputs can be used with .filter_multi_output:\nsplit = (\n    ffmpeg\n    .input('in.mp4')\n    .filter_multi_output('split')  # or `.split()`\n)\n(\n    ffmpeg\n    .concat(split[0], split[1].reverse())\n    .output('out.mp4')\n    .run()\n)\n(In this particular case, .split() is the equivalent shorthand, but the general approach works for other multi-output filters)\nString expressions:\nExpressions to be interpreted by ffmpeg can be included as string parameters and reference any special ffmpeg variable names:\n(\n    ffmpeg\n    .input('in.mp4')\n    .filter('crop', 'in_w-2*10', 'in_h-2*20')\n    .input('out.mp4')\n)\n\nWhen in doubt, refer to the existing filters, examples, and/or the official ffmpeg documentation.\nFrequently asked questions\nWhy do I get an import/attribute/etc. error from import ffmpeg?\nMake sure you ran pip install ffmpeg-python and not pip install ffmpeg (wrong) or pip install python-ffmpeg (also wrong).\nWhy did my audio stream get dropped?\nSome ffmpeg filters drop audio streams, and care must be taken to preserve the audio in the final output.  The .audio and .video operators can be used to reference the audio/video portions of a stream so that they can be processed separately and then re-combined later in the pipeline.\nThis dilemma is intrinsic to ffmpeg, and ffmpeg-python tries to stay out of the way while users may refer to the official ffmpeg documentation as to why certain filters drop audio.\nAs usual, take a look at the examples (Audio/video pipeline in particular).\nHow can I find out the used command line arguments?\nYou can run stream.get_args() before stream.run() to retrieve the command line arguments that will be passed to ffmpeg. You can also run stream.compile() that also includes the ffmpeg executable as the first argument.\nHow do I do XYZ?\nTake a look at each of the links in the Additional Resources section at the end of this README.  If you look everywhere and can't find what you're looking for and have a question that may be relevant to other users, you may open an issue asking how to do it, while providing a thorough explanation of what you're trying to do and what you've tried so far.\nIssues not directly related to ffmpeg-python or issues asking others to write your code for you or how to do the work of solving a complex signal processing problem for you that's not relevant to other users will be closed.\nThat said, we hope to continue improving our documentation and provide a community of support for people using ffmpeg-python to do cool and exciting things.\nContributing\n\nOne of the best things you can do to help make ffmpeg-python better is to answer open questions in the issue tracker.  The questions that are answered will be tagged and incorporated into the documentation, examples, and other learning resources.\nIf you notice things that could be better in the documentation or overall development experience, please say so in the issue tracker.  And of course, feel free to report any bugs or submit feature requests.\nPull requests are welcome as well, but it wouldn't hurt to touch base in the issue tracker or hop on the Matrix chat channel first.\nAnyone who fixes any of the open bugs or implements requested enhancements is a hero, but changes should include passing tests.\nRunning tests\ngit clone git@github.com:kkroening/ffmpeg-python.git\ncd ffmpeg-python\nvirtualenv venv\n. venv/bin/activate  # (OS X / Linux)\nvenv\\bin\\activate    # (Windows)\npip install -e .[dev]\npytest\n\nSpecial thanks\n\nFabrice Bellard\nThe FFmpeg team\nArne de Laat\nDavide Depau\nDim\nNoah Stier\n\nAdditional Resources\n\nAPI Reference\nExamples\nFilters\nFFmpeg Homepage\nFFmpeg Documentation\nFFmpeg Filters Documentation\nTest cases\nIssue tracker\nMatrix Chat: #ffmpeg-python:matrix.org\n\n\n\n"}, {"name": "extract-msg", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nextract-msg\nNOTICE\nChangelog\nUsage\nError Reporting\nSupporting The Module\nInstallation\nVersioning\nTodo\nCredits\nExtra\n\n\n\n\n\nREADME.rst\n\n\n\n\n\n \n \n \n\nextract-msg\nExtracts emails and attachments saved in Microsoft Outlook's .msg files\nThe python package extract_msg automates the extraction of key email\ndata (from, to, cc, date, subject, body) and the email's attachments.\nDocumentation can be found in the code, on the wiki, and on the\nRead the Docs page.\n\nNOTICE\n0.29.* is the branch that supports both Python 2 and Python 3. It is now only\nreceiving bug fixes and will not be receiving feature updates.\n0.39.* is the last versions that supported Python 3.6 and 3.7. Support for those\nwas dropped to allow the use of new features from 3.8 and because the life spans\nof those versions had ended.\nThis module has a Discord server for general discussion. You can find it here:\nDiscord\n\nChangelog\n\nChangelog\n\n\nUsage\nTo use it as a command-line script:\npython -m extract_msg example.msg\n\nThis will produce a new folder named according to the date, time and\nsubject of the message (for example \"2013-07-24_0915 Example\"). The\nemail itself can be found inside the new folder along with the\nattachments.\nThe script uses Philippe Lagadec's Python module that reads Microsoft\nOLE2 files (also called Structured Storage, Compound File Binary Format\nor Compound Document File Format). This is the underlying format of\nOutlook's .msg files. This library currently supports Python 3.8 and above.\nThe script was originally built using Peter Fiskerstrand's documentation of the\n.msg format. Redemption's discussion of the different property types used within\nExtended MAPI was also useful. For future reference, note that Microsoft have\nopened up their documentation of the file format, which is what is currently\nbeing used for development.\n#########REWRITE COMMAND LINE USAGE#############\nCurrently, the README is in the process of being redone. For now, please\nrefer to the usage information provided from the program's help dialog:\nusage: extract_msg [-h] [--use-content-id] [--json] [--file-logging] [-v] [--log LOG] [--config CONFIGPATH] [--out OUTPATH] [--use-filename] [--dump-stdout] [--html] [--pdf] [--wk-path WKPATH] [--wk-options [WKOPTIONS ...]]\n                   [--prepared-html] [--charset CHARSET] [--raw] [--rtf] [--allow-fallback] [--skip-body-not-found] [--zip ZIP] [--save-header] [--attachments-only] [--skip-hidden] [--no-folders] [--skip-embedded] [--extract-embedded]\n                   [--overwrite-existing] [--skip-not-implemented] [--out-name OUTNAME | --glob] [--ignore-rtfde] [--progress]\n                   msg [msg ...]\n\nextract_msg: Extracts emails and attachments saved in Microsoft Outlook's .msg files. https://github.com/TeamMsgExtractor/msg-extractor\n\npositional arguments:\n  msg                   An MSG file to be parsed.\n\noptions:\n  -h, --help            show this help message and exit\n  --use-content-id, --cid\n                        Save attachments by their Content ID, if they have one. Useful when working with the HTML body.\n  --json                Changes to write output files as json.\n  --file-logging        Enables file logging. Implies --verbose level 1.\n  -v, --verbose         Turns on console logging. Specify more than once for higher verbosity.\n  --log LOG             Set the path to write the file log to.\n  --config CONFIGPATH   Set the path to load the logging config from.\n  --out OUTPATH         Set the folder to use for the program output. (Default: Current directory)\n  --use-filename        Sets whether the name of each output is based on the msg filename.\n  --dump-stdout         Tells the program to dump the message body (plain text) to stdout. Overrides saving arguments.\n  --html                Sets whether the output should be HTML. If this is not possible, will error.\n  --pdf                 Saves the body as a PDF. If this is not possible, will error.\n  --wk-path WKPATH      Overrides the path for finding wkhtmltopdf.\n  --wk-options [WKOPTIONS ...]\n                        Sets additional options to be used in wkhtmltopdf. Should be a series of options and values, replacing the - or -- in the beginning with + or ++, respectively. For example: --wk-options \"+O Landscape\"\n  --prepared-html       When used in conjunction with --html, sets whether the HTML output should be prepared for embedded attachments.\n  --charset CHARSET     Character set to use for the prepared HTML in the added tag. (Default: utf-8)\n  --raw                 Sets whether the output should be raw. If this is not possible, will error.\n  --rtf                 Sets whether the output should be RTF. If this is not possible, will error.\n  --allow-fallback      Tells the program to fallback to a different save type if the selected one is not possible.\n  --skip-body-not-found\n                        Skips saving the body if the body cannot be found, rather than throwing an error.\n  --zip ZIP             Path to use for saving to a zip file.\n  --save-header         Store the header in a separate file.\n  --attachments-only    Specify to only save attachments from an msg file.\n  --skip-hidden         Skips any attachment marked as hidden (usually ones embedded in the body).\n  --no-folders          Stores everything in the location specified by --out. Requires --attachments-only and is incompatible with --out-name.\n  --skip-embedded       Skips all embedded MSG files when saving attachments.\n  --extract-embedded    Extracts the embedded MSG files as MSG files instead of running their save functions.\n  --overwrite-existing  Disables filename conflict resolution code for attachments when saving a file, causing files to be overwriten if two attachments with the same filename are on an MSG file.\n  --skip-not-implemented, --skip-ni\n                        Skips any attachments that are not implemented, allowing saving of the rest of the message.\n  --out-name OUTNAME    Name to be used with saving the file output. Cannot be used if you are saving more than one file.\n  --glob, --wildcard    Interpret all paths as having wildcards. Incompatible with --out-name.\n  --ignore-rtfde        Ignores all errors thrown from RTFDE when trying to save. Useful for allowing fallback to continue when an exception happens.\n  --progress            Shows what file the program is currently working on during it's progress.\n\nTo use this in your own script, start by using:\nimport extract_msg\n\nFrom there, open the MSG file:\nmsg = extract_msg.openMsg(\"path/to/msg/file.msg\")\n\nAlternatively, if you wish to send a msg binary string instead of a file\nto the extract_msg.openMsg Method:\nmsg_raw = b'\\xd0\\xcf\\x11\\xe0\\xa1\\xb1\\x1a\\xe1\\x00 ... \\x00\\x00\\x00'\nmsg = extract_msg.openMsg(msg_raw)\n\nIf you want to override the default attachment class and use one of your\nown, simply change the code to:\nmsg = extract_msg.openMsg(\"path/to/msg/file.msg\", attachmentClass = CustomAttachmentClass)\n\nwhere CustomAttachmentClass is your custom class.\n#TODO: Finish this section\nIf you have any questions feel free to contact Destiny at arceusthe [at]\ngmail [dot] com. She is the co-owner and main developer of the project.\nIf you have issues, it would be best to get help for them by opening a\nnew github issue.\n\nError Reporting\nShould you encounter an error that has not already been reported, please\ndo the following when reporting it: * Make sure you are using the\nlatest version of extract_msg (check the version on PyPi). * State your\nPython version. * Include the code, if any, that you used. * Include a\ncopy of the traceback.\n\nSupporting The Module\nIf you'd like to donate to help support the development of the module, you can\ndonate to Destiny using one of the following services:\n\nBuy Me a Coffee\nKo-fi\nPatreon\n\n\nInstallation\nYou can install using pip:\n\nPypi\n\npip install extract-msg\n\nGithub\n\npip install git+https://github.com/TeamMsgExtractor/msg-extractor\nor you can include this in your list of python dependencies with:\n# setup.py\n\nsetup(\n    ...\n    dependency_links=['https://github.com/TeamMsgExtractor/msg-extractor/zipball/master'],\n)\nAdditionally, this module has the following extras which can be optionally\ninstalled:\n\nall: Installs all of the extras.\nmime: Installs dependency used for mimetype generation when a mimetype is not specified.\n\n\nVersioning\nThis module uses Semantic Versioning, however it has not always done so. All versions greater than or equal to 0.40.* conform successfully. As the package is currently in major version zero (0.*.*), anything MAY change at any time, as per point 4 of the SemVer specification. However, I, Destiny, am aware of the module's usage in other packages and code, and so I have taken efforts to make the versioning more reliable.\nAny change to the minor version MUST be considered a potentially breaking change, and the changelog should be checked before assuming the API will function in the way it did in the previous minor version. I do, however, try to keep the API relatively stable between minor versions, so most typical usage is likely to remain entirely unaffected.\nAny change to a patch version before the 1.0.0 release SHOULD either add functionality or have no visible difference in usage, aside from changes to the typing infomation or from a bug fix correcting the data that a component created.\nIn addition to the above conditions, it must be noted that any class, variable, function, etc., that is preceded by one or more underscores, excluding items preceded by two underscores and also proceeded by two underscores, MUST NOT be considered part of the public api. These methods may change at any time, in any way.\nI am aware of the F.A.Q. question that suggests that I should probably have pushed the module to a 1.0.0 release due to its usage in production, however there are a number of different items on the TODO list that I feel should be completed before that time. While some are simply important features I believe should exist, others are overhauls to sections of the public API that have needed careful fixing for quite a while, fixes that have slowly been happening throughout the versions. An important change was made in the 0.45.0 release which deprecates a large number of commonly used private functions and created more stable versions of them in the public API.\nAdditionally, my focus on versioning info has revealed that some of the dependencies are still in major version 0 or do not necessarily conform to Semantic Versioning. As such, these packages are more tightly constrained on what versions are considered acceptable, and careful consideration should be taken before extending the accepted range of versions.\nDetails on Semantic Versioning can be found at semver.org.\n\nTodo\nHere is a list of things that are currently on our todo list:\n\nTests (ie. unittest)\nFinish writing a usage guide\nImprove the intelligence of the saving functions\nImprove README\nCreate a wiki for advanced usage information\n\n\nCredits\nDestiny Peterson (The Elemental of Destruction) - Co-owner, principle programmer, knows more about msg files than anyone probably should.\nMatthew Walker - Original developer and co-owner.\nJP Bourget - Senior programmer, readability and organization expert, secondary manager.\nPhilippe Lagadec - Python OleFile module developer.\nJoel Kaufman - First implementations of the json and filename flags.\nDean Malmgren - First implementation of the setup.py script.\nSeamus Tuohy - Developer of the Python RTFDE module. Gave first examples of how to use the module and has worked with Destiny to ensure functionality.\nLiam - Significant reorganization and transfer of data.\nAnd thank you to everyone who has opened an issue and helped us track down those pesky bugs.\n\nExtra\nCheck out the new project msg-explorer that allows you to open MSG files and\nexplore their contents in a GUI. It is usually updated within a few days of a\nmajor release to ensure continued support. Because of this, it is recommended to\ninstall it to a separate environment (like a vitural env) to not interfere with\nyour access to the newest major version of extract-msg.\n\n\n"}, {"name": "exchange-calendars", "readme": "\nexchange_calendars\n   \nA Python library for defining and querying calendars for security exchanges.\nCalendars for more than 50 exchanges available out-the-box! If you still can't find the calendar you're looking for, create a new one!\nNotice: market_prices - the new library for prices data!\nMuch of the recent development of exchange_calendars has been driven by the new market_prices library. Check it out if you like the idea of using exchange_calendars to create meaningful OHLCV datasets. Works out-the-box with freely available data!\nNotice: v4 released (June 2022)\nThe earliest stable version of v4 is 4.0.1 (not 4.0).\nWhat's changed?\nVersion 4.0.1 completes the transition to a more consistent interface across the package. The most significant changes are:\n\nSessions are now timezone-naive (previously UTC).\nSchedule columns now have timezone set as UTC (whilst the times have always been defined in terms of UTC, previously the dtype was timezone-naive).\nThe following schedule columns were renamed:\n\n'market_open' renamed as 'open'.\n'market_close' renamed as 'close'.\n\n\nDefault calendar 'side' for all calendars is now \"left\" (previously \"right\" for 24-hour calendars and \"both\" for all others). This changes the minutes that are considered trading minutes by default (see minutes tutorial for an explanation of trading minutes).\nThe 'count' parameter of sessions_window and minutes_window methods now reflects the window length (previously window length + 1).\nNew is_open_at_time calendar method to evaluate if an exchange is open as at a specific instance (as opposed to over an evaluated minute).\nThe minimum Python version supported is now 3.8 (previously 3.7).\nParameters have been renamed for some methods (list here)\nThe following methods have been deprecated:\n\nsessions_opens (use .opens[start:end])\nsessions_closes (use .closes[start:end])\n\n\nMethods deprecated in 3.4 have been removed (lists here and here)\n\nSee the 4.0 release todo for a full list of changes and corresponding PRs.\nPlease offer any feedback at the v4 discussion.\nInstallation\n$ pip install exchange_calendars\n\nQuick Start\nimport exchange_calendars as xcals\n\nGet a list of available calendars:\n>>> xcals.get_calendar_names(include_aliases=False)[5:10]\n['CMES', 'IEPA', 'XAMS', 'XASX', 'XBKK']\n\nGet a calendar:\n>>> xnys = xcals.get_calendar(\"XNYS\")  # New York Stock Exchange\n>>> xhkg = xcals.get_calendar(\"XHKG\")  # Hong Kong Stock Exchange\n\nQuery the schedule:\n>>> xhkg.schedule.loc[\"2021-12-29\":\"2022-01-04\"]\n\n\n\n\n\n\n\n\n\n   open break_start break_end close     2021-12-29 2021-12-29 01:30:00+00:00 2021-12-29 04:00:00+00:00 2021-12-29 05:00:00+00:00 2021-12-29 08:00:00+00:00   2021-12-30 2021-12-30 01:30:00+00:00 2021-12-30 04:00:00+00:00 2021-12-30 05:00:00+00:00 2021-12-30 08:00:00+00:00   2021-12-31 2021-12-31 01:30:00+00:00 NaT NaT 2021-12-31 04:00:00+00:00   2022-01-03 2022-01-03 01:30:00+00:00 2022-01-03 04:00:00+00:00 2022-01-03 05:00:00+00:00 2022-01-03 08:00:00+00:00   2022-01-04 2022-01-04 01:30:00+00:00 2022-01-04 04:00:00+00:00 2022-01-04 05:00:00+00:00 2022-01-04 08:00:00+00:00  \n\nWorking with sessions\n>>> xnys.is_session(\"2022-01-01\")\nFalse\n\n>>> xnys.sessions_in_range(\"2022-01-01\", \"2022-01-11\")\nDatetimeIndex(['2022-01-03', '2022-01-04', '2022-01-05', '2022-01-06',\n               '2022-01-07', '2022-01-10', '2022-01-11'],\n              dtype='datetime64[ns]', freq='C')\n\n>>> xnys.sessions_window(\"2022-01-03\", 7)\nDatetimeIndex(['2022-01-03', '2022-01-04', '2022-01-05', '2022-01-06',\n               '2022-01-07', '2022-01-10', '2022-01-11'],\n              dtype='datetime64[ns]', freq='C')\n\n>>> xnys.date_to_session(\"2022-01-01\", direction=\"next\")\nTimestamp('2022-01-03 00:00:00', freq='C')\n\n>>> xnys.previous_session(\"2022-01-11\")\nTimestamp('2022-01-10 00:00:00', freq='C')\n\n>>> xhkg.trading_index(\n...     \"2021-12-30\", \"2021-12-31\", period=\"90T\", force=True\n... )\nIntervalIndex([[2021-12-30 01:30:00, 2021-12-30 03:00:00), [2021-12-30 03:00:00, 2021-12-30 04:00:00), [2021-12-30 05:00:00, 2021-12-30 06:30:00), [2021-12-30 06:30:00, 2021-12-30 08:00:00), [2021-12-31 01:30:00, 2021-12-31 03:00:00), [2021-12-31 03:00:00, 2021-12-31 04:00:00)], dtype='interval[datetime64[ns, UTC], left]')\n\nSee the sessions tutorial for a deeper dive into sessions.\nWorking with minutes\n>>> xhkg.session_minutes(\"2022-01-03\")\nDatetimeIndex(['2022-01-03 01:30:00+00:00', '2022-01-03 01:31:00+00:00',\n               '2022-01-03 01:32:00+00:00', '2022-01-03 01:33:00+00:00',\n               '2022-01-03 01:34:00+00:00', '2022-01-03 01:35:00+00:00',\n               '2022-01-03 01:36:00+00:00', '2022-01-03 01:37:00+00:00',\n               '2022-01-03 01:38:00+00:00', '2022-01-03 01:39:00+00:00',\n               ...\n               '2022-01-03 07:50:00+00:00', '2022-01-03 07:51:00+00:00',\n               '2022-01-03 07:52:00+00:00', '2022-01-03 07:53:00+00:00',\n               '2022-01-03 07:54:00+00:00', '2022-01-03 07:55:00+00:00',\n               '2022-01-03 07:56:00+00:00', '2022-01-03 07:57:00+00:00',\n               '2022-01-03 07:58:00+00:00', '2022-01-03 07:59:00+00:00'],\n              dtype='datetime64[ns, UTC]', length=330, freq=None)\n\n>>> mins = [ \"2022-01-03 \" + tm for tm in [\"01:29\", \"01:30\", \"04:20\", \"07:59\", \"08:00\"] ]\n>>> [ xhkg.is_trading_minute(minute) for minute in mins ]\n[False, True, False, True, False]  # by default minutes are closed on the left side\n\n>>> xhkg.is_break_minute(\"2022-01-03 04:20\")\nTrue\n\n>>> xhkg.previous_close(\"2022-01-03 08:10\")\nTimestamp('2022-01-03 08:00:00+0000', tz='UTC')\n\n>>> xhkg.previous_minute(\"2022-01-03 08:10\")\nTimestamp('2022-01-03 07:59:00+0000', tz='UTC')\n\nCheck out the minutes tutorial for a deeper dive that includes an explanation of the concept of 'minutes' and how the \"side\" option determines which minutes are treated as trading minutes.\nTutorials\n\nsessions.ipynb - all things sessions.\nminutes.ipynb - all things minutes. Don't miss this one!\ncalendar_properties.ipynb - calendar constrution and a walk through the schedule and all other calendar properties.\ncalendar_methods.ipynb - a walk through all the methods available to interrogate a calendar.\ntrading_index.ipynb - a method that warrants a tutorial all of its own.\n\nHopefully you'll find that exchange_calendars has the method you need to get the information you want. If it doesn't, either PR it or raise an issue and let us know!\nCommand Line Usage\nPrint a unix-cal like calendar straight from the command line (holidays are indicated by brackets)...\necal XNYS 2020\n\n                                        2020\n        January                        February                        March\nSu  Mo  Tu  We  Th  Fr  Sa     Su  Mo  Tu  We  Th  Fr  Sa     Su  Mo  Tu  We  Th  Fr  Sa\n            [ 1]  2   3 [ 4]                           [ 1]\n[ 5]  6   7   8   9  10 [11]   [ 2]  3   4   5   6   7 [ 8]   [ 1]  2   3   4   5   6 [ 7]\n[12] 13  14  15  16  17 [18]   [ 9] 10  11  12  13  14 [15]   [ 8]  9  10  11  12  13 [14]\n[19][20] 21  22  23  24 [25]   [16][17] 18  19  20  21 [22]   [15] 16  17  18  19  20 [21]\n[26] 27  28  29  30  31        [23] 24  25  26  27  28 [29]   [22] 23  24  25  26  27 [28]\n                                                              [29] 30  31\n\n        April                           May                            June\nSu  Mo  Tu  We  Th  Fr  Sa     Su  Mo  Tu  We  Th  Fr  Sa     Su  Mo  Tu  We  Th  Fr  Sa\n              1   2   3 [ 4]                         1 [ 2]         1   2   3   4   5 [ 6]\n[ 5]  6   7   8   9 [10][11]   [ 3]  4   5   6   7   8 [ 9]   [ 7]  8   9  10  11  12 [13]\n[12] 13  14  15  16  17 [18]   [10] 11  12  13  14  15 [16]   [14] 15  16  17  18  19 [20]\n[19] 20  21  22  23  24 [25]   [17] 18  19  20  21  22 [23]   [21] 22  23  24  25  26 [27]\n[26] 27  28  29  30            [24][25] 26  27  28  29 [30]   [28] 29  30\n                               [31]\n\n            July                          August                       September\nSu  Mo  Tu  We  Th  Fr  Sa     Su  Mo  Tu  We  Th  Fr  Sa     Su  Mo  Tu  We  Th  Fr  Sa\n              1   2 [ 3][ 4]                           [ 1]             1   2   3   4 [ 5]\n[ 5]  6   7   8   9  10 [11]   [ 2]  3   4   5   6   7 [ 8]   [ 6][ 7]  8   9  10  11 [12]\n[12] 13  14  15  16  17 [18]   [ 9] 10  11  12  13  14 [15]   [13] 14  15  16  17  18 [19]\n[19] 20  21  22  23  24 [25]   [16] 17  18  19  20  21 [22]   [20] 21  22  23  24  25 [26]\n[26] 27  28  29  30  31        [23] 24  25  26  27  28 [29]   [27] 28  29  30\n                               [30] 31\n\n        October                        November                       December\nSu  Mo  Tu  We  Th  Fr  Sa     Su  Mo  Tu  We  Th  Fr  Sa     Su  Mo  Tu  We  Th  Fr  Sa\n                  1   2 [ 3]                                            1   2   3   4 [ 5]\n[ 4]  5   6   7   8   9 [10]   [ 1]  2   3   4   5   6 [ 7]   [ 6]  7   8   9  10  11 [12]\n[11] 12  13  14  15  16 [17]   [ 8]  9  10  11  12  13 [14]   [13] 14  15  16  17  18 [19]\n[18] 19  20  21  22  23 [24]   [15] 16  17  18  19  20 [21]   [20] 21  22  23  24 [25][26]\n[25] 26  27  28  29  30 [31]   [22] 23  24  25 [26] 27 [28]   [27] 28  29  30  31\n                               [29] 30\n\necal XNYS 1 2020\n\n        January 2020\nSu  Mo  Tu  We  Th  Fr  Sa\n            [ 1]  2   3 [ 4]\n[ 5]  6   7   8   9  10 [11]\n[12] 13  14  15  16  17 [18]\n[19][20] 21  22  23  24 [25]\n[26] 27  28  29  30  31\n\nFrequently Asked Questions\nHow can I create a new calendar?\nFirst off, make sure the calendar you're after hasn't already been defined; exchange calendars comes with over 50 pre-defined calendars, including major security exchanges.\nIf you can't find what you're after, a custom calendar can be created as a subclass of ExchangeCalendar. This workflow describes the process to add a new calendar to exchange_calendars. Just follow the relevant parts.\nTo access the new calendar via get_calendar call either xcals.register_calendar or xcals.register_calendar_type to register, respectively, a specific calendar instance or a calendar factory (i.e. the subclass).\nCan I contribute a new calendar to exchange calendars?\nYes please! The workflow can be found here.\n<calendar> is missing a holiday, has a wrong time, should have a break etc...\nAll of the exchange calendars are maintained by user contributions. If a calendar you care about needs revising, please open a PR - that's how this thing works! (Never contributed to a project before and it all seems a bit daunting? Check this out and don't look back!)\nYou'll find the workflow to modify an existing calendar here.\nWhat times are considered open and closed?\nexchange_calendars attempts to be broadly useful by considering an exchange to be open only during periods of regular trading. During any pre-trading, post-trading or auction period the exchange is treated as closed. An exchange is also treated as closed during any observed lunch break.\nSee the minutes tutorial for a detailed explanation of which minutes an exchange is considered open over. If you previously used trading_calendars, or exchange_calendars prior to release 3.4, then this is the place to look for answers to questions of how the definition of trading minutes has changed over time (and is now stable and flexible!).\nCalendars\n\n\n\nExchange\nISO Code\nCountry\nVersion Added\nExchange Website (English)\n\n\n\n\nNew York Stock Exchange\nXNYS\nUSA\n1.0\nhttps://www.nyse.com/index\n\n\nCBOE Futures\nXCBF\nUSA\n1.0\nhttps://markets.cboe.com/us/futures/overview/\n\n\nChicago Mercantile Exchange\nCMES\nUSA\n1.0\nhttps://www.cmegroup.com/\n\n\nICE US\nIEPA\nUSA\n1.0\nhttps://www.theice.com/index\n\n\nToronto Stock Exchange\nXTSE\nCanada\n1.0\nhttps://www.tsx.com/\n\n\nBMF Bovespa\nBVMF\nBrazil\n1.0\nhttp://www.b3.com.br/en_us/\n\n\nLondon Stock Exchange\nXLON\nEngland\n1.0\nhttps://www.londonstockexchange.com/\n\n\nEuronext Amsterdam\nXAMS\nNetherlands\n1.2\nhttps://www.euronext.com/en/regulation/amsterdam\n\n\nEuronext Brussels\nXBRU\nBelgium\n1.2\nhttps://www.euronext.com/en/regulation/brussels\n\n\nEuronext Lisbon\nXLIS\nPortugal\n1.2\nhttps://www.euronext.com/en/regulation/lisbon\n\n\nEuronext Paris\nXPAR\nFrance\n1.2\nhttps://www.euronext.com/en/regulation/paris\n\n\nFrankfurt Stock Exchange\nXFRA\nGermany\n1.2\nhttp://en.boerse-frankfurt.de/\n\n\nSIX Swiss Exchange\nXSWX\nSwitzerland\n1.2\nhttps://www.six-group.com/en/home.html\n\n\nTokyo Stock Exchange\nXTKS\nJapan\n1.2\nhttps://www.jpx.co.jp/english/\n\n\nAustrialian Securities Exchange\nXASX\nAustralia\n1.3\nhttps://www.asx.com.au/\n\n\nBolsa de Madrid\nXMAD\nSpain\n1.3\nhttps://www.bolsamadrid.es\n\n\nBorsa Italiana\nXMIL\nItaly\n1.3\nhttps://www.borsaitaliana.it\n\n\nNew Zealand Exchange\nXNZE\nNew Zealand\n1.3\nhttps://www.nzx.com/\n\n\nWiener Borse\nXWBO\nAustria\n1.3\nhttps://www.wienerborse.at/en/\n\n\nHong Kong Stock Exchange\nXHKG\nHong Kong\n1.3\nhttps://www.hkex.com.hk/?sc_lang=en\n\n\nCopenhagen Stock Exchange\nXCSE\nDenmark\n1.4\nhttp://www.nasdaqomxnordic.com/\n\n\nHelsinki Stock Exchange\nXHEL\nFinland\n1.4\nhttp://www.nasdaqomxnordic.com/\n\n\nStockholm Stock Exchange\nXSTO\nSweden\n1.4\nhttp://www.nasdaqomxnordic.com/\n\n\nOslo Stock Exchange\nXOSL\nNorway\n1.4\nhttps://www.oslobors.no/ob_eng/\n\n\nIrish Stock Exchange\nXDUB\nIreland\n1.4\nhttp://www.ise.ie/\n\n\nBombay Stock Exchange\nXBOM\nIndia\n1.5\nhttps://www.bseindia.com\n\n\nSingapore Exchange\nXSES\nSingapore\n1.5\nhttps://www.sgx.com\n\n\nShanghai Stock Exchange\nXSHG\nChina\n1.5\nhttp://english.sse.com.cn\n\n\nKorea Exchange\nXKRX\nSouth Korea\n1.6\nhttp://global.krx.co.kr\n\n\nIceland Stock Exchange\nXICE\nIceland\n1.7\nhttp://www.nasdaqomxnordic.com/\n\n\nPoland Stock Exchange\nXWAR\nPoland\n1.9\nhttp://www.gpw.pl\n\n\nSantiago Stock Exchange\nXSGO\nChile\n1.9\nhttps://www.bolsadesantiago.com/\n\n\nColombia Securities Exchange\nXBOG\nColombia\n1.9\nhttps://www.bvc.com.co/nueva/https://www.bvc.com.co/nueva/\n\n\nMexican Stock Exchange\nXMEX\nMexico\n1.9\nhttps://www.bmv.com.mx\n\n\nLima Stock Exchange\nXLIM\nPeru\n1.9\nhttps://www.bvl.com.pe\n\n\nPrague Stock Exchange\nXPRA\nCzech Republic\n1.9\nhttps://www.pse.cz/en/\n\n\nBudapest Stock Exchange\nXBUD\nHungary\n1.10\nhttps://bse.hu/\n\n\nAthens Stock Exchange\nASEX\nGreece\n1.10\nhttp://www.helex.gr/\n\n\nIstanbul Stock Exchange\nXIST\nTurkey\n1.10\nhttps://www.borsaistanbul.com/en/\n\n\nJohannesburg Stock Exchange\nXJSE\nSouth Africa\n1.10\nhttps://www.jse.co.za/z\n\n\nMalaysia Stock Exchange\nXKLS\nMalaysia\n1.11\nhttp://www.bursamalaysia.com/market/\n\n\nMoscow Exchange\nXMOS\nRussia\n1.11\nhttps://www.moex.com/en/\n\n\nPhilippine Stock Exchange\nXPHS\nPhilippines\n1.11\nhttps://www.pse.com.ph/\n\n\nStock Exchange of Thailand\nXBKK\nThailand\n1.11\nhttps://www.set.or.th/set/mainpage.do?language=en&country=US\n\n\nIndonesia Stock Exchange\nXIDX\nIndonesia\n1.11\nhttps://www.idx.co.id/\n\n\nTaiwan Stock Exchange Corp.\nXTAI\nTaiwan\n1.11\nhttps://www.twse.com.tw/en/\n\n\nBuenos Aires Stock Exchange\nXBUE\nArgentina\n1.11\nhttps://www.bcba.sba.com.ar/\n\n\nPakistan Stock Exchange\nXKAR\nPakistan\n1.11\nhttps://www.psx.com.pk/\n\n\nXetra\nXETR\nGermany\n2.1\nhttps://www.xetra.com/\n\n\nTel Aviv Stock Exchange\nXTAE\nIsrael\n2.1\nhttps://www.tase.co.il/\n\n\nAstana International Exchange\nAIXK\nKazakhstan\n3.2\nhttps://www.aix.kz/\n\n\nBucharest Stock Exchange\nXBSE\nRomania\n3.2\nhttps://www.bvb.ro/\n\n\nSaudi Stock Exchange\nXSAU\nSaudi Arabia\n4.2\nhttps://www.saudiexchange.sa/\n\n\n\n\nNote that exchange calendars are defined by their ISO-10383 market identifier code.\n\nDeprecations and Renaming\nMethods deprecated in 4.0\n\n\n\nDeprecated method\nReason\n\n\n\n\nsessions_closes\nuse .closes[start:end]\n\n\nsessions_opens\nuse .opens[start:end]\n\n\n\nMethods with a parameter renamed in 4.0\n\n\n\nMethod\n\n\n\n\nis_session\n\n\nis_open_on_minute\n\n\nminutes_in_range\n\n\nminutes_window\n\n\nnext_close\n\n\nnext_minute\n\n\nnext_open\n\n\nprevious_close\n\n\nprevious_minute\n\n\nprevious_open\n\n\nsession_break_end\n\n\nsession_break_start\n\n\nsession_close\n\n\nsession_open\n\n\nsessions_in_range\n\n\nsessions_window\n\n\n\nMethods renamed in version 3.4 and removed in 4.0\n\n\n\nPrevious name\nNew name\n\n\n\n\nall_minutes\nminutes\n\n\nall_minutes_nanos\nminutes_nanos\n\n\nall_sessions\nsessions\n\n\nbreak_start_and_end_for_session\nsession_break_start_end\n\n\ndate_to_session_label\ndate_to_session\n\n\nfirst_trading_minute\nfirst_minute\n\n\nfirst_trading_session\nfirst_session\n\n\nhas_breaks\nsessions_has_break\n\n\nlast_trading_minute\nlast_minute\n\n\nlast_trading_session\nlast_session\n\n\nnext_session_label\nnext_session\n\n\nopen_and_close_for_session\nsession_open_close\n\n\nprevious_session_label\nprevious_session\n\n\nmarket_break_ends_nanos\nbreak_ends_nanos\n\n\nmarket_break_starts_nanos\nbreak_starts_nanos\n\n\nmarket_closes_nanos\ncloses_nanos\n\n\nmarket_opens_nanos\nopens_nanos\n\n\nminute_index_to_session_labels\nminutes_to_sessions\n\n\nminute_to_session_label\nminute_to_session\n\n\nminutes_count_for_sessions_in_range\nsessions_minutes_count\n\n\nminutes_for_session\nsession_minutes\n\n\nminutes_for_sessions_in_range\nsessions_minutes\n\n\nsession_closes_in_range\nsessions_closes\n\n\nsession_distance\nsessions_distance\n\n\nsession_opens_in_range\nsessions_opens\n\n\n\nOther methods deprecated in 3.4 and removed in 4.0\n\n\n\nRemoved Method\n\n\n\n\nexecution_minute_for_session\n\n\nexecution_minute_for_sessions_in_range\n\n\nexecution_time_from_close\n\n\nexecution_time_from_open\n\n\n\n"}, {"name": "et-xmlfile", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nWarehouse\nGetting Started\nDiscussion\nTesting\nCode of Conduct\n\n\n\n\n\nREADME.rst\n\n\n\n\nWarehouse\nWarehouse is the software that powers PyPI.\nSee our development roadmap, documentation, and\narchitectural overview.\n\nGetting Started\nYou can run Warehouse locally in a development environment using\ndocker. See Getting started\ndocumentation for instructions on how to set it up.\nThe canonical deployment of Warehouse is in production at pypi.org.\n\nDiscussion\nYou can find help or get involved on:\n\nGithub issue tracker for reporting issues\nIRC: on Libera, channel #pypa for general packaging discussion\nand user support, and #pypa-dev for\ndiscussions about development of packaging tools\nThe PyPA Discord for live discussions\nThe Packaging category on Discourse for discussing\nnew ideas and community initiatives\n\n\nTesting\nRead the running tests and linters section of our documentation to\nlearn how to test your code.  For cross-browser testing, we use an\nopen source account from BrowserStack. If your pull request makes\nany change to the user interface, it will need to be tested to confirm\nit works in our supported browsers.\n\n\nCode of Conduct\nEveryone interacting in the Warehouse project's codebases, issue trackers, chat\nrooms, and mailing lists is expected to follow the PSF Code of Conduct.\n\n\n"}, {"name": "email-validator", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nemail-validator: Validate Email Addresses\nInstallation\nQuick Start\nUsage\nOverview\nOptions\nDNS timeout and cache\nTest addresses\nInternationalized email addresses\nInternationalized domain names (IDN)\nInternationalized local parts\nIf you know ahead of time that SMTPUTF8 is not supported by your mail submission stack\nNormalization\nUnicode Normalization\nOther Normalization\nExamples\nReturn value\nAssumptions\nTesting\nFor Project Maintainers\n\n\n\n\n\nREADME.md\n\n\n\n\nemail-validator: Validate Email Addresses\nA robust email address syntax and deliverability validation library for\nPython 3.7+ by Joshua Tauberer.\nThis library validates that a string is of the form name@example.com\nand optionally checks that the domain name is set up to receive email.\nThis is the sort of validation you would want when you are identifying\nusers by their email address like on a registration/login form (but not\nnecessarily for composing an email message, see below).\nKey features:\n\nChecks that an email address has the correct syntax --- good for\nregistration/login forms or other uses related to identifying users.\nGives friendly English error messages when validation fails that you\ncan display to end-users.\nChecks deliverability (optional): Does the domain name resolve?\n(You can override the default DNS resolver to add query caching.)\nSupports internationalized domain names and internationalized local parts.\nRejects addresses with unsafe Unicode characters, obsolete email address\nsyntax that you'd find unexpected, special use domain names like\n@localhost, and domains without a dot by default. This is an\nopinionated library!\nNormalizes email addresses (important for internationalized\nand quoted-string addresses! see below).\nPython type annotations are used.\n\nThis is an opinionated library. You should definitely also consider using\nthe less-opinionated pyIsEmail and\nflanker if they are better for your\nuse case.\n\nView the CHANGELOG / Release Notes for the version history of changes in the library. Occasionally this README is ahead of the latest published package --- see the CHANGELOG for details.\n\nInstallation\nThis package is on PyPI, so:\npip install email-validator\n(You might need to use pip3 depending on your local environment.)\nQuick Start\nIf you're validating a user's email address before creating a user\naccount in your application, you might do this:\nfrom email_validator import validate_email, EmailNotValidError\n\nemail = \"my+address@example.org\"\n\ntry:\n\n  # Check that the email address is valid. Turn on check_deliverability\n  # for first-time validations like on account creation pages (but not\n  # login pages).\n  emailinfo = validate_email(email, check_deliverability=False)\n\n  # After this point, use only the normalized form of the email address,\n  # especially before going to a database query.\n  email = emailinfo.normalized\n\nexcept EmailNotValidError as e:\n\n  # The exception message is human-readable explanation of why it's\n  # not a valid (or deliverable) email address.\n  print(str(e))\nThis validates the address and gives you its normalized form. You should\nput the normalized form in your database and always normalize before\nchecking if an address is in your database. When using this in a login form,\nset check_deliverability to False to avoid unnecessary DNS queries.\nUsage\nOverview\nThe module provides a function validate_email(email_address) which\ntakes an email address and:\n\nRaises a EmailNotValidError with a helpful, human-readable error\nmessage explaining why the email address is not valid, or\nReturns an object with a normalized form of the email address (which\nyou should use!) and other information about it.\n\nWhen an email address is not valid, validate_email raises either an\nEmailSyntaxError if the form of the address is invalid or an\nEmailUndeliverableError if the domain name fails DNS checks. Both\nexception classes are subclasses of EmailNotValidError, which in turn\nis a subclass of ValueError.\nBut when an email address is valid, an object is returned containing\na normalized form of the email address (which you should use!) and\nother information.\nThe validator doesn't, by default, permit obsoleted forms of email addresses\nthat no one uses anymore even though they are still valid and deliverable, since\nthey will probably give you grief if you're using email for login. (See\nlater in the document about how to allow some obsolete forms.)\nThe validator optionally checks that the domain name in the email address has\na DNS MX record indicating that it can receive email. (Except a Null MX record.\nIf there is no MX record, a fallback A/AAAA-record is permitted, unless\na reject-all SPF record is present.) DNS is slow and sometimes unavailable or\nunreliable, so consider whether these checks are useful for your use case and\nturn them off if they aren't.\nThere is nothing to be gained by trying to actually contact an SMTP server, so\nthat's not done here. For privacy, security, and practicality reasons, servers\nare good at not giving away whether an address is\ndeliverable or not: email addresses that appear to accept mail at first\ncan bounce mail after a delay, and bounced mail may indicate a temporary\nfailure of a good email address (sometimes an intentional failure, like\ngreylisting).\nOptions\nThe validate_email function also accepts the following keyword arguments\n(defaults are as shown below):\ncheck_deliverability=True: If true, DNS queries are made to check that the domain name in the email address (the part after the @-sign) can receive mail, as described above. Set to False to skip this DNS-based check. It is recommended to pass False when performing validation for login pages (but not account creation pages) since re-validation of a previously validated domain in your database by querying DNS at every login is probably undesirable. You can also set email_validator.CHECK_DELIVERABILITY to False to turn this off for all calls by default.\ndns_resolver=None: Pass an instance of dns.resolver.Resolver to control the DNS resolver including setting a timeout and a cache. The caching_resolver function shown below is a helper function to construct a dns.resolver.Resolver with a LRUCache. Reuse the same resolver instance across calls to validate_email to make use of the cache.\ntest_environment=False: If True, DNS-based deliverability checks are disabled and  test and **.test domain names are permitted (see below). You can also set email_validator.TEST_ENVIRONMENT to True to turn it on for all calls by default.\nallow_smtputf8=True: Set to False to prohibit internationalized addresses that would\nrequire the\nSMTPUTF8 extension. You can also set email_validator.ALLOW_SMTPUTF8 to False to turn it off for all calls by default.\nallow_quoted_local=False: Set to True to allow obscure and potentially problematic email addresses in which the part of the address before the @-sign contains spaces, @-signs, or other surprising characters when the local part is surrounded in quotes (so-called quoted-string local parts). In the object returned by validate_email, the normalized local part removes any unnecessary backslash-escaping and even removes the surrounding quotes if the address would be valid without them. You can also set email_validator.ALLOW_QUOTED_LOCAL to True to turn this on for all calls by default.\nallow_domain_literal=False: Set to True to allow bracketed IPv4 and \"IPv6:\"-prefixd IPv6 addresses in the domain part of the email address. No deliverability checks are performed for these addresses. In the object returned by validate_email, the normalized domain will use the condensed IPv6 format, if applicable. The object's domain_address attribute will hold the parsed ipaddress.IPv4Address or ipaddress.IPv6Address object if applicable. You can also set email_validator.ALLOW_DOMAIN_LITERAL to True to turn this on for all calls by default.\nallow_empty_local=False: Set to True to allow an empty local part (i.e.\n@example.com), e.g. for validating Postfix aliases.\nDNS timeout and cache\nWhen validating many email addresses or to control the timeout (the default is 15 seconds), create a caching dns.resolver.Resolver to reuse in each call. The caching_resolver function returns one easily for you:\nfrom email_validator import validate_email, caching_resolver\n\nresolver = caching_resolver(timeout=10)\n\nwhile True:\n  validate_email(email, dns_resolver=resolver)\nTest addresses\nThis library rejects email addresess that use the Special Use Domain Names invalid, localhost, test, and some others by raising EmailSyntaxError. This is to protect your system from abuse: You probably don't want a user to be able to cause an email to be sent to localhost (although they might be able to still do so via a malicious MX record). However, in your non-production test environments you may want to use @test or @myname.test email addresses. There are three ways you can allow this:\n\nAdd test_environment=True to the call to validate_email (see above).\nSet email_validator.TEST_ENVIRONMENT to True globally.\nRemove the special-use domain name that you want to use from email_validator.SPECIAL_USE_DOMAIN_NAMES, e.g.:\n\nimport email_validator\nemail_validator.SPECIAL_USE_DOMAIN_NAMES.remove(\"test\")\nIt is tempting to use @example.com/net/org in tests. They are not in this library's SPECIAL_USE_DOMAIN_NAMES list so you can, but shouldn't, use them. These domains are reserved to IANA for use in documentation so there is no risk of accidentally emailing someone at those domains. But beware that this library will nevertheless reject these domain names if DNS-based deliverability checks are not disabled because these domains do not resolve to domains that accept email. In tests, consider using your own domain name or @test or @myname.test instead.\nInternationalized email addresses\nThe email protocol SMTP and the domain name system DNS have historically\nonly allowed English (ASCII) characters in email addresses and domain names,\nrespectively. Each has adapted to internationalization in a separate\nway, creating two separate aspects to email address\ninternationalization.\nInternationalized domain names (IDN)\nThe first is internationalized domain names (RFC\n5891), a.k.a IDNA 2008. The DNS\nsystem has not been updated with Unicode support. Instead, internationalized\ndomain names are converted into a special IDNA ASCII \"Punycode\"\nform starting with xn--. When an email address has non-ASCII\ncharacters in its domain part, the domain part is replaced with its IDNA\nASCII equivalent form in the process of mail transmission. Your mail\nsubmission library probably does this for you transparently. (Compliance\naround the web is not very good though.) This library conforms to IDNA 2008\nusing the idna module by Kim Davies.\nInternationalized local parts\nThe second sort of internationalization is internationalization in the\nlocal part of the address (before the @-sign). In non-internationalized\nemail addresses, only English letters, numbers, and some punctuation\n(._!#$%&'^``*+-=~/?{|}) are allowed. In internationalized email address\nlocal parts, a wider range of Unicode characters are allowed.\nA surprisingly large number of Unicode characters are not safe to display,\nespecially when the email address is concatenated with other text, so this\nlibrary tries to protect you by not permitting resvered, non-, private use,\nformatting (which can be used to alter the display order of characters),\nwhitespace, and control characters, and combining characters\nas the first character of the local part and the domain name (so that they\ncannot combine with something outside of the email address string or with\nthe @-sign). See https://qntm.org/safe and https://trojansource.codes/\nfor relevant prior work. (Other than whitespace, these are checks that\nyou should be applying to nearly all user inputs in a security-sensitive\ncontext.)\nThese character checks are performed after Unicode normalization (see below),\nso you are only fully protected if you replace all user-provided email addresses\nwith the normalized email address string returned by this library. This does not\nguard against the well known problem that many Unicode characters look alike\n(or are identical), which can be used to fool humans reading displayed text.\nEmail addresses with these non-ASCII characters require that your mail\nsubmission library and the mail servers along the route to the destination,\nincluding your own outbound mail server, all support the\nSMTPUTF8 (RFC 6531) extension.\nSupport for SMTPUTF8 varies. See the allow_smtputf8 parameter.\nIf you know ahead of time that SMTPUTF8 is not supported by your mail submission stack\nBy default all internationalized forms are accepted by the validator.\nBut if you know ahead of time that SMTPUTF8 is not supported by your\nmail submission stack, then you must filter out addresses that require\nSMTPUTF8 using the allow_smtputf8=False keyword argument (see above).\nThis will cause the validation function to raise a EmailSyntaxError if\ndelivery would require SMTPUTF8. That's just in those cases where\nnon-ASCII characters appear before the @-sign. If you do not set\nallow_smtputf8=False, you can also check the value of the smtputf8\nfield in the returned object.\nIf your mail submission library doesn't support Unicode at all --- even\nin the domain part of the address --- then immediately prior to mail\nsubmission you must replace the email address with its ASCII-ized form.\nThis library gives you back the ASCII-ized form in the ascii_email\nfield in the returned object, which you can get like this:\nemailinfo = validate_email(email, allow_smtputf8=False)\nemail = emailinfo.ascii_email\nThe local part is left alone (if it has internationalized characters\nallow_smtputf8=False will force validation to fail) and the domain\npart is converted to IDNA ASCII.\n(You probably should not do this at account creation time so you don't\nchange the user's login information without telling them.)\nNormalization\nUnicode Normalization\nThe use of Unicode in email addresses introduced a normalization\nproblem. Different Unicode strings can look identical and have the same\nsemantic meaning to the user. The normalized field returned on successful\nvalidation provides the correctly normalized form of the given email\naddress.\nFor example, the CJK fullwidth Latin letters are considered semantically\nequivalent in domain names to their ASCII counterparts. This library\nnormalizes them to their ASCII counterparts:\nemailinfo = validate_email(\"me@\uff24\uff4f\uff4d\uff41\uff49\uff4e.com\")\nprint(emailinfo.normalized)\nprint(emailinfo.ascii_email)\n# prints \"me@domain.com\" twice\nBecause an end-user might type their email address in different (but\nequivalent) un-normalized forms at different times, you ought to\nreplace what they enter with the normalized form immediately prior to\ngoing into your database (during account creation), querying your database\n(during login), or sending outbound mail. Normalization may also change\nthe length of an email address, and this may affect whether it is valid\nand acceptable by your SMTP provider.\nThe normalizations include lowercasing the domain part of the email\naddress (domain names are case-insensitive), Unicode \"NFC\"\nnormalization of the\nwhole address (which turns characters plus combining\ncharacters into\nprecomposed characters where possible, replacement of fullwidth and\nhalfwidth\ncharacters\nin the domain part, possibly other\nUTS46 mappings on the domain part,\nand conversion from Punycode to Unicode characters.\n(See RFC 6532 (internationalized email) section\n3.1 and RFC 5895\n(IDNA 2008) section 2.)\nOther Normalization\nNormalization is also applied to quoted-string local parts and domain\nliteral IPv6 addresses if you have allowed them by the allow_quoted_local\nand allow_domain_literal options. In quoted-string local parts, unnecessary\nbackslash escaping is removed and even the surrounding quotes are removed if\nthey are unnecessary. For IPv6 domain literals, the IPv6 address is\nnormalized to condensed form. RFC 2142\nalso requires lowercase normalization for some specific mailbox names like postmaster@.\nExamples\nFor the email address test@joshdata.me, the returned object is:\nValidatedEmail(\n  normalized='test@joshdata.me',\n  local_part='test',\n  domain='joshdata.me',\n  ascii_email='test@joshdata.me',\n  ascii_local_part='test',\n  ascii_domain='joshdata.me',\n  smtputf8=False)\nFor the fictitious but valid address example@\u30c4.\u24c1\u24be\u24bb\u24ba, which has an\ninternationalized domain but ASCII local part, the returned object is:\nValidatedEmail(\n  normalized='example@\u30c4.life',\n  local_part='example',\n  domain='\u30c4.life',\n  ascii_email='example@xn--bdk.life',\n  ascii_local_part='example',\n  ascii_domain='xn--bdk.life',\n  smtputf8=False)\nNote that normalized and other fields provide a normalized form of the\nemail address, domain name, and (in other cases) local part (see earlier\ndiscussion of normalization), which you should use in your database.\nCalling validate_email with the ASCII form of the above email address,\nexample@xn--bdk.life, returns the exact same information (i.e., the\nnormalized field always will contain Unicode characters, not Punycode).\nFor the fictitious address \u30c4-test@joshdata.me, which has an\ninternationalized local part, the returned object is:\nValidatedEmail(\n  normalized='\u30c4-test@joshdata.me',\n  local_part='\u30c4-test',\n  domain='joshdata.me',\n  ascii_email=None,\n  ascii_local_part=None,\n  ascii_domain='joshdata.me',\n  smtputf8=True)\nNow smtputf8 is True and ascii_email is None because the local\npart of the address is internationalized. The local_part and normalized fields\nreturn the normalized form of the address.\nReturn value\nWhen an email address passes validation, the fields in the returned object\nare:\n\n\n\nField\nValue\n\n\n\n\nnormalized\nThe normalized form of the email address that you should put in your database. This combines the local_part and domain fields (see below).\n\n\nascii_email\nIf set, an ASCII-only form of the normalized email address by replacing the domain part with IDNA Punycode. This field will be present when an ASCII-only form of the email address exists (including if the email address is already ASCII). If the local part of the email address contains internationalized characters, ascii_email will be None. If set, it merely combines ascii_local_part and ascii_domain.\n\n\nlocal_part\nThe normalized local part of the given email address (before the @-sign). Normalization includes Unicode NFC normalization and removing unnecessary quoted-string quotes and backslashes. If allow_quoted_local is True and the surrounding quotes are necessary, the quotes will be present in this field.\n\n\nascii_local_part\nIf set, the local part, which is composed of ASCII characters only.\n\n\ndomain\nThe canonical internationalized Unicode form of the domain part of the email address. If the returned string contains non-ASCII characters, either the SMTPUTF8 feature of your mail relay will be required to transmit the message or else the email address's domain part must be converted to IDNA ASCII first: Use ascii_domain field instead.\n\n\nascii_domain\nThe IDNA Punycode-encoded form of the domain part of the given email address, as it would be transmitted on the wire.\n\n\ndomain_address\nIf domain literals are allowed and if the email address contains one, an ipaddress.IPv4Address or ipaddress.IPv6Address object.\n\n\nsmtputf8\nA boolean indicating that the SMTPUTF8 feature of your mail relay will be required to transmit messages to this address because the local part of the address has non-ASCII characters (the local part cannot be IDNA-encoded). If allow_smtputf8=False is passed as an argument, this flag will always be false because an exception is raised if it would have been true.\n\n\nmx\nA list of (priority, domain) tuples of MX records specified in the DNS for the domain (see RFC 5321 section 5). May be None if the deliverability check could not be completed because of a temporary issue like a timeout.\n\n\nmx_fallback_type\nNone if an MX record is found. If no MX records are actually specified in DNS and instead are inferred, through an obsolete mechanism, from A or AAAA records, the value is the type of DNS record used instead (A or AAAA). May be None if the deliverability check could not be completed because of a temporary issue like a timeout.\n\n\nspf\nAny SPF record found while checking deliverability. Only set if the SPF record is queried.\n\n\n\nAssumptions\nBy design, this validator does not pass all email addresses that\nstrictly conform to the standards. Many email address forms are obsolete\nor likely to cause trouble:\n\nThe validator assumes the email address is intended to be\nusable on the public Internet. The domain part\nof the email address must be a resolvable domain name\n(see the deliverability checks described above).\nMost Special Use Domain Names\nand their subdomains, as well as\ndomain names without a ., are rejected as a syntax error\n(except see the test_environment parameter above).\nObsolete email syntaxes are rejected:\nThe unusual \"(comment)\" syntax\nis rejected. Extremely old obsolete syntaxes are\nrejected. Quoted-string local parts and domain-literal addresses\nare rejected by default, but there are options to allow them (see above).\nNo one uses these forms anymore, and I can't think of any reason why anyone\nusing this library would need to accept them.\n\nTesting\nTests can be run using\npip install -r test_requirements.txt \nmake test\nTests run with mocked DNS responses. When adding or changing tests, temporarily turn on the BUILD_MOCKED_DNS_RESPONSE_DATA flag in tests/mocked_dns_responses.py to re-build the database of mocked responses from live queries.\nFor Project Maintainers\nThe package is distributed as a universal wheel and as a source package.\nTo release:\n\nUpdate CHANGELOG.md.\nUpdate the version number in email_validator/version.py.\nMake & push a commit with the new version number and make sure tests pass.\nMake & push a tag (see command below).\nMake a release at https://github.com/JoshData/python-email-validator/releases/new.\nPublish a source and wheel distribution to pypi (see command below).\n\ngit tag v$(grep version setup.cfg | sed \"s/.*= //\")\ngit push --tags\n./release_to_pypi.sh\n\n\n"}, {"name": "databricks-sql-connector", "readme": "\nDatabricks SQL Connector for Python\n\n\nThe Databricks SQL Connector for Python allows you to develop Python applications that connect to Databricks clusters and SQL warehouses. It is a Thrift-based client with no dependencies on ODBC or JDBC. It conforms to the Python DB API 2.0 specification and exposes a SQLAlchemy dialect for use with tools like pandas and alembic which use SQLAlchemy to execute DDL.\nThis connector uses Arrow as the data-exchange format, and supports APIs to directly fetch Arrow tables. Arrow tables are wrapped in the ArrowQueue class to provide a natural API to get several rows at a time.\nYou are welcome to file an issue here for general use cases. You can also contact Databricks Support here.\nRequirements\nPython 3.7 or above is required.\nDocumentation\nFor the latest documentation, see\n\nDatabricks\nAzure Databricks\n\nQuickstart\nInstall the library with pip install databricks-sql-connector\nNote: Don't hard-code authentication secrets into your Python. Use environment variables\nexport DATABRICKS_HOST=********.databricks.com\nexport DATABRICKS_HTTP_PATH=/sql/1.0/endpoints/****************\nexport DATABRICKS_TOKEN=dapi********************************\n\nExample usage:\nimport os\nfrom databricks import sql\n\nhost = os.getenv(\"DATABRICKS_HOST\")\nhttp_path = os.getenv(\"DATABRICKS_HTTP_PATH\")\naccess_token = os.getenv(\"DATABRICKS_TOKEN\")\n\nconnection = sql.connect(\n  server_hostname=host,\n  http_path=http_path,\n  access_token=access_token)\n\ncursor = connection.cursor()\n\ncursor.execute('SELECT * FROM RANGE(10)')\nresult = cursor.fetchall()\nfor row in result:\n  print(row)\n\ncursor.close()\nconnection.close()\n\nIn the above example:\n\nserver-hostname is the Databricks instance host name.\nhttp-path is the HTTP Path either to a Databricks SQL endpoint (e.g. /sql/1.0/endpoints/1234567890abcdef),\nor to a Databricks Runtime interactive cluster (e.g. /sql/protocolv1/o/1234567890123456/1234-123456-slid123)\npersonal-access-token is the Databricks Personal Access Token for the account that will execute commands and queries\n\nContributing\nSee CONTRIBUTING.md\nLicense\nApache License 2.0\n"}, {"name": "compressed-rtf", "readme": "\n\n\n\n\n\n\n\n\n\n\n\ncompressed_rtf\nDescription:\nUsage example:\nLicense:\n\n\n\n\n\nREADME.md\n\n\n\n\ncompressed_rtf\n\n\n\n\nCompressed Rich Text Format (RTF) compression worker in Python\nDescription:\nCompressed RTF also known as \"LZFu\" compression format\nBased on Rich Text Format (RTF) Compression Algorithm:\nhttps://msdn.microsoft.com/en-us/library/cc463890(v=exchg.80).aspx\nUsage example:\n>>> from compressed_rtf import compress, decompress\n>>>\n>>> data = '{\\\\rtf1\\\\ansi\\\\ansicpg1252\\\\pard test}'\n>>> comp = compress(data, compressed=True)  # compressed\n>>> comp\n'#\\x00\\x00\\x00\"\\x00\\x00\\x00LZFu3\\\\\\xe8t\\x03\\x00\\n\\x00rcpg125\\x922\\n\\xf3 t\\x07\\x90t}\\x0f\\x10'\n>>>\n>>> raw = compress(data, compressed=False)  # raw/uncompressed\n>>> raw\n'.\\x00\\x00\\x00\"\\x00\\x00\\x00MELA \\xdf\\x12\\xce{\\\\rtf1\\\\ansi\\\\ansicpg1252\\\\pard test}'\n>>>\n>>> decompress(comp)\n'{\\\\rtf1\\\\ansi\\\\ansicpg1252\\\\pard test}'\n>>>\n>>> decompress(raw)\n'{\\\\rtf1\\\\ansi\\\\ansicpg1252\\\\pard test}'\n>>>\nLicense:\nReleased under The MIT License.\n\n\n"}, {"name": "click-plugins", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nclick-plugins\nWhy?\nEnabling Plugins\nDeveloping Plugins\nBroken and Incompatible Plugins\nBest Practices and Extra Credit\nInstallation\nDeveloping\nChangelog\nAuthors\nLicense\n\n\n\n\n\nREADME.rst\n\n\n\n\nclick-plugins\n\n\nAn extension module for click to register\nexternal CLI commands via setuptools entry-points.\n\nWhy?\nLets say you develop a commandline interface and someone requests a new feature\nthat is absolutely related to your project but would have negative consequences\nlike additional dependencies, major refactoring, or maybe its just too domain\nspecific to be supported directly.  Rather than developing a separate standalone\nutility you could offer up a setuptools entry point\nthat allows others to use your commandline utility as a home for their related\nsub-commands.  You get to choose where these sub-commands or sub-groups CAN be\nregistered but the plugin developer gets to choose they ARE registered.  You\ncould have all plugins register alongside the core commands, in a special\nsub-group, across multiple sub-groups, or some combination.\n\nEnabling Plugins\nFor a more detailed example see the examples section.\nThe only requirement is decorating click.group() with click_plugins.with_plugins()\nwhich handles attaching external commands and groups.  In this case the core CLI developer\nregisters CLI plugins from core_package.cli_plugins.\nfrom pkg_resources import iter_entry_points\n\nimport click\nfrom click_plugins import with_plugins\n\n\n@with_plugins(iter_entry_points('core_package.cli_plugins'))\n@click.group()\ndef cli():\n    \"\"\"Commandline interface for yourpackage.\"\"\"\n\n@cli.command()\ndef subcommand():\n    \"\"\"Subcommand that does something.\"\"\"\n\nDeveloping Plugins\nPlugin developers need to register their sub-commands or sub-groups to an\nentry-point in their setup.py that is loaded by the core package.\nfrom setuptools import setup\n\nsetup(\n    name='yourscript',\n    version='0.1',\n    py_modules=['yourscript'],\n    install_requires=[\n        'click',\n    ],\n    entry_points='''\n        [core_package.cli_plugins]\n        cool_subcommand=yourscript.cli:cool_subcommand\n        another_subcommand=yourscript.cli:another_subcommand\n    ''',\n)\n\nBroken and Incompatible Plugins\nAny sub-command or sub-group that cannot be loaded is caught and converted to\na click_plugins.core.BrokenCommand() rather than just crashing the entire\nCLI.  The short-help is converted to a warning message like:\nWarning: could not load plugin. See ``<CLI> <command/group> --help``.\nand if the sub-command or group is executed the entire traceback is printed.\n\nBest Practices and Extra Credit\nOpening a CLI to plugins encourages other developers to independently extend\nfunctionality independently but there is no guarantee these new features will\nbe \"on brand\".  Plugin developers are almost certainly already using features\nin the core package the CLI belongs to so defining commonly used arguments and\noptions in one place lets plugin developers reuse these flags to produce a more\ncohesive CLI.  If the CLI is simple maybe just define them at the top of\nyourpackage/cli.py or for more complex packages something like\nyourpackage/cli/options.py.  These common options need to be easy to find\nand be well documented so that plugin developers know what variable to give to\ntheir sub-command's function and what object they can expect to receive.  Don't\nforget to document non-obvious callbacks.\nKeep in mind that plugin developers also have access to the parent group's\nctx.obj, which is very useful for passing things like verbosity levels or\nconfig values around to sub-commands.\nHere's some code that sub-commands could re-use:\nfrom multiprocessing import cpu_count\n\nimport click\n\njobs_opt = click.option(\n    '-j', '--jobs', metavar='CORES', type=click.IntRange(min=1, max=cpu_count()), default=1,\n    show_default=True, help=\"Process data across N cores.\"\n)\nPlugin developers can access this with:\nimport click\nimport parent_cli_package.cli.options\n\n\n@click.command()\n@parent_cli_package.cli.options.jobs_opt\ndef subcommand(jobs):\n    \"\"\"I do something domain specific.\"\"\"\n\nInstallation\nWith pip:\n$ pip install click-plugins\nFrom source:\n$ git clone https://github.com/click-contrib/click-plugins.git\n$ cd click-plugins\n$ python setup.py install\n\nDeveloping\n$ git clone https://github.com/click-contrib/click-plugins.git\n$ cd click-plugins\n$ pip install -e .\\[dev\\]\n$ pytest tests --cov click_plugins --cov-report term-missing\n\nChangelog\nSee CHANGES.txt\n\nAuthors\nSee AUTHORS.txt\n\nLicense\nSee LICENSE.txt\n\n\n"}, {"name": "charset-normalizer", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharset Detection, for Everyone \ud83d\udc4b\n\u26a1 Performance\n\u2728 Installation\n\ud83d\ude80 Basic Usage\nCLI\nPython\n\ud83d\ude07 Why\n\ud83c\udf70 How\n\u26a1 Known limitations\n\u26a0\ufe0f About Python EOLs\n\ud83d\udc64 Contributing\n\ud83d\udcdd License\n\ud83d\udcbc For Enterprise\n\n\n\n\n\nREADME.md\n\n\n\n\nCharset Detection, for Everyone \ud83d\udc4b\n\nThe Real First Universal Charset Detector\n\n\n\n\n\n\n\n\n\n\n\nFeatured Packages\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA library that helps you read text from an unknown charset encoding. Motivated by chardet,\nI'm trying to resolve the issue by taking a new approach.\nAll IANA character set names for which the Python core library provides codecs are supported.\n\n\n  >>>>> \ud83d\udc49 Try Me Online Now, Then Adopt Me \ud83d\udc48  <<<<<\n\nThis project offers you an alternative to Universal Charset Encoding Detector, also known as Chardet.\n\n\n\nFeature\nChardet\nCharset Normalizer\ncChardet\n\n\n\n\nFast\n\u274c\n\u2705\n\u2705\n\n\nUniversal**\n\u274c\n\u2705\n\u274c\n\n\nReliable without distinguishable standards\n\u274c\n\u2705\n\u2705\n\n\nReliable with distinguishable standards\n\u2705\n\u2705\n\u2705\n\n\nLicense\nLGPL-2.1restrictive\nMIT\nMPL-1.1restrictive\n\n\nNative Python\n\u2705\n\u2705\n\u274c\n\n\nDetect spoken language\n\u274c\n\u2705\nN/A\n\n\nUnicodeDecodeError Safety\n\u274c\n\u2705\n\u274c\n\n\nWhl Size (min)\n193.6 kB\n42 kB\n~200 kB\n\n\nSupported Encoding\n33\n\ud83c\udf89 99\n40\n\n\n\n\n\n** : They are clearly using specific code for a specific encoding even if covering most of used one\nDid you got there because of the logs? See https://charset-normalizer.readthedocs.io/en/latest/user/miscellaneous.html\n\u26a1 Performance\nThis package offer better performance than its counterpart Chardet. Here are some numbers.\n\n\n\nPackage\nAccuracy\nMean per file (ms)\nFile per sec (est)\n\n\n\n\nchardet\n86 %\n200 ms\n5 file/sec\n\n\ncharset-normalizer\n98 %\n10 ms\n100 file/sec\n\n\n\n\n\n\nPackage\n99th percentile\n95th percentile\n50th percentile\n\n\n\n\nchardet\n1200 ms\n287 ms\n23 ms\n\n\ncharset-normalizer\n100 ms\n50 ms\n5 ms\n\n\n\nChardet's performance on larger file (1MB+) are very poor. Expect huge difference on large payload.\n\nStats are generated using 400+ files using default parameters. More details on used files, see GHA workflows.\nAnd yes, these results might change at any time. The dataset can be updated to include more files.\nThe actual delays heavily depends on your CPU capabilities. The factors should remain the same.\nKeep in mind that the stats are generous and that Chardet accuracy vs our is measured using Chardet initial capability\n(eg. Supported Encoding) Challenge-them if you want.\n\n\u2728 Installation\nUsing pip:\npip install charset-normalizer -U\n\ud83d\ude80 Basic Usage\nCLI\nThis package comes with a CLI.\nusage: normalizer [-h] [-v] [-a] [-n] [-m] [-r] [-f] [-t THRESHOLD]\n                  file [file ...]\n\nThe Real First Universal Charset Detector. Discover originating encoding used\non text file. Normalize text to unicode.\n\npositional arguments:\n  files                 File(s) to be analysed\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -v, --verbose         Display complementary information about file if any.\n                        Stdout will contain logs about the detection process.\n  -a, --with-alternative\n                        Output complementary possibilities if any. Top-level\n                        JSON WILL be a list.\n  -n, --normalize       Permit to normalize input file. If not set, program\n                        does not write anything.\n  -m, --minimal         Only output the charset detected to STDOUT. Disabling\n                        JSON output.\n  -r, --replace         Replace file when trying to normalize it instead of\n                        creating a new one.\n  -f, --force           Replace file without asking if you are sure, use this\n                        flag with caution.\n  -t THRESHOLD, --threshold THRESHOLD\n                        Define a custom maximum amount of chaos allowed in\n                        decoded content. 0. <= chaos <= 1.\n  --version             Show version information and exit.\n\nnormalizer ./data/sample.1.fr.srt\nor\npython -m charset_normalizer ./data/sample.1.fr.srt\n\ud83c\udf89 Since version 1.4.0 the CLI produce easily usable stdout result in JSON format.\n{\n    \"path\": \"/home/default/projects/charset_normalizer/data/sample.1.fr.srt\",\n    \"encoding\": \"cp1252\",\n    \"encoding_aliases\": [\n        \"1252\",\n        \"windows_1252\"\n    ],\n    \"alternative_encodings\": [\n        \"cp1254\",\n        \"cp1256\",\n        \"cp1258\",\n        \"iso8859_14\",\n        \"iso8859_15\",\n        \"iso8859_16\",\n        \"iso8859_3\",\n        \"iso8859_9\",\n        \"latin_1\",\n        \"mbcs\"\n    ],\n    \"language\": \"French\",\n    \"alphabets\": [\n        \"Basic Latin\",\n        \"Latin-1 Supplement\"\n    ],\n    \"has_sig_or_bom\": false,\n    \"chaos\": 0.149,\n    \"coherence\": 97.152,\n    \"unicode_path\": null,\n    \"is_preferred\": true\n}\nPython\nJust print out normalized text\nfrom charset_normalizer import from_path\n\nresults = from_path('./my_subtitle.srt')\n\nprint(str(results.best()))\nUpgrade your code without effort\nfrom charset_normalizer import detect\nThe above code will behave the same as chardet. We ensure that we offer the best (reasonable) BC result possible.\nSee the docs for advanced usage : readthedocs.io\n\ud83d\ude07 Why\nWhen I started using Chardet, I noticed that it was not suited to my expectations, and I wanted to propose a\nreliable alternative using a completely different method. Also! I never back down on a good challenge!\nI don't care about the originating charset encoding, because two different tables can\nproduce two identical rendered string.\nWhat I want is to get readable text, the best I can.\nIn a way, I'm brute forcing text decoding. How cool is that ? \ud83d\ude0e\nDon't confuse package ftfy with charset-normalizer or chardet. ftfy goal is to repair unicode string whereas charset-normalizer to convert raw file in unknown encoding to unicode.\n\ud83c\udf70 How\n\nDiscard all charset encoding table that could not fit the binary content.\nMeasure noise, or the mess once opened (by chunks) with a corresponding charset encoding.\nExtract matches with the lowest mess detected.\nAdditionally, we measure coherence / probe for a language.\n\nWait a minute, what is noise/mess and coherence according to YOU ?\nNoise : I opened hundred of text files, written by humans, with the wrong encoding table. I observed, then\nI established some ground rules about what is obvious when it seems like a mess.\nI know that my interpretation of what is noise is probably incomplete, feel free to contribute in order to\nimprove or rewrite it.\nCoherence : For each language there is on earth, we have computed ranked letter appearance occurrences (the best we can). So I thought\nthat intel is worth something here. So I use those records against decoded text to check if I can detect intelligent design.\n\u26a1 Known limitations\n\nLanguage detection is unreliable when text contains two or more languages sharing identical letters. (eg. HTML (english tags) + Turkish content (Sharing Latin characters))\nEvery charset detector heavily depends on sufficient content. In common cases, do not bother run detection on very tiny content.\n\n\u26a0\ufe0f About Python EOLs\nIf you are running:\n\nPython >=2.7,<3.5: Unsupported\nPython 3.5: charset-normalizer < 2.1\nPython 3.6: charset-normalizer < 3.1\nPython 3.7: charset-normalizer < 4.0\n\nUpgrade your Python interpreter as soon as possible.\n\ud83d\udc64 Contributing\nContributions, issues and feature requests are very much welcome.\nFeel free to check issues page if you want to contribute.\n\ud83d\udcdd License\nCopyright \u00a9 Ahmed TAHRI @Ousret.\nThis project is MIT licensed.\nCharacters frequencies used in this project \u00a9 2012 Denny Vrande\u010di\u0107\n\ud83d\udcbc For Enterprise\nProfessional support for charset-normalizer is available as part of the Tidelift\nSubscription. Tidelift gives software development teams a single source for\npurchasing and maintaining their software, with professional grade assurances\nfrom the experts who know it best, while seamlessly integrating with existing\ntools.\n\n\n"}, {"name": "catalogue", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncatalogue: Super lightweight function registries for your library\n\u23f3 Installation\n\ud83d\udc69\u200d\ud83d\udcbb Usage\n\u2753 FAQ\nBut can't the user just pass in the custom_loader function directly?\nHow do I make sure all of the registration decorators have run?\n\ud83c\udf9b API\nfunctioncatalogue.create\nclassRegistry\nmethodRegistry.__init__\nmethodRegistry.__contains__\nmethodRegistry.__call__\nmethodRegistry.register\nmethodRegistry.get\nmethodRegistry.get_all\nmethodRegistry.get_entry_points\nmethodRegistry.get_entry_point\nmethodRegistry.find\nfunctioncatalogue.check_exists\n\n\n\n\n\nREADME.md\n\n\n\n\n\ncatalogue: Super lightweight function registries for your library\ncatalogue is a tiny, zero-dependencies library that makes it easy to add\nfunction (or object) registries to your code. Function registries are helpful\nwhen you have objects that need to be both easily serializable and fully\ncustomizable. Instead of passing a function into your object, you pass in an\nidentifier name, which the object can use to lookup the function from the\nregistry. This makes the object easy to serialize, because the name is a simple\nstring. If you instead saved the function, you'd have to use Pickle for\nserialization, which has many drawbacks.\n\n\n\n\n\n\u23f3 Installation\npip install catalogue\nconda install -c conda-forge catalogue\n\n\u26a0\ufe0f Important note: catalogue v3.0+ is only compatible with Python 3.8+.\nFor Python 3.6+ compatibility, use catalogue v2.x and for Python 2.7+\ncompatibility, use catalogue v1.x.\n\n\ud83d\udc69\u200d\ud83d\udcbb Usage\nLet's imagine you're developing a Python package that needs to load data\nsomewhere. You've already implemented some loader functions for the most common\ndata types, but you want to allow the user to easily add their own. Using\ncatalogue.create you can create a new registry under the namespace\nyour_package \u2192 loaders.\n# YOUR PACKAGE\nimport catalogue\n\nloaders = catalogue.create(\"your_package\", \"loaders\")\nThis gives you a loaders.register decorator that your users can import and\ndecorate their custom loader functions with.\n# USER CODE\nfrom your_package import loaders\n\n@loaders.register(\"custom_loader\")\ndef custom_loader(data):\n    # Load something here...\n    return data\nThe decorated function will be registered automatically and in your package,\nyou'll be able to access all loaders by calling loaders.get_all.\n# YOUR PACKAGE\ndef load_data(data, loader_id):\n    print(\"All loaders:\", loaders.get_all()) # {\"custom_loader\": <custom_loader>}\n    loader = loaders.get(loader_id)\n    return loader(data)\nThe user can now refer to their custom loader using only its string name\n(\"custom_loader\") and your application will know what to do and will use their\ncustom function.\n# USER CODE\nfrom your_package import load_data\n\nload_data(data, loader_id=\"custom_loader\")\n\u2753 FAQ\nBut can't the user just pass in the custom_loader function directly?\nSure, that's the more classic callback approach. Instead of a string ID,\nload_data could also take a function, in which case you wouldn't need a\npackage like this. catalogue helps you when you need to produce a serializable\nrecord of which functions were passed in. For instance, you might want to write\na log message, or save a config to load back your object later. With\ncatalogue, your functions can be parameterized by strings, so logging and\nserialization remains easy \u2013 while still giving you full extensibility.\nHow do I make sure all of the registration decorators have run?\nDecorators normally run when modules are imported. Relying on this side-effect\ncan sometimes lead to confusion, especially if there's no other reason the\nmodule would be imported. One solution is to use\nentry points.\nFor instance, in spaCy we're starting to use function\nregistries to make the pipeline components much more customizable. Let's say one\nuser, Jo, develops a better tagging model using new machine learning research.\nEnd-users of Jo's package should be able to write\nspacy.load(\"jo_tagging_model\"). They shouldn't need to remember to write\nimport jos_tagged_model first, just to run the function registries as a\nside-effect. With entry points, the registration happens at install time \u2013 so\nyou don't need to rely on the import side-effects.\n\ud83c\udf9b API\nfunction catalogue.create\nCreate a new registry for a given namespace. Returns a setter function that can\nbe used as a decorator or called with a name and func keyword argument. If\nentry_points=True is set, the registry will check for\nPython entry points\nadvertised for the given namespace, e.g. the entry point group\nspacy_architectures for the namespace \"spacy\", \"architectures\", in\nRegistry.get and Registry.get_all. This allows other packages to\nauto-register functions.\n\n\n\nArgument\nType\nDescription\n\n\n\n\n*namespace\nstr\nThe namespace, e.g. \"spacy\" or \"spacy\", \"architectures\".\n\n\nentry_points\nbool\nWhether to check for entry points of the given namespace and pre-populate the global registry.\n\n\nRETURNS\nRegistry\nThe Registry object with methods to register and retrieve functions.\n\n\n\narchitectures = catalogue.create(\"spacy\", \"architectures\")\n\n# Use as decorator\n@architectures.register(\"custom_architecture\")\ndef custom_architecture():\n    pass\n\n# Use as regular function\narchitectures.register(\"custom_architecture\", func=custom_architecture)\nclass Registry\nThe registry object that can be used to register and retrieve functions. It's\nusually created internally when you call catalogue.create.\nmethod Registry.__init__\nInitialize a new registry. If entry_points=True is set, the registry will\ncheck for\nPython entry points\nadvertised for the given namespace, e.g. the entry point group\nspacy_architectures for the namespace \"spacy\", \"architectures\", in\nRegistry.get and Registry.get_all.\n\n\n\nArgument\nType\nDescription\n\n\n\n\nnamespace\nTuple[str]\nThe namespace, e.g. \"spacy\" or \"spacy\", \"architectures\".\n\n\nentry_points\nbool\nWhether to check for entry points of the given namespace in get and get_all.\n\n\nRETURNS\nRegistry\nThe newly created object.\n\n\n\n# User-facing API\narchitectures = catalogue.create(\"spacy\", \"architectures\")\n# Internal API\narchitectures = Registry((\"spacy\", \"architectures\"))\nmethod Registry.__contains__\nCheck whether a name is in the registry.\n\n\n\nArgument\nType\nDescription\n\n\n\n\nname\nstr\nThe name to check.\n\n\nRETURNS\nbool\nWhether the name is in the registry.\n\n\n\narchitectures = catalogue.create(\"spacy\", \"architectures\")\n\n@architectures.register(\"custom_architecture\")\ndef custom_architecture():\n    pass\n\nassert \"custom_architecture\" in architectures\nmethod Registry.__call__\nRegister a function in the registry's namespace. Can be used as a decorator or\ncalled as a function with the func keyword argument supplying the function to\nregister. Delegates to Registry.register.\nmethod Registry.register\nRegister a function in the registry's namespace. Can be used as a decorator or\ncalled as a function with the func keyword argument supplying the function to\nregister.\n\n\n\nArgument\nType\nDescription\n\n\n\n\nname\nstr\nThe name to register under the namespace.\n\n\nfunc\nAny\nOptional function to register (if not used as decorator).\n\n\nRETURNS\nCallable\nThe decorator that takes one argument, the name.\n\n\n\narchitectures = catalogue.create(\"spacy\", \"architectures\")\n\n# Use as decorator\n@architectures.register(\"custom_architecture\")\ndef custom_architecture():\n    pass\n\n# Use as regular function\narchitectures.register(\"custom_architecture\", func=custom_architecture)\nmethod Registry.get\nGet a function registered in the namespace.\n\n\n\nArgument\nType\nDescription\n\n\n\n\nname\nstr\nThe name.\n\n\nRETURNS\nAny\nThe registered function.\n\n\n\ncustom_architecture = architectures.get(\"custom_architecture\")\nmethod Registry.get_all\nGet all functions in the registry's namespace.\n\n\n\nArgument\nType\nDescription\n\n\n\n\nRETURNS\nDict[str, Any]\nThe registered functions, keyed by name.\n\n\n\nall_architectures = architectures.get_all()\n# {\"custom_architecture\": <custom_architecture>}\nmethod Registry.get_entry_points\nGet registered entry points from other packages for this namespace. The name of\nthe entry point group is the namespace joined by _.\n\n\n\nArgument\nType\nDescription\n\n\n\n\nRETURNS\nDict[str, Any]\nThe loaded entry points, keyed by name.\n\n\n\narchitectures = catalogue.create(\"spacy\", \"architectures\", entry_points=True)\n# Will get all entry points of the group \"spacy_architectures\"\nall_entry_points = architectures.get_entry_points()\nmethod Registry.get_entry_point\nCheck if registered entry point is available for a given name in the namespace\nand load it. Otherwise, return the default value.\n\n\n\nArgument\nType\nDescription\n\n\n\n\nname\nstr\nName of entry point to load.\n\n\ndefault\nAny\nThe default value to return. Defaults to None.\n\n\nRETURNS\nAny\nThe loaded entry point or the default value.\n\n\n\narchitectures = catalogue.create(\"spacy\", \"architectures\", entry_points=True)\n# Will get entry point \"custom_architecture\" of the group \"spacy_architectures\"\ncustom_architecture = architectures.get_entry_point(\"custom_architecture\")\nmethod Registry.find\nFind the information about a registered function, including the module and path\nto the file it's defined in, the line number and the docstring, if available.\n\n\n\nArgument\nType\nDescription\n\n\n\n\nname\nstr\nName of the registered function.\n\n\nRETURNS\nDict[str, Union[str, int]]\nThe information about the function.\n\n\n\nimport catalogue\n\narchitectures = catalogue.create(\"spacy\", \"architectures\", entry_points=True)\n\n@architectures(\"my_architecture\")\ndef my_architecture():\n    \"\"\"This is an architecture\"\"\"\n    pass\n\ninfo = architectures.find(\"my_architecture\")\n# {'module': 'your_package.architectures',\n#  'file': '/path/to/your_package/architectures.py',\n#  'line_no': 5,\n#  'docstring': 'This is an architecture'}\nfunction catalogue.check_exists\nCheck if a namespace exists.\n\n\n\nArgument\nType\nDescription\n\n\n\n\n*namespace\nstr\nThe namespace, e.g. \"spacy\" or \"spacy\", \"architectures\".\n\n\nRETURNS\nbool\nWhether the namespace exists.\n\n\n\n\n\n"}, {"name": "camelot-py", "readme": "\n\n\n\nCamelot: PDF Table Extraction for Humans\n \n\n   \n\nCamelot is a Python library that can help you extract tables from PDFs!\nNote: You can also check out Excalibur, the web interface to Camelot!\n\nHere's how you can extract tables from PDFs. You can check out the PDF used in this example here.\n>>> import camelot\n>>> tables = camelot.read_pdf('foo.pdf')\n>>> tables\n<TableList n=1>\n>>> tables.export('foo.csv', f='csv', compress=True) # json, excel, html, markdown, sqlite\n>>> tables[0]\n<Table shape=(7, 7)>\n>>> tables[0].parsing_report\n{\n    'accuracy': 99.02,\n    'whitespace': 12.24,\n    'order': 1,\n    'page': 1\n}\n>>> tables[0].to_csv('foo.csv') # to_json, to_excel, to_html, to_markdown, to_sqlite\n>>> tables[0].df # get a pandas DataFrame!\n\n\n\n\nCycle Name\nKI (1/km)\nDistance (mi)\nPercent Fuel Savings\n\n\n\n\n\n\n\n\n\n\nImproved Speed\nDecreased Accel\nEliminate Stops\nDecreased Idle\n\n\n2012_2\n3.30\n1.3\n5.9%\n9.5%\n29.2%\n17.4%\n\n\n2145_1\n0.68\n11.2\n2.4%\n0.1%\n9.5%\n2.7%\n\n\n4234_1\n0.59\n58.7\n8.5%\n1.3%\n8.5%\n3.3%\n\n\n2032_2\n0.17\n57.8\n21.7%\n0.3%\n2.7%\n1.2%\n\n\n4171_1\n0.07\n173.9\n58.1%\n1.6%\n2.1%\n0.5%\n\n\n\nCamelot also comes packaged with a command-line interface!\nNote: Camelot only works with text-based PDFs and not scanned documents. (As Tabula explains, \"If you can click and drag to select text in your table in a PDF viewer, then your PDF is text-based\".)\nYou can check out some frequently asked questions here.\nWhy Camelot?\n\nConfigurability: Camelot gives you control over the table extraction process with tweakable settings.\nMetrics: You can discard bad tables based on metrics like accuracy and whitespace, without having to manually look at each table.\nOutput: Each table is extracted into a pandas DataFrame, which seamlessly integrates into ETL and data analysis workflows. You can also export tables to multiple formats, which include CSV, JSON, Excel, HTML, Markdown, and Sqlite.\n\nSee comparison with similar libraries and tools.\nSupport the development\nIf Camelot has helped you, please consider supporting its development with a one-time or monthly donation on OpenCollective.\nInstallation\nUsing conda\nThe easiest way to install Camelot is with conda, which is a package manager and environment management system for the Anaconda distribution.\n$ conda install -c conda-forge camelot-py\n\nUsing pip\nAfter installing the dependencies (tk and ghostscript), you can also just use pip to install Camelot:\n$ pip install \"camelot-py[base]\"\n\nFrom the source code\nAfter installing the dependencies, clone the repo using:\n$ git clone https://www.github.com/camelot-dev/camelot\n\nand install Camelot using pip:\n$ cd camelot\n$ pip install \".[base]\"\n\nDocumentation\nThe documentation is available at http://camelot-py.readthedocs.io/.\nWrappers\n\ncamelot-php provides a PHP wrapper on Camelot.\n\nContributing\nThe Contributor's Guide has detailed information about contributing issues, documentation, code, and tests.\nVersioning\nCamelot uses Semantic Versioning. For the available versions, see the tags on this repository. For the changelog, you can check out HISTORY.md.\nLicense\nThis project is licensed under the MIT License, see the LICENSE file for details.\n"}, {"name": "CairoSVG", "readme": "\n\n\n\nREADME.rst\n\n\n\n\nCairoSVG is an SVG converter based on Cairo. It can export SVG files to PDF,\nEPS, PS, and PNG files.\n\nFree software: LGPL license\nFor Python 3.7+, tested on CPython and PyPy\nDocumentation: https://cairosvg.org/documentation/\nChangelog: https://github.com/Kozea/CairoSVG/releases\nCode, issues, tests: https://github.com/Kozea/CairoSVG\nCode of conduct: https://www.courtbouillon.org/code-of-conduct\nProfessional support: https://www.courtbouillon.org\nDonation: https://opencollective.com/courtbouillon\n\nCairoSVG has been created and developed by Kozea (https://kozea.fr).\nProfessional support, maintenance and community management is provided by\nCourtBouillon (https://www.courtbouillon.org).\nCopyrights are retained by their contributors, no copyright assignment is\nrequired to contribute to CairoSVG. Unless explicitly stated otherwise, any\ncontribution intentionally submitted for inclusion is licensed under the LGPL\nlicense, without any additional terms or conditions. For full\nauthorship information, see the version control history.\n\n\n"}, {"name": "cairocffi", "readme": "\n\n\n\nREADME.rst\n\n\n\n\ncairocffi is a CFFI-based drop-in replacement for Pycairo,\na set of Python bindings and object-oriented API for cairo.\nCairo is a 2D vector graphics library with support for multiple backends\nincluding image buffers, PNG, PostScript, PDF, and SVG file output.\nAdditionally, the cairocffi.pixbuf module uses GDK-PixBuf\nto decode various image formats for use in cairo.\n\nFree software: BSD license\nFor Python 3.7+, tested on CPython and PyPy\nDocumentation: https://doc.courtbouillon.org/cairocffi/\nChangelog: https://doc.courtbouillon.org/cairocffi/stable/changelog.html\nCode, issues, tests: https://github.com/Kozea/cairocffi\nCode of conduct: https://www.courtbouillon.org/code-of-conduct\nProfessional support: https://www.courtbouillon.org\nDonation: https://opencollective.com/courtbouillon\nAPI partially compatible with Pycairo.\nWorks with any version of cairo.\n\ncairocffi has been created and developed by Kozea (https://kozea.fr).\nProfessional support, maintenance and community management is provided by\nCourtBouillon (https://www.courtbouillon.org).\nCopyrights are retained by their contributors, no copyright assignment is\nrequired to contribute to cairocffi. Unless explicitly stated otherwise, any\ncontribution intentionally submitted for inclusion is licensed under the BSD\n3-clause license, without any additional terms or conditions. For full\nauthorship information, see the version control history.\n\n\n"}, {"name": "cachetools", "readme": "\n\n\n\n\n\n\n\n\n\n\n\ncachetools\nInstallation\nProject Resources\nRelated Projects\nLicense\n\n\n\n\n\nREADME.rst\n\n\n\n\ncachetools\n\n\n\n\n\n\n\n\n\nThis module provides various memoizing collections and decorators,\nincluding variants of the Python Standard Library's @lru_cache\nfunction decorator.\nfrom cachetools import cached, LRUCache, TTLCache\n\n# speed up calculating Fibonacci numbers with dynamic programming\n@cached(cache={})\ndef fib(n):\n    return n if n < 2 else fib(n - 1) + fib(n - 2)\n\n# cache least recently used Python Enhancement Proposals\n@cached(cache=LRUCache(maxsize=32))\ndef get_pep(num):\n    url = 'http://www.python.org/dev/peps/pep-%04d/' % num\n    with urllib.request.urlopen(url) as s:\n        return s.read()\n\n# cache weather data for no longer than ten minutes\n@cached(cache=TTLCache(maxsize=1024, ttl=600))\ndef get_weather(place):\n    return owm.weather_at_place(place).get_weather()\nFor the purpose of this module, a cache is a mutable mapping of a\nfixed maximum size.  When the cache is full, i.e. by adding another\nitem the cache would exceed its maximum size, the cache must choose\nwhich item(s) to discard based on a suitable cache algorithm.\nThis module provides multiple cache classes based on different cache\nalgorithms, as well as decorators for easily memoizing function and\nmethod calls.\n\nInstallation\ncachetools is available from PyPI and can be installed by running:\npip install cachetools\n\nTyping stubs for this package are provided by typeshed and can be\ninstalled by running:\npip install types-cachetools\n\n\nProject Resources\n\nDocumentation\nIssue tracker\nSource code\nChange log\n\n\nRelated Projects\n\nasyncache: Helpers to use cachetools with async functions\nCacheToolsUtils: Cachetools Utilities\nkids.cache: Kids caching library\nshelved-cache: Persistent cache for Python cachetools\n\n\nLicense\nCopyright (c) 2014-2023 Thomas Kemmer.\nLicensed under the MIT License.\n\n\n"}, {"name": "basemap", "readme": "\nbasemap\nPlot on map projections (with coastlines and political boundaries) using\nmatplotlib.\nThis package depends on the support package basemap-data with the\nbasic basemap data assets, and optionally on the support package\nbasemap-data-hires with high-resolution data assets.\nInstallation\nPrecompiled binary wheels for Windows and GNU/Linux are available in\nPyPI (architectures x86 and x64, Python 2.7 and 3.5+) and can be\ninstalled with pip:\npython -m pip install basemap\n\nIf you need to install from source, please visit the\nGitHub repository for a\nstep-by-step description.\nLicense\nThe library is licensed under the terms of the MIT license (see\nLICENSE). The GEOS dynamic library bundled with the package wheels\nis provided under the terms of the LGPLv2.1 license as given in\nLICENSE.geos.\n"}, {"name": "basemap-data", "readme": "\nbasemap-data\nPlot on map projections (with coastlines and political boundaries) using\nmatplotlib.\nThis is a support package for basemap with the basic data assets\nrequired by basemap to work.\nInstallation\nThe package is available in PyPI and can be installed with pip:\npython -m pip install basemap-data\n\nLicense\nThe land-sea mask, coastline, lake, river and political boundary data\nare extracted from the GSHHG datasets (version 2.3.6) using GMT\n(5.x series) and are included under the terms of the LGPLv3+ license\n(see COPYING and COPYING.LESSER).\nThe other files are included under the terms of the MIT license. See\nLICENSE.epsg for the EPSG file (taken from the PROJ.4 package) and\nLICENSE.mit for the rest.\n"}, {"name": "backports.zoneinfo", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nbackports.zoneinfo: Backport of the standard library module zoneinfo\nInstallation and depending on this library\nUse\nContributing\n\n\n\n\n\nREADME.md\n\n\n\n\nbackports.zoneinfo: Backport of the standard library module zoneinfo\nThis package was originally the reference implementation for PEP 615, which proposes support for the IANA time zone database in the standard library, and now serves as a backport to Python 3.6+ (including PyPy).\nThis exposes the backports.zoneinfo module, which is a backport of the zoneinfo module. The backport's documentation can be found on readthedocs.\nThe module uses the system time zone data if available, and falls back to the tzdata package (available on PyPI) if installed.\nInstallation and depending on this library\nThis module is called backports.zoneinfo on PyPI. To install it in your local environment, use:\npip install backports.zoneinfo\n\nOr (particularly on Windows), you can also use the tzdata extra (which basically just declares a dependency on tzdata, so this doesn't actually save you any typing \ud83d\ude05):\npip install backports.zoneinfo[tzdata]\n\nIf you want to use this in your application, it is best to use PEP 508 environment markers to declare a dependency conditional on the Python version:\nbackports.zoneinfo;python_version<\"3.9\"\n\nSupport for backports.zoneinfo in Python 3.9+ is currently minimal, since it is expected that you would use the standard library zoneinfo module instead.\nUse\nThe backports.zoneinfo module should be a drop-in replacement for the Python 3.9 standard library module zoneinfo. If you do not support anything earlier than Python 3.9, you do not need this library; if you are supporting Python 3.6+, you may want to use this idiom to \"fall back\" to backports.zoneinfo:\ntry:\n    import zoneinfo\nexcept ImportError:\n    from backports import zoneinfo\nTo get access to time zones with this module, construct a ZoneInfo object and attach it to your datetime:\n>>> from backports.zoneinfo import ZoneInfo\n>>> from datetime import datetime, timedelta, timezone\n>>> dt = datetime(1992, 3, 1, tzinfo=ZoneInfo(\"Europe/Minsk\"))\n>>> print(dt)\n1992-03-01 00:00:00+02:00\n>>> print(dt.utcoffset())\n2:00:00\n>>> print(dt.tzname())\nEET\nArithmetic works as expected without the need for a \"normalization\" step:\n>>> dt += timedelta(days=90)\n>>> print(dt)\n1992-05-30 00:00:00+03:00\n>>> dt.utcoffset()\ndatetime.timedelta(seconds=10800)\n>>> dt.tzname()\n'EEST'\nAmbiguous and imaginary times are handled using the fold attribute added in PEP 495:\n>>> dt = datetime(2020, 11, 1, 1, tzinfo=ZoneInfo(\"America/Chicago\"))\n>>> print(dt)\n2020-11-01 01:00:00-05:00\n>>> print(dt.replace(fold=1))\n2020-11-01 01:00:00-06:00\n\n>>> UTC = timezone.utc\n>>> print(dt.astimezone(UTC))\n2020-11-01 06:00:00+00:00\n>>> print(dt.replace(fold=1).astimezone(UTC))\n2020-11-01 07:00:00+00:00\nContributing\nCurrently we are not accepting contributions to this repository because we have not put the CLA in place and we would like to avoid complicating the process of adoption into the standard library. Contributions to CPython will eventually be backported to this repository \u2014 see the Python developer's guide for more information on how to contribute to CPython.\n\n\n"}, {"name": "async-timeout", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nasync-timeout\nUsage example\nInstallation\nAuthors and License\n\n\n\n\n\nREADME.rst\n\n\n\n\nasync-timeout\n\n\n\n\n\n\n\nasyncio-compatible timeout context manager.\n\nUsage example\nThe context manager is useful in cases when you want to apply timeout\nlogic around block of code or in cases when asyncio.wait_for() is\nnot suitable. Also it's much faster than asyncio.wait_for()\nbecause timeout doesn't create a new task.\nThe timeout(delay, *, loop=None) call returns a context manager\nthat cancels a block on timeout expiring:\nfrom async_timeout import timeout\nasync with timeout(1.5):\n    await inner()\n\n\nIf inner() is executed faster than in 1.5 seconds nothing\nhappens.\nOtherwise inner() is cancelled internally by sending\nasyncio.CancelledError into but asyncio.TimeoutError is\nraised outside of context manager scope.\n\ntimeout parameter could be None for skipping timeout functionality.\nAlternatively, timeout_at(when) can be used for scheduling\nat the absolute time:\nloop = asyncio.get_event_loop()\nnow = loop.time()\n\nasync with timeout_at(now + 1.5):\n    await inner()\n\nPlease note: it is not POSIX time but a time with\nundefined starting base, e.g. the time of the system power on.\nContext manager has .expired property for check if timeout happens\nexactly in context manager:\nasync with timeout(1.5) as cm:\n    await inner()\nprint(cm.expired)\n\nThe property is True if inner() execution is cancelled by\ntimeout context manager.\nIf inner() call explicitly raises TimeoutError cm.expired\nis False.\nThe scheduled deadline time is available as .deadline property:\nasync with timeout(1.5) as cm:\n    cm.deadline\n\nNot finished yet timeout can be rescheduled by shift_by()\nor shift_to() methods:\nasync with timeout(1.5) as cm:\n    cm.shift(1)  # add another second on waiting\n    cm.update(loop.time() + 5)  # reschedule to now+5 seconds\n\nRescheduling is forbidden if the timeout is expired or after exit from async with\ncode block.\n\nInstallation\n$ pip install async-timeout\n\nThe library is Python 3 only!\n\nAuthors and License\nThe module is written by Andrew Svetlov.\nIt's Apache 2 licensed and freely available.\n\n\n"}, {"name": "argon2-cffi", "readme": "\nargon2-cffi: Argon2 for Python\nArgon2 won the Password Hashing Competition and argon2-cffi is the simplest way to use it in Python:\n>>> from argon2 import PasswordHasher\n>>> ph = PasswordHasher()\n>>> hash = ph.hash(\"correct horse battery staple\")\n>>> hash  # doctest: +SKIP\n'$argon2id$v=19$m=65536,t=3,p=4$MIIRqgvgQbgj220jfp0MPA$YfwJSVjtjSU0zzV/P3S9nnQ/USre2wvJMjfCIjrTQbg'\n>>> ph.verify(hash, \"correct horse battery staple\")\nTrue\n>>> ph.check_needs_rehash(hash)\nFalse\n>>> ph.verify(hash, \"Tr0ub4dor&3\")\nTraceback (most recent call last):\n  ...\nargon2.exceptions.VerifyMismatchError: The password does not match the supplied hash\n\nProject Links\n\nPyPI\nGitHub\nDocumentation\nChangelog\nFunding\nThe low-level Argon2 CFFI bindings are maintained in the separate argon2-cffi-bindings project.\n\nRelease Information\nRemoved\n\nPython 3.6 is not supported anymore.\n\nDeprecated\n\n\nThe InvalidHash exception is deprecated in favor of InvalidHashError.\nNo plans for removal currently exist and the names can (but shouldn't) be used interchangeably.\n\n\nargon2.hash_password(), argon2.hash_password_raw(), and argon2.verify_password() that have been soft-deprecated since 2016 are now hard-deprecated.\nThey now raise DeprecationWarnings and will be removed in 2024.\n\n\nAdded\n\n\nOfficial support for Python 3.11 and 3.12.\nNo code changes were necessary.\n\n\nargon2.exceptions.InvalidHashError as a replacement for InvalidHash.\n\n\nsalt parameter to argon2.PasswordHasher.hash() to allow for custom salts.\nThis is only useful for specialized use-cases -- leave it on None unless you know exactly what you are doing.\n#153\n\n\n\n\u2192 Full Changelog\nCredits\nargon2-cffi is maintained by Hynek Schlawack.\nThe development is kindly supported by my employer Variomedia AG, argon2-cffi Tidelift subscribers, and my amazing GitHub Sponsors.\nargon2-cffi for Enterprise\nAvailable as part of the Tidelift Subscription.\nThe maintainers of argon2-cffi and thousands of other packages are working with Tidelift to deliver commercial support and maintenance for the open-source packages you use to build your applications.\nSave time, reduce risk, and improve code health, while paying the maintainers of the exact packages you use.\nLearn more.\n"}, {"name": "argon2-cffi-bindings", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLow-level Python CFFI Bindings for Argon2\nUsage\nDisabling Vendored Code\nOverriding Automatic SSE2 Detection\nPython API\nProject Information\nCredits & License\nVendored Code\nargon2-cffi-bindings for Enterprise\n\n\n\n\n\nREADME.md\n\n\n\n\nLow-level Python CFFI Bindings for Argon2\n\n\n\nargon2-cffi-bindings provides low-level CFFI bindings to the official implementation of the Argon2 password hashing algorithm.\nThe currently vendored Argon2 commit ID is f57e61e.\nNote\nIf you want to hash passwords in an application, this package is not for you.\nHave a look at argon2-cffi with its high-level abstractions!\nThese bindings have been extracted from argon2-cffi and it remains its main consumer.\nHowever, they may be used by other packages that want to use the Argon2 library without dealing with C-related complexities.\nUsage\nargon2-cffi-bindings is available from PyPI.\nThe provided CFFI bindings are compiled in API mode.\nBest effort is given to provide binary wheels for as many platforms as possible.\nDisabling Vendored Code\nA copy of Argon2 is vendored and used by default, but can be disabled if argon2-cffi-bindings is installed using:\n$ env ARGON2_CFFI_USE_SYSTEM=1 \\\n  python -Im pip install --no-binary=argon2-cffi-bindings argon2-cffi-bindings\nOverriding Automatic SSE2 Detection\nUsually the build process tries to guess whether or not it should use SSE2-optimized code (see _ffi_build.py for details).\nThis can go wrong and is problematic for cross-compiling.\nTherefore you can use the ARGON2_CFFI_USE_SSE2 environment variable to control the process:\n\nIf you set it to 1, argon2-cffi-bindings will build with SSE2 support.\nIf you set it to 0, argon2-cffi-bindings will build without SSE2 support.\nIf you set it to anything else, it will be ignored and argon2-cffi-bindings will try to guess.\n\nHowever, if our heuristics fail you, we would welcome a bug report.\nPython API\nSince this package is intended to be an implementation detail, it uses a private module name to prevent your users from using it by accident.\nTherefore you have to import the symbols from _argon2_cffi_bindings:\nfrom _argon2_cffi_bindings import ffi, lib\nPlease refer to cffi documentation on how to use the ffi and lib objects.\nThe list of symbols that are provided can be found in the _ffi_build.py file.\nProject Information\n\nChangelog\nDocumentation\nPyPI\nSource Code\n\nCredits & License\nargon2-cffi-bindings is written and maintained by Hynek Schlawack.\nIt is released under the MIT license.\nThe development is kindly supported by Variomedia AG.\nThe authors of Argon2 were very helpful to get the library to compile on ancient versions of Visual Studio for ancient versions of Python.\nThe documentation quotes frequently in verbatim from the Argon2 paper to avoid mistakes by rephrasing.\nVendored Code\nThe original Argon2 repo can be found at https://github.com/P-H-C/phc-winner-argon2/.\nExcept for the components listed below, the Argon2 code in this repository is copyright (c) 2015 Daniel Dinu, Dmitry Khovratovich (main authors), Jean-Philippe Aumasson and Samuel Neves, and under CC0 license.\nThe string encoding routines in src/encoding.c are copyright (c) 2015 Thomas Pornin, and under CC0 license.\nThe BLAKE2 code in src/blake2/ is copyright (c) Samuel Neves, 2013-2015, and under CC0 license.\nargon2-cffi-bindings for Enterprise\nAvailable as part of the Tidelift Subscription.\nThe maintainers of argon2-cffi-bindings and thousands of other packages are working with Tidelift to deliver commercial support and maintenance for the open-source packages you use to build your applications.\nSave time, reduce risk, and improve code health, while paying the maintainers of the exact packages you use.\nLearn more.\n\n\n"}, {"name": "analytics-python", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nanalytics-python\n\ud83d\ude80 How to get started\n\ud83e\udd14 Why?\n\ud83d\udc68\u200d\ud83d\udcbb Getting Started\n\ud83d\ude80 Startup Program\nDocumentation\nLicense\n\n\n\n\n\nREADME.md\n\n\n\n\nanalytics-python\n\nanalytics-python is a python client for Segment\n\n\nYou can't fix what you can't measure\n\nAnalytics helps you measure your users, product, and business. It unlocks insights into your app's funnel, core business metrics, and whether you have a product-market fit.\n\ud83d\ude80 How to get started\n\nCollect analytics data from your app(s).\n\nThe top 200 Segment companies collect data from 5+ source types (web, mobile, server, CRM, etc.).\n\n\nSend the data to analytics tools (for example, Google Analytics, Amplitude, Mixpanel).\n\nOver 250+ Segment companies send data to eight categories of destinations such as analytics tools, warehouses, email marketing, and remarketing systems, session recording, and more.\n\n\nExplore your data by creating metrics (for example, new signups, retention cohorts, and revenue generation).\n\nThe best Segment companies use retention cohorts to measure product-market fit. Netflix has 70% paid retention after 12 months, 30% after 7 years.\n\n\n\nSegment collects analytics data and allows you to send it to more than 250 apps (such as Google Analytics, Mixpanel, Optimizely, Facebook Ads, Slack, Sentry) just by flipping a switch. You only need one Segment code snippet, and you can turn integrations on and off at will, with no additional code. Sign up with Segment today.\n\ud83e\udd14 Why?\n\n\nPower all your analytics apps with the same data. Instead of writing code to integrate all of your tools individually, send data to Segment, once.\n\n\nInstall tracking for the last time. We're the last integration you'll ever need to write. You only need to instrument Segment once. Reduce all of your tracking code and advertising tags into a single set of API calls.\n\n\nSend data from anywhere. Send Segment data from any device, and we'll transform and send it on to any tool.\n\n\nQuery your data in SQL. Slice, dice, and analyze your data in detail with Segment SQL. We'll transform and load your customer behavioral data directly from your apps into Amazon Redshift, Google BigQuery, or Postgres. Save weeks of engineering time by not having to invent your data warehouse and ETL pipeline.\nFor example, you can capture data on any app:\nanalytics.track('Order Completed', { price: 99.84 })\nThen, query the resulting data in SQL:\nselect * from app.order_completed\norder by price desc\n\n\n\ud83d\udc68\u200d\ud83d\udcbb Getting Started\nInstall segment-analytics-python using pip:\npip3 install segment-analytics-python\nor you can clone this repo:\ngit clone https://github.com/segmentio/analytics-python.git\n\ncd analytics-python\n\nsudo python3 setup.py install\nNow inside your app, you'll want to set your write_key before making any analytics calls:\nimport segment.analytics as analytics\n\nanalytics.write_key = 'YOUR_WRITE_KEY'\nNote If you need to send data to multiple Segment sources, you can initialize a new Client for each write_key\n\ud83d\ude80 Startup Program\n\n\n\nIf you are part of a new startup  (<$5M raised, <2 years since founding), we just launched a new startup program for you. You can get a Segment Team plan  (up to $25,000 value in Segment credits) for free up to 2 years \u2014 apply here!\nDocumentation\nDocumentation is available at https://segment.com/libraries/python.\nLicense\nWWWWWW||WWWWWW\n W W W||W W W\n      ||\n    ( OO )__________\n     /  |           \\\n    /o o|    MIT     \\\n    \\___/||_||__||_|| *\n         || ||  || ||\n        _||_|| _||_||\n       (__|__|(__|__|\n\n(The MIT License)\nCopyright (c) 2013 Segment Inc. friends@segment.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the 'Software'), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\n"}, {"name": "absl-py", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbseil Python Common Libraries\nFeatures\nGetting Started\nInstallation\nRunning Tests\nExample Code\nDocumentation\nFuture Releases\nLicense\n\n\n\n\n\nREADME.md\n\n\n\n\nAbseil Python Common Libraries\nThis repository is a collection of Python library code for building Python\napplications. The code is collected from Google's own Python code base, and has\nbeen extensively tested and used in production.\nFeatures\n\nSimple application startup\nDistributed commandline flags system\nCustom logging module with additional features\nTesting utilities\n\nGetting Started\nInstallation\nTo install the package, simply run:\npip install absl-py\nOr install from source:\npython setup.py install\nRunning Tests\nTo run Abseil tests, you can clone the git repo and run\nbazel:\ngit clone https://github.com/abseil/abseil-py.git\ncd abseil-py\nbazel test absl/...\nExample Code\nPlease refer to\nsmoke_tests/sample_app.py\nas an example to get started.\nDocumentation\nSee the Abseil Python Developer Guide.\nFuture Releases\nThe current repository includes an initial set of libraries for early adoption.\nMore components and interoperability with Abseil C++ Common Libraries\nwill come in future releases.\nLicense\nThe Abseil Python library is licensed under the terms of the Apache\nlicense. See LICENSE for more information.\n\n\n"}, {"name": "unattended-upgrades", "readme": ""}, {"name": "requests-unixsocket", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nrequests-unixsocket\nUsage\nExplicit\nImplicit (monkeypatching)\nAbstract namespace sockets\nSee also\n\n\n\n\n\nREADME.rst\n\n\n\n\nrequests-unixsocket\n\n\n\n\nUse requests to talk HTTP via a UNIX domain socket\n\nUsage\n\nExplicit\nYou can use it by instantiating a special Session object:\nimport json\n\nimport requests_unixsocket\n\nsession = requests_unixsocket.Session()\n\nr = session.get('http+unix://%2Fvar%2Frun%2Fdocker.sock/info')\nregistry_config = r.json()['RegistryConfig']\nprint(json.dumps(registry_config, indent=4))\n\nImplicit (monkeypatching)\nMonkeypatching allows you to use the functionality in this module, while making\nminimal changes to your code. Note that in the above example we had to\ninstantiate a special requests_unixsocket.Session object and call the\nget method on that object. Calling requests.get(url) (the easiest way\nto use requests and probably very common), would not work. But we can make it\nwork by doing monkeypatching.\nYou can monkeypatch globally:\nimport requests_unixsocket\n\nrequests_unixsocket.monkeypatch()\n\nr = requests.get('http+unix://%2Fvar%2Frun%2Fdocker.sock/info')\nassert r.status_code == 200\nor you can do it temporarily using a context manager:\nimport requests_unixsocket\n\nwith requests_unixsocket.monkeypatch():\n    r = requests.get('http+unix://%2Fvar%2Frun%2Fdocker.sock/info')\n    assert r.status_code == 200\n\nAbstract namespace sockets\nTo connect to an abstract namespace\nsocket\n(Linux only), prefix the name with a NULL byte (i.e.: 0) - e.g.:\nimport requests_unixsocket\n\nsession = requests_unixsocket.Session()\nres = session.get('http+unix://\\0test_socket/get')\nprint(res.text)\nFor an example program that illustrates this, see\nexamples/abstract_namespace.py in the git repo. Since abstract namespace\nsockets are specific to Linux, the program will only work on Linux.\n\nSee also\n\nhttps://github.com/httpie/httpie-unixsocket - a plugin for HTTPie that allows you to interact with UNIX domain sockets\n\n\n\n"}, {"name": "python-apt", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nWarehouse\nGetting Started\nDiscussion\nTesting\nCode of Conduct\n\n\n\n\n\nREADME.rst\n\n\n\n\nWarehouse\nWarehouse is the software that powers PyPI.\nSee our development roadmap, documentation, and\narchitectural overview.\n\nGetting Started\nYou can run Warehouse locally in a development environment using\ndocker. See Getting started\ndocumentation for instructions on how to set it up.\nThe canonical deployment of Warehouse is in production at pypi.org.\n\nDiscussion\nYou can find help or get involved on:\n\nGithub issue tracker for reporting issues\nIRC: on Libera, channel #pypa for general packaging discussion\nand user support, and #pypa-dev for\ndiscussions about development of packaging tools\nThe PyPA Discord for live discussions\nThe Packaging category on Discourse for discussing\nnew ideas and community initiatives\n\n\nTesting\nRead the running tests and linters section of our documentation to\nlearn how to test your code.  For cross-browser testing, we use an\nopen source account from BrowserStack. If your pull request makes\nany change to the user interface, it will need to be tested to confirm\nit works in our supported browsers.\n\n\nCode of Conduct\nEveryone interacting in the Warehouse project's codebases, issue trackers, chat\nrooms, and mailing lists is expected to follow the PSF Code of Conduct.\n\n\n"}, {"name": "distro-info", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nWarehouse\nGetting Started\nDiscussion\nTesting\nCode of Conduct\n\n\n\n\n\nREADME.rst\n\n\n\n\nWarehouse\nWarehouse is the software that powers PyPI.\nSee our development roadmap, documentation, and\narchitectural overview.\n\nGetting Started\nYou can run Warehouse locally in a development environment using\ndocker. See Getting started\ndocumentation for instructions on how to set it up.\nThe canonical deployment of Warehouse is in production at pypi.org.\n\nDiscussion\nYou can find help or get involved on:\n\nGithub issue tracker for reporting issues\nIRC: on Libera, channel #pypa for general packaging discussion\nand user support, and #pypa-dev for\ndiscussions about development of packaging tools\nThe PyPA Discord for live discussions\nThe Packaging category on Discourse for discussing\nnew ideas and community initiatives\n\n\nTesting\nRead the running tests and linters section of our documentation to\nlearn how to test your code.  For cross-browser testing, we use an\nopen source account from BrowserStack. If your pull request makes\nany change to the user interface, it will need to be tested to confirm\nit works in our supported browsers.\n\n\nCode of Conduct\nEveryone interacting in the Warehouse project's codebases, issue trackers, chat\nrooms, and mailing lists is expected to follow the PSF Code of Conduct.\n\n\n"}, {"name": "dbus-python", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nWarehouse\nGetting Started\nDiscussion\nTesting\nCode of Conduct\n\n\n\n\n\nREADME.rst\n\n\n\n\nWarehouse\nWarehouse is the software that powers PyPI.\nSee our development roadmap, documentation, and\narchitectural overview.\n\nGetting Started\nYou can run Warehouse locally in a development environment using\ndocker. See Getting started\ndocumentation for instructions on how to set it up.\nThe canonical deployment of Warehouse is in production at pypi.org.\n\nDiscussion\nYou can find help or get involved on:\n\nGithub issue tracker for reporting issues\nIRC: on Libera, channel #pypa for general packaging discussion\nand user support, and #pypa-dev for\ndiscussions about development of packaging tools\nThe PyPA Discord for live discussions\nThe Packaging category on Discourse for discussing\nnew ideas and community initiatives\n\n\nTesting\nRead the running tests and linters section of our documentation to\nlearn how to test your code.  For cross-browser testing, we use an\nopen source account from BrowserStack. If your pull request makes\nany change to the user interface, it will need to be tested to confirm\nit works in our supported browsers.\n\n\nCode of Conduct\nEveryone interacting in the Warehouse project's codebases, issue trackers, chat\nrooms, and mailing lists is expected to follow the PSF Code of Conduct.\n\n\n"}]