[{"name": "xml-python", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nWarehouse\nGetting Started\nDiscussion\nTesting\nCode of Conduct\n\n\n\n\n\nREADME.rst\n\n\n\n\nWarehouse\nWarehouse is the software that powers PyPI.\nSee our development roadmap, documentation, and\narchitectural overview.\n\nGetting Started\nYou can run Warehouse locally in a development environment using\ndocker. See Getting started\ndocumentation for instructions on how to set it up.\nThe canonical deployment of Warehouse is in production at pypi.org.\n\nDiscussion\nYou can find help or get involved on:\n\nGithub issue tracker for reporting issues\nIRC: on Libera, channel #pypa for general packaging discussion\nand user support, and #pypa-dev for\ndiscussions about development of packaging tools\nThe PyPA Discord for live discussions\nThe Packaging category on Discourse for discussing\nnew ideas and community initiatives\n\n\nTesting\nRead the running tests and linters section of our documentation to\nlearn how to test your code.  For cross-browser testing, we use an\nopen source account from BrowserStack. If your pull request makes\nany change to the user interface, it will need to be tested to confirm\nit works in our supported browsers.\n\n\nCode of Conduct\nEveryone interacting in the Warehouse project's codebases, issue trackers, chat\nrooms, and mailing lists is expected to follow the PSF Code of Conduct.\n\n\n"}, {"name": "xarray-einstats", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nxarray-einstats\nInstallation\nOverview\nContributing\nRelevant links\nSimilar projects\nCite xarray-einstats\n\n\n\n\n\nREADME.md\n\n\n\n\nxarray-einstats\n\n\n\n\n\n\nStats, linear algebra and einops for xarray\nInstallation\nTo install, run\n(.venv) $ pip install xarray-einstats\n\nSee the docs for more extensive install instructions.\nOverview\nAs stated in their website:\n\nxarray makes working with multi-dimensional labeled arrays simple, efficient and fun!\n\nThe code is often more verbose, but it is generally because it is clearer and thus less error prone\nand more intuitive.\nHere are some examples of such trade-off where we believe the increased clarity is worth\nthe extra characters:\n\n\n\nnumpy\nxarray\n\n\n\n\na[2, 5]\nda.sel(drug=\"paracetamol\", subject=5)\n\n\na.mean(axis=(0, 1))\nda.mean(dim=(\"chain\", \"draw\"))\n\n\na.reshape((-1, 10))\nda.stack(sample=(\"chain\", \"draw\"))\n\n\na.transpose(2, 0, 1)\nda.transpose(\"drug\", \"chain\", \"draw\")\n\n\n\nIn some other cases however, using xarray can result in overly verbose code\nthat often also becomes less clear. xarray_einstats provides wrappers\naround some numpy and scipy functions (mostly numpy.linalg and scipy.stats)\nand around einops with an api and features adapted to xarray.\nContinue at the getting started page.\nContributing\nxarray-einstats is in active development and all types of contributions are welcome!\nSee the contributing guide for details on how to contribute.\nRelevant links\n\nDocumentation: https://einstats.python.arviz.org/en/latest/\nContributing guide: https://einstats.python.arviz.org/en/latest/contributing/overview.html\nArviZ project website: https://www.arviz.org\n\nSimilar projects\nHere we list some similar projects we know of. Note that all of\nthem are complementary and don't overlap:\n\nxr-scipy\nxarray-extras\nxhistogram\nxrft\n\nCite xarray-einstats\nIf you use this software, please cite it using the following template and the version\nspecific DOI provided by Zenodo. Click on the badge to go to the Zenodo page\nand select the DOI corresponding to the version you used\n\n\nOriol Abril-Pla. (2022). arviz-devs/xarray-einstats <version>. Zenodo. <version_doi>\n\nor in bibtex format:\n@software{xarray_einstats2022,\n  author       = {Abril-Pla, Oriol},\n  title        = {{xarray-einstats}},\n  year         = 2022,\n  url          = {https://github.com/arviz-devs/xarray-einstats}\n  publisher    = {Zenodo},\n  version      = {<version>},\n  doi          = {<version_doi>},\n}\n\n\n\n"}, {"name": "websocket-client", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwebsocket-client\nDocumentation\nContributing\nInstallation\nUsage Tips\nPerformance\nExamples\nLong-lived Connection\nShort-lived Connection\n\n\n\n\n\nREADME.md\n\n\n\n\n\n\n\n\n\nwebsocket-client\nwebsocket-client is a WebSocket client for Python. It provides access\nto low level APIs for WebSockets. websocket-client implements version\nhybi-13\nof the WebSocket protocol. This client does not currently support the\npermessage-deflate extension from\nRFC 7692.\nDocumentation\nThis project's documentation can be found at\nhttps://websocket-client.readthedocs.io/\nContributing\nPlease see the contribution guidelines\nInstallation\nYou can use either python3 setup.py install or pip3 install websocket-client\nto install. This module is tested on Python 3.8+.\nThere are several optional dependencies that can be installed to enable\nspecific websocket-client features.\n\nTo install python-socks for proxy usage and wsaccel for a minor performance boost, use:\npip3 install websocket-client[optional]\nTo install websockets to run unit tests using the local echo server, use:\npip3 install websocket-client[test]\nTo install Sphinx and sphinx_rtd_theme to build project documentation, use:\npip3 install websocket-client[docs]\n\nWhile not a strict dependency, rel\nis useful when using run_forever with automatic reconnect. Install rel with pip3 install rel.\nFootnote: Some shells, such as zsh, require you to escape the [ and ] characters with a \\.\nUsage Tips\nCheck out the documentation's FAQ for additional guidelines:\nhttps://websocket-client.readthedocs.io/en/latest/faq.html\nKnown issues with this library include lack of WebSocket Compression\nsupport (RFC 7692) and minimal threading documentation/support.\nPerformance\nThe send and validate_utf8 methods can sometimes be bottleneck.\nYou can disable UTF8 validation in this library (and receive a\nperformance enhancement) with the skip_utf8_validation parameter.\nIf you want to get better performance, install wsaccel. While\nwebsocket-client does not depend on wsaccel, it will be used if\navailable. wsaccel doubles the speed of UTF8 validation and\noffers a very minor 10% performance boost when masking the\npayload data as part of the send process. Numpy used to\nbe a suggested performance enhancement alternative, but\nissue #687\nfound it didn't help.\nExamples\nMany more examples are found in the\nexamples documentation.\nLong-lived Connection\nMost real-world WebSockets situations involve longer-lived connections.\nThe WebSocketApp run_forever loop will automatically try to reconnect\nto an open WebSocket connection when a network\nconnection is lost if it is provided with:\n\na dispatcher argument (async dispatcher like rel or pyevent)\na non-zero reconnect argument (delay between disconnection and attempted reconnection)\n\nrun_forever provides a variety of event-based connection controls\nusing callbacks like on_message and on_error.\nrun_forever does not automatically reconnect if the server\ncloses the WebSocket gracefully (returning\na standard websocket close code).\nThis is the logic behind the decision.\nCustomizing behavior when the server closes\nthe WebSocket should be handled in the on_close callback.\nThis example uses rel\nfor the dispatcher to provide automatic reconnection.\nimport websocket\nimport _thread\nimport time\nimport rel\n\ndef on_message(ws, message):\n    print(message)\n\ndef on_error(ws, error):\n    print(error)\n\ndef on_close(ws, close_status_code, close_msg):\n    print(\"### closed ###\")\n\ndef on_open(ws):\n    print(\"Opened connection\")\n\nif __name__ == \"__main__\":\n    websocket.enableTrace(True)\n    ws = websocket.WebSocketApp(\"wss://api.gemini.com/v1/marketdata/BTCUSD\",\n                              on_open=on_open,\n                              on_message=on_message,\n                              on_error=on_error,\n                              on_close=on_close)\n\n    ws.run_forever(dispatcher=rel, reconnect=5)  # Set dispatcher to automatic reconnection, 5 second reconnect delay if connection closed unexpectedly\n    rel.signal(2, rel.abort)  # Keyboard Interrupt\n    rel.dispatch()\nShort-lived Connection\nThis is if you want to communicate a short message and disconnect\nimmediately when done. For example, if you want to confirm that a WebSocket\nserver is running and responds properly to a specific request.\nfrom websocket import create_connection\n\nws = create_connection(\"ws://echo.websocket.events/\")\nprint(ws.recv())\nprint(\"Sending 'Hello, World'...\")\nws.send(\"Hello, World\")\nprint(\"Sent\")\nprint(\"Receiving...\")\nresult =  ws.recv()\nprint(\"Received '%s'\" % result)\nws.close()\n\n\n"}, {"name": "typing-extensions", "readme": "\nTyping Extensions\n\nDocumentation \u2013\nPyPI\nOverview\nThe typing_extensions module serves two related purposes:\n\nEnable use of new type system features on older Python versions. For example,\ntyping.TypeGuard is new in Python 3.10, but typing_extensions allows\nusers on previous Python versions to use it too.\nEnable experimentation with new type system PEPs before they are accepted and\nadded to the typing module.\n\ntyping_extensions is treated specially by static type checkers such as\nmypy and pyright. Objects defined in typing_extensions are treated the same\nway as equivalent forms in typing.\ntyping_extensions uses\nSemantic Versioning. The\nmajor version will be incremented only for backwards-incompatible changes.\nTherefore, it's safe to depend\non typing_extensions like this: typing_extensions >=x.y, <(x+1),\nwhere x.y is the first version that includes all features you need.\ntyping_extensions supports Python versions 3.7 and higher.\nIncluded items\nSee the documentation for a\ncomplete listing of module contents.\nContributing\nSee CONTRIBUTING.md\nfor how to contribute to typing_extensions.\n"}, {"name": "Theano-PyMC", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nWarehouse\nGetting Started\nDiscussion\nTesting\nCode of Conduct\n\n\n\n\n\nREADME.rst\n\n\n\n\nWarehouse\nWarehouse is the software that powers PyPI.\nSee our development roadmap, documentation, and\narchitectural overview.\n\nGetting Started\nYou can run Warehouse locally in a development environment using\ndocker. See Getting started\ndocumentation for instructions on how to set it up.\nThe canonical deployment of Warehouse is in production at pypi.org.\n\nDiscussion\nYou can find help or get involved on:\n\nGithub issue tracker for reporting issues\nIRC: on Libera, channel #pypa for general packaging discussion\nand user support, and #pypa-dev for\ndiscussions about development of packaging tools\nThe PyPA Discord for live discussions\nThe Packaging category on Discourse for discussing\nnew ideas and community initiatives\n\n\nTesting\nRead the running tests and linters section of our documentation to\nlearn how to test your code.  For cross-browser testing, we use an\nopen source account from BrowserStack. If your pull request makes\nany change to the user interface, it will need to be tested to confirm\nit works in our supported browsers.\n\n\nCode of Conduct\nEveryone interacting in the Warehouse project's codebases, issue trackers, chat\nrooms, and mailing lists is expected to follow the PSF Code of Conduct.\n\n\n"}, {"name": "text-unidecode", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nText-Unidecode\nInstallation\nUsage\n\n\n\n\n\nREADME.rst\n\n\n\n\nText-Unidecode\n\ntext-unidecode is the most basic port of the\nText::Unidecode\nPerl library.\nThere are other Python ports of Text::Unidecode (unidecode\nand isounidecode). unidecode is GPL; isounidecode uses too much memory,\nand it didn't support Python 3 when this package was created.\nYou can redistribute it and/or modify this port under the terms of either:\n\nArtistic License, or\nGPL or GPLv2+\n\nIf you're OK with GPL-only, use unidecode (it has better memory usage and\nbetter transliteration quality).\ntext-unidecode supports Python 2.7 and 3.4+.\n\nInstallation\npip install text-unidecode\n\n\nUsage\n>>> from text_unidecode import unidecode\n>>> unidecode(u'\u043a\u0430\u043a\u043e\u0439-\u0442\u043e \u0442\u0435\u043a\u0441\u0442')\n'kakoi-to tekst'\n\n\n\n"}, {"name": "stack-data", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nstack_data\nBasic usage\nVariables\nRendering lines with ranges and markers\nSyntax highlighting with Pygments\nGetting the full stack\n\n\n\n\n\nREADME.md\n\n\n\n\nstack_data\n  \nThis is a library that extracts data from stack frames and tracebacks, particularly to display more useful tracebacks than the default. It powers the tracebacks in IPython and futurecoder:\n\nYou can install it from PyPI:\npip install stack_data\n\nBasic usage\nHere's some code we'd like to inspect:\ndef foo():\n    result = []\n    for i in range(5):\n        row = []\n        result.append(row)\n        print_stack()\n        for j in range(5):\n            row.append(i * j)\n    return result\nNote that foo calls a function print_stack(). In reality we can imagine that an exception was raised at this line, or a debugger stopped there, but this is easy to play with directly. Here's a basic implementation:\nimport inspect\nimport stack_data\n\n\ndef print_stack():\n    frame = inspect.currentframe().f_back\n    frame_info = stack_data.FrameInfo(frame)\n    print(f\"{frame_info.code.co_name} at line {frame_info.lineno}\")\n    print(\"-----------\")\n    for line in frame_info.lines:\n        print(f\"{'-->' if line.is_current else '   '} {line.lineno:4} | {line.render()}\")\n(Beware that this has a major bug - it doesn't account for line gaps, which we'll learn about later)\nThe output of one call to print_stack() looks like:\nfoo at line 9\n-----------\n       6 | for i in range(5):\n       7 |     row = []\n       8 |     result.append(row)\n-->    9 |     print_stack()\n      10 |     for j in range(5):\n\nThe code for print_stack() is fairly self-explanatory. If you want to learn more details about a particular class or method I suggest looking through some docstrings. FrameInfo is a class that accepts either a frame or a traceback object and provides a bunch of nice attributes and properties (which are cached so you don't need to worry about performance). In particular frame_info.lines is a list of Line objects. line.render() returns the source code of that line suitable for display. Without any arguments it simply strips any common leading indentation. Later on we'll see a more powerful use for it.\nYou can see that frame_info.lines includes some lines of surrounding context. By default it includes 3 pieces of context before the main line and 1 piece after. We can configure the amount of context by passing options:\noptions = stack_data.Options(before=1, after=0)\nframe_info = stack_data.FrameInfo(frame, options)\nThen the output looks like:\nfoo at line 9\n-----------\n       8 | result.append(row)\n-->    9 | print_stack()\n\nNote that these parameters are not the number of lines before and after to include, but the number of pieces. A piece is a range of one or more lines in a file that should logically be grouped together. A piece contains either a single simple statement or a part of a compound statement (loops, if, try/except, etc) that doesn't contain any other statements. Most pieces are a single line, but a multi-line statement or if condition is a single piece. In the example above, all pieces are one line, because nothing is spread across multiple lines. If we change our code to include some multiline bits:\ndef foo():\n    result = []\n    for i in range(5):\n        row = []\n        result.append(\n            row\n        )\n        print_stack()\n        for j in range(\n                5\n        ):\n            row.append(i * j)\n    return result\nand then run the original code with the default options, then the output is:\nfoo at line 11\n-----------\n       6 | for i in range(5):\n       7 |     row = []\n       8 |     result.append(\n       9 |         row\n      10 |     )\n-->   11 |     print_stack()\n      12 |     for j in range(\n      13 |             5\n      14 |     ):\n\nNow lines 8-10 and lines 12-14 are each a single piece. Note that the output is essentially the same as the original in terms of the amount of code. The division of files into pieces means that the edge of the context is intuitive and doesn't crop out parts of statements or expressions. For example, if context was measured in lines instead of pieces, the last line of the above would be for j in range( which is much less useful.\nHowever, if a piece is very long, including all of it could be cumbersome. For this, Options has a parameter max_lines_per_piece, which is 6 by default. Suppose we have a piece in our code that's longer than that:\n        row = [\n            1,\n            2,\n            3,\n            4,\n            5,\n        ]\nframe_info.lines will truncate this piece so that instead of 7 Line objects it will produce 5 Line objects and one LINE_GAP in the middle, making 6 objects in total for the piece. Our code doesn't currently handle gaps, so it will raise an exception. We can modify it like so:\n    for line in frame_info.lines:\n        if line is stack_data.LINE_GAP:\n            print(\"       (...)\")\n        else:\n            print(f\"{'-->' if line.is_current else '   '} {line.lineno:4} | {line.render()}\")\nNow the output looks like:\nfoo at line 15\n-----------\n       6 | for i in range(5):\n       7 |     row = [\n       8 |         1,\n       9 |         2,\n       (...)\n      12 |         5,\n      13 |     ]\n      14 |     result.append(row)\n-->   15 |     print_stack()\n      16 |     for j in range(5):\n\nAlternatively, you can flip the condition around and check if isinstance(line, stack_data.Line):. Either way, you should always check for line gaps, or your code may appear to work at first but fail when it encounters a long piece.\nNote that the executing piece, i.e. the piece containing the current line being executed (line 15 in this case) is never truncated, no matter how long it is.\nThe lines of context never stray outside frame_info.scope, which is the innermost function or class definition containing the current line. For example, this is the output for a short function which has neither 3 lines before nor 1 line after the current line:\nbar at line 6\n-----------\n       4 | def bar():\n       5 |     foo()\n-->    6 |     print_stack()\n\nSometimes it's nice to ensure that the function signature is always showing. This can be done with Options(include_signature=True). The result looks like this:\nfoo at line 14\n-----------\n       9 | def foo():\n       (...)\n      11 |     for i in range(5):\n      12 |         row = []\n      13 |         result.append(row)\n-->   14 |         print_stack()\n      15 |         for j in range(5):\n\nTo avoid wasting space, pieces never start or end with a blank line, and blank lines between pieces are excluded. So if our code looks like this:\n    for i in range(5):\n        row = []\n\n        result.append(row)\n        print_stack()\n\n        for j in range(5):\nThe output doesn't change much, except you can see jumps in the line numbers:\n      11 |     for i in range(5):\n      12 |         row = []\n      14 |         result.append(row)\n-->   15 |         print_stack()\n      17 |         for j in range(5):\n\nVariables\nYou can also inspect variables and other expressions in a frame, e.g:\n    for var in frame_info.variables:\n        print(f\"{var.name} = {repr(var.value)}\")\nwhich may output:\nresult = [[0, 0, 0, 0, 0], [0, 1, 2, 3, 4], [0, 2, 4, 6, 8], [0, 3, 6, 9, 12], []]\ni = 4\nrow = []\nj = 4\nframe_info.variables returns a list of Variable objects, which have attributes name, value, and nodes, which is a list of all AST representing that expression.\nA Variable may refer to an expression other than a simple variable name. It can be any expression evaluated by the library pure_eval which it deems 'interesting' (see those docs for more info). This includes expressions like foo.bar or foo[bar]. In these cases name is the source code of that expression. pure_eval ensures that it only evaluates expressions that won't have any side effects, e.g. where foo.bar is a normal attribute rather than a descriptor such as a property.\nframe_info.variables is a list of all the interesting expressions found in frame_info.scope, e.g. the current function, which may include expressions not visible in frame_info.lines. You can restrict the list by using frame_info.variables_in_lines or even frame_info.variables_in_executing_piece. For more control you can use frame_info.variables_by_lineno. See the docstrings for more information.\nRendering lines with ranges and markers\nSometimes you may want to insert special characters into the text for display purposes, e.g. HTML or ANSI color codes. stack_data provides a few tools to make this easier.\nLet's say we have a Line object where line.text (the original raw source code of that line) is \"foo = bar\", so line.text[6:9] is \"bar\", and we want to emphasise that part by inserting HTML at positions 6 and 9 in the text. Here's how we can do that directly:\nmarkers = [\n    stack_data.MarkerInLine(position=6, is_start=True, string=\"<b>\"),\n    stack_data.MarkerInLine(position=9, is_start=False, string=\"</b>\"),\n]\nline.render(markers)  # returns \"foo = <b>bar</b>\"\nHere is_start=True indicates that the marker is the first of a pair. This helps line.render() sort and insert the markers correctly so you don't end up with malformed HTML like foo<b>.<i></b>bar</i> where tags overlap.\nSince we're inserting HTML, we should actually use line.render(markers, escape_html=True) which will escape special HTML characters in the Python source (but not the markers) so for example foo = bar < spam would be rendered as foo = <b>bar</b> &lt; spam.\nUsually though you wouldn't create markers directly yourself. Instead you would start with one or more ranges and then convert them, like so:\nranges = [\n    stack_data.RangeInLine(start=0, end=3, data=\"foo\"),\n    stack_data.RangeInLine(start=6, end=9, data=\"bar\"),\n]\n\ndef convert_ranges(r):\n    if r.data == \"bar\":\n        return \"<b>\", \"</b>\"        \n\n# This results in `markers` being the same as in the above example.\nmarkers = stack_data.markers_from_ranges(ranges, convert_ranges)\nRangeInLine has a data attribute which can be any object. markers_from_ranges accepts a converter function to which it passes all the RangeInLine objects. If the converter function returns a pair of strings, it creates two markers from them. Otherwise it should return None to indicate that the range should be ignored, as with the first range containing \"foo\" in this example.\nThe reason this is useful is because there are built in tools to create these ranges for you. For example, if we change our print_stack() function to contain this:\ndef convert_variable_ranges(r):\n    variable, _node = r.data\n    return f'<span data-value=\"{repr(variable.value)}\">', '</span>'\n\nmarkers = stack_data.markers_from_ranges(line.variable_ranges, convert_variable_ranges)\nprint(f\"{'-->' if line.is_current else '   '} {line.lineno:4} | {line.render(markers, escape_html=True)}\")\nThen the output becomes:\nfoo at line 15\n-----------\n       9 | def foo():\n       (...)\n      11 |     for <span data-value=\"4\">i</span> in range(5):\n      12 |         <span data-value=\"[]\">row</span> = []\n      14 |         <span data-value=\"[[0, 0, 0, 0, 0], [0, 1, 2, 3, 4], [0, 2, 4, 6, 8], [0, 3, 6, 9, 12], []]\">result</span>.append(<span data-value=\"[]\">row</span>)\n-->   15 |         print_stack()\n      17 |         for <span data-value=\"4\">j</span> in range(5):\n\nline.variable_ranges is a list of RangeInLines for each Variable that appears at least partially in this line. The data attribute of the range is a pair (variable, node) where node is the particular AST node from the list variable.nodes that corresponds to this range.\nYou can also use line.token_ranges (e.g. if you want to do your own syntax highlighting) or line.executing_node_ranges if you want to highlight the currently executing node identified by the executing library. Or if you want to make your own range from an AST node, use line.range_from_node(node, data). See the docstrings for more info.\nSyntax highlighting with Pygments\nIf you'd like pretty colored text without the work, you can let Pygments do it for you. Just follow these steps:\n\npip install pygments separately as it's not a dependency of stack_data.\nCreate a pygments formatter object such as HtmlFormatter or Terminal256Formatter.\nPass the formatter to Options in the argument pygments_formatter.\nUse line.render(pygmented=True) to get your formatted text. In this case you can't pass any markers to render.\n\nIf you want, you can also highlight the executing node in the frame in combination with the pygments syntax highlighting. For this you will need:\n\nA pygments style - either a style class or a string that names it. See the documentation on styles and the styles gallery.\nA modification to make to the style for the executing node, which is a string such as \"bold\" or \"bg:#ffff00\" (yellow background). See the documentation on style rules.\nPass these two things to stack_data.style_with_executing_node(style, modifier) to get a new style class.\nPass the new style to your formatter when you create it.\n\nNote that this doesn't work with TerminalFormatter which just uses the basic ANSI colors and doesn't use the style passed to it in general.\nGetting the full stack\nCurrently print_stack() doesn't actually print the stack, it just prints one frame. Instead of frame_info = FrameInfo(frame, options), let's do this:\nfor frame_info in FrameInfo.stack_data(frame, options):\nNow the output looks something like this:\n<module> at line 18\n-----------\n      14 |         for j in range(5):\n      15 |             row.append(i * j)\n      16 |     return result\n-->   18 | bar()\n\nbar at line 5\n-----------\n       4 | def bar():\n-->    5 |     foo()\n\nfoo at line 13\n-----------\n      10 | for i in range(5):\n      11 |     row = []\n      12 |     result.append(row)\n-->   13 |     print_stack()\n      14 |     for j in range(5):\n\nHowever, just as frame_info.lines doesn't always yield Line objects, FrameInfo.stack_data doesn't always yield FrameInfo objects, and we must modify our code to handle that. Let's look at some different sample code:\ndef factorial(x):\n    return x * factorial(x - 1)\n\n\ntry:\n    print(factorial(5))\nexcept:\n    print_stack()\nIn this code we've forgotten to include a base case in our factorial function so it will fail with a RecursionError and there'll be many frames with similar information. Similar to the built in Python traceback, stack_data avoids showing all of these frames. Instead you will get a RepeatedFrames object which summarises the information. See its docstring for more details.\nHere is our updated implementation:\ndef print_stack():\n    for frame_info in FrameInfo.stack_data(sys.exc_info()[2]):\n        if isinstance(frame_info, FrameInfo):\n            print(f\"{frame_info.code.co_name} at line {frame_info.lineno}\")\n            print(\"-----------\")\n            for line in frame_info.lines:\n                print(f\"{'-->' if line.is_current else '   '} {line.lineno:4} | {line.render()}\")\n\n            for var in frame_info.variables:\n                print(f\"{var.name} = {repr(var.value)}\")\n\n            print()\n        else:\n            print(f\"... {frame_info.description} ...\\n\")\nAnd the output:\n<module> at line 9\n-----------\n       4 | def factorial(x):\n       5 |     return x * factorial(x - 1)\n       8 | try:\n-->    9 |     print(factorial(5))\n      10 | except:\n\nfactorial at line 5\n-----------\n       4 | def factorial(x):\n-->    5 |     return x * factorial(x - 1)\nx = 5\n\nfactorial at line 5\n-----------\n       4 | def factorial(x):\n-->    5 |     return x * factorial(x - 1)\nx = 4\n\n... factorial at line 5 (996 times) ...\n\nfactorial at line 5\n-----------\n       4 | def factorial(x):\n-->    5 |     return x * factorial(x - 1)\nx = -993\n\nIn addition to handling repeated frames, we've passed a traceback object to FrameInfo.stack_data instead of a frame.\nIf you want, you can pass collapse_repeated_frames=False to FrameInfo.stack_data (not to Options) and it will just yield FrameInfo objects for the full stack.\n\n\n"}, {"name": "spacy-legacy", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nWarehouse\nGetting Started\nDiscussion\nTesting\nCode of Conduct\n\n\n\n\n\nREADME.rst\n\n\n\n\nWarehouse\nWarehouse is the software that powers PyPI.\nSee our development roadmap, documentation, and\narchitectural overview.\n\nGetting Started\nYou can run Warehouse locally in a development environment using\ndocker. See Getting started\ndocumentation for instructions on how to set it up.\nThe canonical deployment of Warehouse is in production at pypi.org.\n\nDiscussion\nYou can find help or get involved on:\n\nGithub issue tracker for reporting issues\nIRC: on Libera, channel #pypa for general packaging discussion\nand user support, and #pypa-dev for\ndiscussions about development of packaging tools\nThe PyPA Discord for live discussions\nThe Packaging category on Discourse for discussing\nnew ideas and community initiatives\n\n\nTesting\nRead the running tests and linters section of our documentation to\nlearn how to test your code.  For cross-browser testing, we use an\nopen source account from BrowserStack. If your pull request makes\nany change to the user interface, it will need to be tested to confirm\nit works in our supported browsers.\n\n\nCode of Conduct\nEveryone interacting in the Warehouse project's codebases, issue trackers, chat\nrooms, and mailing lists is expected to follow the PSF Code of Conduct.\n\n\n"}, {"name": "snowflake-connector-python", "readme": "\nThis package includes the Snowflake Connector for Python, which conforms to the Python DB API 2.0 specification:\nhttps://www.python.org/dev/peps/pep-0249/\nSnowflake Documentation is available at:\nhttps://docs.snowflake.com/\nSource code is also available at: https://github.com/snowflakedb/snowflake-connector-python\nRelease Notes\n\n\nv3.1.1(August 28,2023)\n\nFixed a bug in retry logic for okta authentication to refresh token.\nSupport RSAPublicKey when constructing AuthByKeyPair in addition to raw bytes.\nFixed a bug when connecting through SOCKS5 proxy, the attribute proxy_header is missing on SOCKSProxyManager.\nCherry-picked https://github.com/urllib3/urllib3/commit/fd2759aa16b12b33298900c77d29b3813c6582de onto vendored urllib3 (v1.26.15) to enable enforce_content_length by default.\nFixed a bug in tag generation of OOB telemetry event.\n\n\n\nv3.1.0(July 31,2023)\n\n\nAdded a feature that lets you add connection definitions to the connections.toml configuration file. A connection definition refers to a collection of connection parameters, for example, if you wanted to define a connection named `prod``:\n[prod]\naccount = \"my_account\"\nuser = \"my_user\"\npassword = \"my_password\"\n\nBy default, we look for the connections.toml file in the location specified in the SNOWFLAKE_HOME environment variable (default: ~/.snowflake). If this folder does not exist, the Python connector looks for the file in the platformdirs location, as follows:\n\nOn Linux: ~/.config/snowflake/,  but follows XDG settings\nOn Mac: ~/Library/Application Support/snowflake/\nOn Windows: %USERPROFILE%\\AppData\\Local\\snowflake\\\n\nYou can determine which file is used by running the following command:\npython -c \"from snowflake.connector.constants import CONNECTIONS_FILE; print(str(CONNECTIONS_FILE))\"\n\n\n\nBumped cryptography dependency from <41.0.0,>=3.1.0 to >=3.1.0,<42.0.0.\n\n\nImproved OCSP response caching to remove tmp cache files on Windows.\n\n\nImproved OCSP response caching to reduce the times of disk writing.\n\n\nAdded a parameter server_session_keep_alive in SnowflakeConnection that skips session deletion when client connection closes.\n\n\nTightened our pinning of platformdirs, to prevent their new releases breaking us.\n\n\nFixed a bug where SFPlatformDirs would incorrectly append application_name/version to its path.\n\n\nAdded retry reason for queries that are retried by the client.\n\n\nFixed a bug where write_pandas fails when user does not have the privilege to create stage or file format in the target schema, but has the right privilege for the current schema.\n\n\nRemove Python 3.7 support.\n\n\nWorked around a segfault which sometimes occurred during cache serialization in multi-threaded scenarios.\n\n\nImproved error handling of connection reset error.\n\n\nFixed a bug about deleting the temporary files happened when running PUT command.\n\n\nAllowed to pass type_mapper to fetch_pandas_batches() and fetch_pandas_all().\n\n\nFixed a bug where pickle.dump segfaults during cache serialization in multi-threaded scenarios.\n\n\nImproved retry logic for okta authentication to refresh token if authentication gets throttled.\n\n\nNote that this release does not include the changes introduced in the previous 3.1.0a1 release. Those will be released at a later time.\n\n\n\n\nv3.0.4(May 23,2023)\n\nFixed a bug in which cursor.execute() could modify the argument statement_params dictionary object when executing a multistatement query.\nAdded the json_result_force_utf8_decoding connection parameter to force decoding JSON content in utf-8 when the result format is JSON.\nFixed a bug in which we cannot call SnowflakeCursor.nextset before fetching the result of the first query if the cursor runs an async multistatement query.\nBumped vendored library urllib3 to 1.26.15\nBumped vendored library requests to 2.29.0\nFixed a bug when _prefetch_hook() was not called before yielding results of execute_async().\nFixed a bug where some ResultMetadata fields were marked as required when they were optional.\nBumped pandas dependency from <1.6.0,>=1.0.0 to >=1.0.0,<2.1.0\nFixed a bug where bulk insert converts date incorrectly.\nAdd support for Geometry types.\n\n\n\nv3.0.3(April 20, 2023)\n\nFixed a bug that prints error in logs for GET command on GCS.\nAdded a parameter that allows users to skip file uploads to stage if file exists on stage and contents of the file match.\nFixed a bug that occurred when writing a Pandas DataFrame with non-default index in snowflake.connector.pandas_tool.write_pandas.\nFixed a bug that occurred when writing a Pandas DataFrame with column names containing double quotes in snowflake.connector.pandas_tool.write_pandas.\nFixed a bug that occurred when writing a Pandas DataFrame with binary data in snowflake.connector.pandas_tool.write_pandas.\nImproved type hint of SnowflakeCursor.execute method.\nFail instantly upon receiving 403: Forbidden HTTP response for a login-request.\nImproved GET logging to warn when downloading multiple files with the same name.\n\n\n\nv3.0.2(March 23, 2023)\n\nFixed a memory leak in the logging module of the Cython extension.\nFixed a bug where the put command on AWS raised AttributeError when uploading file composed of multiple parts.\nFixed a bug of incorrect type hints of SnowflakeCursor.fetch_arrow_all and SnowflakeCursor.fetchall.\nFixed a bug where snowflake.connector.util_text.split_statements swallows the final line break in the case when there are no space between lines.\nImproved logging to mask tokens in case of errors.\nValidate SSO URL before opening it in the browser for External browser authenticator.\n\n\n\nv3.0.1(February 28, 2023)\n\nImproved the robustness of OCSP response caching to handle errors in cases of serialization and deserialization.\nUpdated async_executes method's doc-string.\nErrors raised now have a query field that contains the SQL query that caused them when available.\nFixed a bug where MFA token caching would refuse to work until restarted instead of reauthenticating.\nReplaced the dependency on setuptools in favor of packaging.\nFixed a bug where AuthByKeyPair.handle_timeout should pass keyword arguments instead of positional arguments when calling AuthByKeyPair.prepare.\n\n\n\nv3.0.0(January 26, 2023)\n\nFixed a bug where write_pandas did not use user-specified schema and database to create intermediate objects\nFixed a bug where HTTP response code of 429 were not retried\nFixed a bug where MFA token caching was not working\nBumped pyarrow dependency from >=8.0.0,<8.1.0 to >=10.0.1,<10.1.0\nBumped pyOpenSSL dependency from <23.0.0 to <24.0.0\nDuring browser-based authentication, the SSO url is now printed before opening it in the browser\nIncreased the level of a log for when ArrowResult cannot be imported\nAdded a minimum MacOS version check when compiling C-extensions\nEnabled fetch_arrow_all and fetch_arrow_batches to handle async query results\n\n\n\nv2.9.0(December 9, 2022)\n\nFixed a bug where the permission of the file downloaded via GET command is changed\nReworked authentication internals to allow users to plug custom key-pair authenticators\nMulti-statement query execution is now supported through cursor.execute and cursor.executemany\n\nThe Snowflake parameter MULTI_STATEMENT_COUNT can be altered at the account, session, or statement level. An additional argument, num_statements, can be provided to execute to use this parameter at the statement level. It must be provided to executemany to submit a multi-statement query through the method. Note that bulk insert optimizations available through executemany are not available when submitting multi-statement queries.\n\nBy default the parameter is 1, meaning only a single query can be submitted at a time\nSet to 0 to submit any number of statements in a multi-statement query\nSet to >1 to submit the specified exact number of statements in a multi-statement query\n\n\nBindings are accepted in the same way for multi-statements as they are for single statement queries\nAsynchronous multi-statement query execution is supported. Users should still use get_results_from_sfqid to retrieve results\nTo access the results of each query, users can call SnowflakeCursor.nextset() as specified in the DB 2.0 API (PEP-249), to iterate through each statements results\n\nThe first statement's results are accessible immediately after calling execute (or get_results_from_sfqid if asynchronous) through the existing fetch*() methods\n\n\n\n\n\n\n\nv2.8.3(November 28,2022)\n\nBumped cryptography dependency from <39.0.0 to <41.0.0\nFixed a bug where expired OCSP response cache caused infinite recursion during cache loading\n\n\n\nv2.8.2(November 18,2022)\n\nImproved performance of OCSP response caching\nDuring the execution of GET commands we no longer resolve target location on the local machine\nImproved performance of regexes used for PUT/GET SQL statement detection. CVE-2022-42965\n\n\n\nv2.8.1(October 30,2022)\n\nBumped cryptography dependency from <37.0.0 to <39.0.0\nBumped pandas dependency from <1.5.0 to <1.6.0\nFixed a bug where write_pandas wouldn't write an empty DataFrame to Snowflake\nWhen closing connection async query status checking is now parallelized\nFixed a bug where test logging would be enabled on Jenkins workers in non-Snowflake Jenkins machines\nEnhanced the atomicity of write_pandas when overwrite is set to True\n\n\n\nv2.8.0(September 27,2022)\n\nFixed a bug where rowcount was deleted when the cursor was closed\nFixed a bug where extTypeName was used even when it was empty\nUpdated how telemetry entries are constructed\nAdded telemetry for imported root packages during run-time\nAdded telemetry for using write_pandas\nFixed missing dtypes when calling fetch_pandas_all() on empty result\nThe write_pandas function now supports providing additional arguments to be used by DataFrame.to_parquet\nAll optional parameters of write_pandas can now be provided to pd_writer and make_pd_writer to be used with DataFrame.to_sql\n\n\n\nv2.7.12(August 26,2022)\n\nFixed a bug where timestamps fetched as pandas.DataFrame or pyarrow.Table would overflow for the sake of unnecessary precision. In the case where an overflow cannot be prevented a clear error will be raised now.\nAdded in-file caching for OCSP response caching\nThe write_pandas function now supports transient tables through the new table_type argument which supersedes create_temp_table argument\nFixed a bug where calling fetch_pandas_batches incorrectly raised NotSupportedError after an async query was executed\nAdded support for OKTA Identity Engine\n\n\n\nv2.7.11(July 26,2022)\n\nAdded minimum version pin to typing_extensions\n\n\n\nv2.7.10(July 22,2022)\n\nRelease wheels are now built on manylinux2014\nBumped supported pyarrow version to >=8.0.0,<8.1.0\nUpdated vendored library versions requests to 2.28.1 and urllib3 to 1.26.10\nAdded in-memory cache to OCSP requests\nAdded overwrite option to write_pandas\nAdded attribute lastrowid to SnowflakeCursor in compliance with PEP249.\nFixed a bug where gzip compressed http requests might be garbled by an unflushed buffer\nAdded new connection diagnostics capabilities to snowflake-connector-python\nBumped numpy dependency from <1.23.0 to <1.24.0\n\n\n\nv2.7.9(June 26,2022)\n\nFixed a bug where errors raised during get_results_from_sfqid() were missing errno\nFixed a bug where empty results containing GEOGRAPHY type raised IndexError\n\n\n\nv2.7.8(May 28,2022)\n\nUpdated PyPi documentation link to python specific main page\nFixed an error message that appears when pandas optional dependency group is required but is not installed\nImplemented the DB API 2 callproc() method\nFixed a bug where decryption took place before decompression when downloading files from stages\nFixed a bug where s3 accelerate configuration was handled incorrectly\nExtra named arguments given executemany() are now forwarded to execute()\nAutomatically sets the application name to streamlit when streamlit is imported and application name was not explicitly set\nBumped pyopenssl dependency version to >=16.2.0,<23.0.0\n\n\n\nv2.7.7(April 30,2022)\n\nBumped supported pandas version to < 1.5.0\nFixed a bug where partner name (from SF_PARTNER environmental variable) was set after connection was established\nAdded a new _no_retry option to executing queries\nFixed a bug where extreme timestamps lost precision\n\n\n\nv2.7.6(March 17,2022)\n\nFixed missing python_requires tag in setup.cfg\n\n\n\nv2.7.5(March 17,2022)\n\nAdded an option for partners to inject their name through an environmental variable (SF_PARTNER)\nFixed a bug where we would not wait for input if a browser window couldn't be opened for SSO login\nDeprecate support for Python 3.6\nExported a type definition for SnowflakeConnection\nFixed a bug where final Arrow table would contain duplicate index numbers when using fetch_pandas_all\n\n\n\nv2.7.4(February 05,2022)\n\nAdd Geography Types\nRemoving automated incident reporting code\nFixed a bug where circular reference would prevent garbage collection on some objects\nFixed a bug where DatabaseError was thrown when executing against a closed cursor instead of InterfaceError\nFixed a bug where calling executemany would crash if an iterator was supplied as args\nFixed a bug where violating NOT NULL constraint raised DatabaseError instead of IntegrityError\n\n\n\nv2.7.3(January 22,2022)\n\nFixed a bug where timezone was missing from retrieved Timestamp_TZ columns\nFixed a bug where a long running PUT/GET command could hit a Storage Credential Error while renewing credentials\nFixed a bug where py.typed was not being included in our release wheels\nFixed a bug where negative numbers were mangled when fetched with the connection parameter arrow_number_to_decimal\nImproved the error message that is encountered when running GET for a non-existing file\nFixed rendering of our long description for PyPi\nFixed a bug where DUO authentication ran into errors if sms authentication was disabled for the user\nAdd the ability to auto-create a table when writing a pandas DataFrame to a Snowflake table\nBumped the maximum dependency version of numpy from <1.22.0 to <1.23.0\n\n\n\nv2.7.2(December 17,2021)\n\nAdded support for Python version 3.10.\nFixed an issue bug where _get_query_status failed if there was a network error.\nAdded the interpolate_empty_sequences connection parameter to control interpolating empty sequences into queries.\nFixed an issue where where BLOCKED was considered to be an error by is_an_error.\nAdded source field to Telemetry.\nIncreased the cryptography dependency version.\nIncreased the pyopenssl dependency version.\nFixed an issue where dbapi.Binary returned a string instead of bytes.\nIncreased the required version of numpy.\nIncreased the required version of keyring.\nFixed issue so that fetch functions now return a typed DataFrames and pyarrow Tables for empty results.\nAdded py.typed\nImproved error messages for PUT/GET.\nAdded Cursor.query attribute for accessing last query.\nIncreased the required version of pyarrow.\n\n\n\nv2.7.1(November 19,2021)\n\nFixed a bug where uploading a streaming file with multiple parts did not work.\nJWT tokens are now regenerated when a request is retired.\nUpdated URL escaping when uploading to AWS S3 to match how S3 escapes URLs.\nRemoved the unused s3_connection_pool_size connection parameter.\nBlocked queries are now be considered to be still running.\nSnowflake specific exceptions are now set using Exception arguments.\nFixed an issue where use_s3_regional_url was not set correctly by the connector.\n\n\n\nv2.7.0(October 25,2021)\n\nRemoving cloud sdks.snowflake-connector-python will not install them anymore. Recreate your virtualenv to get rid of unnecessary dependencies.\nInclude Standard C++ headers.\nUpdate minimum dependency version pin of cryptography.\nFixed a bug where error number would not be added to Exception messages.\nFixed a bug where client_prefetch_threads parameter was not respected when pre-fetching results.\nUpdate signature of SnowflakeCursor.execute's params argument.\n\n\n\nv2.6.2(September 27,2021)\n\nUpdated vendored urllib3 and requests versions.\nFixed a bug where GET commands would fail to download files from sub directories from stages.\nAdded a feature where where the connector will print the url it tried to open when it is unable to open it for external browser authentication.\n\n\n\nv2.6.1(September 16,2021)\n\nBump pandas version from <1.3 to <1.4\nFixing Python deprecation warnings.\nAdded more type-hints.\nMarked HeartBeatTimer threads as daemon threads.\nForce cast a column into integer in write_pandas to avoid a rare behavior that would lead to crashing.\nImplement AWS signature V4 to new SDKless PUT and GET.\nRemoved a deprecated setuptools option from setup.py.\nFixed a bug where error logs would be printed for query executions that produce no results.\nFixed a bug where the temporary stage for bulk array inserts exists.\n\n\n\nv2.6.0(August 29,2021)\n\nInternal change to the implementation of result fetching.\nUpgraded Pyarrow version from 3.0 to 5.0.\nInternal change to the implementation for PUT and GET. A new connection parameter use_new_put_get was added to toggle between implementations.\nFixed a bug where executemany did not detect the type of data it was inserting.\nUpdated the minimum Mac OSX build target from 10.13 to 10.14.\n\n\n\nv2.5.1(July 31,2021)\n\nFixes Python Connector bug that prevents the connector from using AWS S3 Regional URL. The driver currently overrides the regional URL information with the default S3 URL causing failure in PUT.\n\n\n\nv2.5.0(July 22,2021)\n\nFixed a bug in write_pandas when quote_identifiers is set to True the function would not actually quote column names.\nBumping idna dependency pin from <3,>=2.5 to >=2.5,<4\nFix describe method when running insert into ... commands\n\n\n\nv2.4.6(June 25,2021)\n\nFixed a potential memory leak.\nRemoved upper certifi version pin.\nUpdated vendored libraries , urllib(1.26.5) and requests(2.25.1).\nReplace pointers with UniqueRefs.\nChanged default value of client_session_keep_alive to None.\nAdded the ability to retrieve metadata/schema without executing the query (describe method).\n\n\n\nv2.4.5(June 15,2021)\n\nFix for incorrect JWT token invalidity when an account alias with a dash in it is used for regionless account URL.\n\n\n\nv2.4.4(May 30,2021)\n\nFixed a segfault issue when using DictCursor and arrow result format with out of range dates.\nAdds new make_pd_writer helper function\n\n\n\nv2.4.3(April 29,2021)\n\nUses s3 regional URL in private links when a param is set.\nNew Arrow NUMBER to Decimal converter option.\nUpdate pyopenssl requirement from <20.0.0,>=16.2.0 to >=16.2.0,<21.0.0.\nUpdate pandas requirement from <1.2.0,>=1.0.0 to >=1.0.0,<1.3.0.\nUpdate numpy requirement from <1.20.0 to <1.21.0.\n\n\n\nv2.4.2(April 03,2021)\n\nPUT statements are now thread-safe.\n\n\n\nv2.4.1(March 04,2021)\n\nMake connection object exit() aware of status of parameter autocommit\n\n\n\nv2.4.0(March 04,2021)\n\nAdded support for Python 3.9 and PyArrow 3.0.x.\nAdded support for the upcoming multipart PUT threshold keyword.\nAdded support for using the PUT command with a file-like object.\nAdded some compilation flags to ease building conda community package.\nRemoved the pytz pin because it doesn't follow semantic versioning release format.\nAdded support for optimizing batch inserts through bulk array binding.\n\n\n\nv2.3.10(February 01,2021)\n\nImproved query ID logging and added request GUID logging.\nFor dependency checking, increased the version condition for the pyjwt package from <2.0.0 to <3.0.0.\n\n\n\nv2.3.9(January 27,2021)\n\nThe fix to add proper proxy CONNECT headers for connections made over proxies.\n\n\n\nv2.3.8(January 14,2021)\n\nArrow result conversion speed up.\nSend all Python Connector exceptions to in-band or out-of-band telemetry.\nVendoring requests and urllib3 to contain OCSP monkey patching to our library only.\nDeclare dependency on setuptools.\n\n\n\nv2.3.7(December 10,2020)\n\nAdded support for upcoming downscoped GCS credentials.\nTightened the pyOpenSSL dependency pin.\nRelaxed the boto3 dependency pin up to the next major release.\nRelaxed the cffi dependency pin up to the next major release.\nAdded support for executing asynchronous queries.\nDropped support for Python 3.5.\n\n\n\nv2.3.6(November 16,2020)\n\nFixed a bug that was preventing the connector from working on Windows with Python 3.8.\nImproved the string formatting in exception messages.\nFor dependency checking, increased the version condition for the cryptography package from <3.0.0 to <4.0.0.\nFor dependency checking, increased the version condition for the pandas package from <1.1 to <1.2.\n\n\n\nv2.3.5(November 03,2020)\n\nUpdated the dependency on the cryptography package from version 2.9.2 to 3.2.1.\n\n\n\nv2.3.4(October 26,2020)\n\nAdded an optional parameter to the write_pandas function to specify that identifiers should not be quoted before being sent to the server.\nThe write_pandas function now honors default and auto-increment values for columns when inserting new rows.\nUpdated the Python Connector OCSP error messages and accompanying telemetry Information.\nEnabled the runtime pyarrow version verification to fail gracefully. Fixed a bug with AWS glue environment.\nUpgraded the version of boto3 from 1.14.47 to 1.15.9.\nUpgraded the version of idna from 2.9 to 2.10.\n\n\n\nv2.3.3(October 05,2020)\n\nSimplified the configuration files by consolidating test settings.\nIn the Connection object, the execute_stream and execute_string methods now filter out empty lines from their inputs.\n\n\n\nv2.3.2(September 14,2020)\n\nFixed a bug where a file handler was not closed properly.\nFixed various documentation typos.\n\n\n\nv2.3.1(August 25,2020)\n\nFixed a bug where 2 constants were removed by mistake.\n\n\n\nv2.3.0(August 24,2020)\n\nWhen the log level is set to DEBUG, log the OOB telemetry entries that are sent to Snowflake.\nFixed a bug in the PUT command where long running PUTs would fail to re-authenticate to GCP for storage.\nUpdated the minimum build target MacOS version to 10.13.\n\n\n\nv2.2.10(August 03,2020)\n\nImproved an error message for when \"pandas\" optional dependency group is not installed and user tries to fetch data into a pandas DataFrame. It'll now point user to our online documentation.\n\n\n\nv2.2.9(July 13,2020)\n\nConnection parameter validate_default_parameters now verifies known connection parameter names and types. It emits warnings for anything unexpected types or names.\nCorrect logging messages for compiled C++ code.\nFixed an issue in write_pandas with location determination when database, or schema name was included.\nBumped boto3 dependency version.\nFixed an issue where uploading a file with special UTF-8 characters in their names corrupted file.\n\n\n\nv2.2.8(June 22,2020)\n\nSwitched docstring style to Google from Epydoc and added automated tests to enforce the standard.\nFixed a memory leak in DictCursor's Arrow format code.\n\n\n\nv2.2.7(June 1,2020)\n\nSupport azure-storage-blob v12 as well as v2 (for Python 3.5.0-3.5.1) by Python Connector\nFixed a bug where temporary directory path was not Windows compatible in write_pandas function\nAdded out of band telemetry error reporting of unknown errors\n\n\n\nv2.2.6(May 11,2020)\n\nUpdate Pyarrow version from 0.16.0 to 0.17.0 for Python connector\nRemove more restrictive application name enforcement.\nMissing keyring dependency will not raise an exception, only emit a debug log from now on.\nBumping boto3 to <1.14\nFix flake8 3.8.0 new issues\nImplement Python log interceptor\n\n\n\nv2.2.5(April 30,2020)\n\nAdded more efficient way to ingest a pandas.Dataframe into Snowflake, located in snowflake.connector.pandas_tools\nMore restrictive application name enforcement and standardizing it with other Snowflake drivers\nAdded checking and warning for users when they have a wrong version of pyarrow installed\n\n\n\nv2.2.4(April 10,2020)\n\nEmit warning only if trying to set different setting of use_openssl_only parameter\n\n\n\nv2.2.3(March 30,2020)\n\nSecure SSO ID Token\nAdd use_openssl_only connection parameter, which disables the usage of pure Python cryptographic libraries for FIPS\nAdd manylinux1 as well as manylinux2010\nFix a bug where a certificate file was opened and never closed in snowflake-connector-python.\nFix python connector skips validating GCP URLs\nAdds additional client driver config information to in band telemetry.\n\n\n\nv2.2.2(March 9,2020)\n\nFix retry with chunck_downloader.py for stability.\nSupport Python 3.8 for Linux and Mac.\n\n\n\nv2.2.1(February 18,2020)\n\nFix use DictCursor with execute_string #248\n\n\n\nv2.2.0(January 27,2020)\n\nDrop Python 2.7 support\nAWS: When OVERWRITE is false, which is set by default, the file is uploaded if no same file name exists in the stage. This used to check the content signature but it will no longer check. Azure and GCP already work this way.\nDocument Python connector dependencies on our GitHub page in addition to Snowflake docs.\nFix sqlalchemy and possibly python-connector warnings.\nFix GCP exception using the Python connector to PUT a file in a stage with auto_compress=false.\nBump up botocore requirements to 1.14.\nFix uppercaseing authenticator breaks Okta URL which may include case-sensitive elements(#257).\nFix wrong result bug while using fetch_pandas_all() to get fixed numbers with large scales.\nIncrease multi part upload threshold for S3 to 64MB.\n\n\n\nv2.1.3(January 06,2020)\n\nFix GCP Put failed after hours\n\n\n\nv2.1.2(December 16,2019)\n\nFix the arrow bundling issue for python connector on mac.\nFix the arrow dll bundle issue on windows.Add more logging.\n\n\n\nv2.1.1(December 12,2019)\n\nFix GZIP uncompressed content for Azure GET command.\nAdd support for GCS PUT and GET for private preview.\nSupport fetch as numpy value in arrow result format.\nFix NameError: name 'EmptyPyArrowIterator' is not defined for Mac.\nReturn empty dataframe for fetch_pandas_all() api if result set is empty.\n\n\n\nv2.1.0(December 2,2019)\n\nFix default ssl_context options\nPin more dependencies for Python Connector\nFix import of SnowflakeOCSPAsn1Crypto crashes Python on MacOS Catalina\nUpdate the release note that 1.9.0 was removed\nSupport DictCursor for arrow result format\nUpgrade Python's arrow lib to 0.15.1\nRaise Exception when PUT fails to Upload Data\nHandle year out of range correctly in arrow result format\n\n\n\nv2.0.4(November 13,2019)\n\nIncrease OCSP Cache expiry time from 24 hours to 120 hours.\nFix pyarrow cxx11 abi compatibility issue\nUse new query result format parameter in python tests\n\n\n\nv2.0.3(November 1,2019)\n\nFix for ,Pandas fetch API did not handle the case that first chunk is empty correctly.\nUpdated with botocore, boto3 and requests packages to the latest version.\nPinned stable versions of Azure urllib3 packages.\n\n\n\nv2.0.2(October 21,2019)\n\nFix sessions remaining open even if they are disposed manually. Retry deleting session if the connection is explicitly closed.\nFix memory leak in the new fetch pandas API\nFix Auditwheel failed with python37\nReduce the footprint of Python Connector\nSupport asn1crypto 1.1.x\nEnsure that the cython components are present for Conda package\n\n\n\nv2.0.1(October 04,2019)\n\nAdd asn1crypto requirement to mitigate incompatibility change\n\n\n\nv2.0.0(September 30,2019)\n\nRelease Python Connector 2.0.0 for Arrow format change.\nFix\u00a0SF_OCSP_RESPONSE_CACHE_DIR referring to the OCSP cache response file directory and not the top level of directory.\nFix Malformed certificate ID key causes uncaught KeyError.\nNo retry for certificate errors.\nFix In-Memory OCSP Response Cache - PythonConnector\nMove AWS_ID and AWS_SECRET_KEY to their newer versions in the Python client\nFix result set downloader for ijson 2.5\nMake authenticator field case insensitive earlier\nUpdate\u00a0USER-AGENT to be consistent with new format\nUpdate Python Driver URL Whitelist to support US Gov domain\nFix memory leak in python connector panda df fetch API\n\n\n\nv1.9.1(October 4,2019)\n\nAdd asn1crypto requirement to mitigate incompatibility change.\n\n\n\nv1.9.0(August 26,2019) REMOVED from pypi due to dependency compatibility issues\n\nImplement converter for all arrow data types in python connector extension\nFix arrow error when returning empty result using python connecter\nFix OCSP responder hang, AttributeError: 'ReadTimeout' object has no attribute 'message'\nUpdate OCSP Connection timeout.\nFix RevokedCertificateError OOB Telemetry events are not sent\nUncaught RevocationCheckError for FAIL_OPEN in create_pair_issuer_subject\nFix uncaught exception in generate_telemetry_data function\nFix connector looses context after connection drop/restore by retrying IncompleteRead error.\nMake tzinfo class at the module level instead of inlining\n\n\n\nv1.8.7(August 12,2019)\n\nRewrote validateDefaultParameters to validate the database, schema and warehouse at connection time. False by default.\nFix OCSP Server URL problem in multithreaded env\nFix Azure Gov PUT and GET issue\n\n\n\nv1.8.6(July 29,2019)\n\nReduce retries for OCSP from Python Driver\nAzure PUT issue: ValueError: I/O operation on closed file\nAdd client information to USER-AGENT HTTP header - PythonConnector\nBetter handling of OCSP cache download failure\n\n\n\nv1.8.5(July 15,2019)\n\nDrop Python 3.4 support for Python Connector\n\n\n\nv1.8.4(July 01,2019)\n\nUpdate Python Connector to discard invalid OCSP Responses while merging caches\n\n\n\nv1.8.3(June 17,2019)\n\nUpdate Client Driver OCSP Endpoint URL for Private Link Customers\nIgnore session gone 390111 when closing\nPython3.4 using requests 2.21.0 needs older version of urllib3\nUse Account Name for Global URL\n\n\n\nv1.8.2 (June 03,2019)\n\nPendulum datatype support\n\n\n\nv1.8.1 (May 20,2019)\n\nRevoked OCSP Responses persists in Driver Cache + Logging Fix\nFixed DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated\n\n\n\nv1.8.0 (May 10, 2019)\n\nsupport numpy.bool_ in binding type\nAdd Option to Skip Request Pooling\nAdd OCSP_MODE metric\nFixed PUT URI issue for Windows path\nOCSP SoftFail\n\n\n\nv1.7.11 (April 22, 2019)\n\nnumpy timestamp with timezone support\nqmark not binding None\n\n\n\nv1.7.10 (April 8, 2019)\n\nFix the incorrect custom Server URL in Python Driver for Privatelink\n\n\n\nv1.7.9 (March 25,2019)\n\nPython Interim Solution for Custom Cache Server URL\nInternal change for pending feature\n\n\n\nv1.7.8 (March 12,2019)\n\nAdd OCSP signing certificate validity check\n\n\n\nv1.7.7 (February 22,2019)\n\nSkip HEAD operation when OVERWRITE=true for PUT\nUpdate copyright year from 2018 to 2019 for Python\n\n\n\nv1.7.6 (February 08,2019)\n\nAdjusted pyasn1 and pyasn1-module requirements for Python Connector\nAdded idna to setup.py. made pyasn1 optional for Python2\n\n\n\nv1.7.5 (January 25, 2019)\n\nIncorporate \"kwargs\" style group of key-value pairs in connection's \"execute_string\" function.\n\n\n\nv1.7.4 (January 3, 2019)\n\nInvalidate outdated OCSP response when checking cache hit\nMade keyring use optional in Python Connector\nAdded SnowflakeNullConverter for Python Connector to skip all client side conversions\nHonor CLIENT_PREFETCH_THREADS to download the result set.\nFixed the hang when region=us-west-2 is specified.\nAdded Python 3.7 tests\n\n\n\nv1.7.3 (December 11, 2018)\n\nImproved the progress bar control for SnowSQL\nFixed PUT/GET progress bar for Azure\n\n\n\nv1.7.2 (December 4, 2018)\n\nRefactored OCSP checks\nAdjusted log level to mitigate confusions\n\n\n\nv1.7.1 (November 27, 2018)\n\nFixed regex pattern warning in cursor.py\nFixed 403 error for EU deployment\nFixed the epoch time to datetime object converter for Windoww\n\n\n\nv1.7.0 (November 13, 2018)\n\nInternal change for pending feature.\n\n\n\nv1.6.12 (October 30, 2018)\n\nUpdated boto3 and botocore version dependeny.\nCatch socket.EAI_NONAME for localhost socket and raise a better error message\nAdded client_session_keep_alive_heartbeat_frequency to control heartbeat timings for client_session_keep_alive.\n\n\n\nv1.6.11 (October 23, 2018)\n\nFixed exit_on_error=true didn't work if PUT / GET error occurs\nFixed a backslash followed by a quote in a literal was not taken into account.\nAdded request_guid to each HTTP request for tracing.\n\n\n\nv1.6.10 (September 25, 2018)\n\nAdded client_session_keep_alive support.\nFixed multiline double quote expressions PR #117 (@bensowden)\nFixed binding datetime for TIMESTAMP type in qmark binding mode. PR #118 (@rhlahuja)\nRetry HTTP 405 to mitigate Nginx bug.\nAccept consent response for id token cache. WIP.\n\n\n\nv1.6.9 (September 13, 2018)\n\nChanged most INFO logs to DEBUG. Added INFO for key operations.\nFixed the URL query parser to get multiple values.\n\n\n\nv1.6.8 (August 30, 2018)\n\nUpdated boto3 and botocore version dependeny.\n\n\n\nv1.6.7 (August 22, 2018)\n\nEnforce virtual host URL for PUT and GET.\nAdded retryCount, clientStarTime for query-request for better service.\n\n\n\nv1.6.6 (August 9, 2018)\n\nReplaced pycryptodome with pycryptodomex to avoid namespace conflict with PyCrypto.\nFixed hang if the connection is not explicitly closed since 1.6.4.\nReauthenticate for externalbrowser while running a query.\nFixed remove_comments option for SnowSQL.\n\n\n\nv1.6.5 (July 13, 2018)\n\nFixed the current object cache in the connection for id token use.\nAdded no OCSP cache server use option.\n\n\n\nv1.6.4 (July 5, 2018)\n\nFixed div by zero for Azure PUT command.\nCache id token for SSO. This feature is WIP.\nAdded telemetry client and job timings by @dsouzam.\n\n\n\nv1.6.3 (June 14, 2018)\n\nFixed binding long value for Python 2.\n\n\n\nv1.6.2 (June 7, 2018)\n\nRemoves username restriction for OAuth. PR 86(@tjj5036)\nRetry OpenSSL.SysError in tests\nUpdated concurrent insert test as the server improved.\n\n\n\nv1.6.1 (May 17, 2018)\n\nEnable OCSP Dynamic Cache server for privatelink.\nEnsure the type of login_timeout attribute is int.\n\n\n\nv1.6.0 (May 3, 2018)\n\nEnable OCSP Cache server by default.\n\n\n\nv1.5.8 (April 26, 2018)\n\nFixed PUT command error 'Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.' for Azure deployment.\n\n\n\nv1.5.7 (April 19, 2018)\n\nFixed object has no attribute errors in Python3 for Azure deployment.\nRemoved ContentEncoding=gzip from the header for PUT command. This caused COPY failure if autocompress=false.\n\n\n\nv1.5.6 (April 5, 2018)\n\nUpdated boto3 and botocore version dependeny.\n\n\n\nv1.5.5 (March 22, 2018)\n\nFixed TypeError: list indices must be integers or slices, not str. PR/Issue 75 (@daniel-sali).\nUpdated cryptography dependency.\n\n\n\nv1.5.4 (March 15, 2018)\n\nTightened pyasn and pyasn1-modules version requirements\nAdded OS and OS_VERSION session info.\nRelaxed pycryptodome version requirements. No 3.5.0 should be used.\n\n\n\nv1.5.3 (March 9, 2018)\n\nPulled back pyasn1 for OCSP check in Python 2. Python 3 continue using asn1crypto for better performance.\nLimit the upper bound of pycryptodome version to less than 3.5.0 for Issue 65.\n\n\n\nv1.5.2 (March 1, 2018)\n\nFixed failue in case HOME/USERPROFILE is not set.\nUpdated boto3 and botocore version dependeny.\n\n\n\nv1.5.1 (February 15, 2018)\n\nPrototyped oauth. Won't work without the server change.\nRetry OCSP data parse failure\nFixed paramstyle=qmark binding for SQLAlchemy\n\n\n\nv1.5.0 (January 26, 2018)\n\nRemoved pyasn1 and pyasn1-modules from the dependency.\nPrototyped key pair authentication.\nFixed OCSP response cache expiration check.\n\n\n\nv1.4.17 (January 19, 2018)\n\nAdjusted pyasn1 and pyasn1-modules version dependency. PR 48 (@baxen)\nStarted replacing pyasn1 with asn1crypto Not activated yet.\n\n\n\nv1.4.16 (January 16, 2018)\n\nAdded OCSP cache related tools.\n\n\n\nv1.4.15 (January 11, 2018)\n\nAdded OCSP cache server option.\n\n\n\nv1.4.14 (December 14, 2017)\n\nImproved OCSP response dump util.\n\n\n\nv1.4.13 (November 30, 2017)\n\nUpdated boto3 and botocore version dependeny.\n\n\n\nv1.4.12 (November 16, 2017)\n\nAdded qmark and numeric paramstyle support for server side binding.\nAdded timezone session parameter support to connections.\nFixed a file handler leak in OCSP checks.\n\n\n\nv1.4.11 (November 9, 2017)\n\nFixed Azure PUT command to use AES CBC key encryption.\nAdded retry for intermittent PyAsn1Error.\n\n\n\nv1.4.10 (October 26, 2017)\n\nAdded Azure support for PUT and GET commands.\nUpdated cryptography, boto3 and botocore version dependeny.\n\n\n\nv1.4.9 (October 10, 2017)\n\nFixed a regression caused by pyasn1 upgrade.\n\n\n\nv1.4.8 (October 5, 2017)\n\nUpdated Fed/SSO parameters. The production version of Fed/SSO from Python Connector requires this version.\nRefactored for Azure support\nSet CLIENT_APP_ID and CLIENT_APP_VERSION in all requests\nSupport new behaviors of newer version of pyasn1. Relaxed the dependency.\nMaking socket timeout same as the login time\nFixed the case where no error message is attached.\n\n\n\nv1.4.7 (September 20, 2017)\n\nRefresh AWS token in PUT command if S3UploadFailedError includes the ExpiredToken error\nRetry all of 5xx in connection\n\n\n\nv1.4.6 (September 14, 2017)\n\nMitigated sigint handler config failure for SQLAlchemy\nImproved the message for invalid SSL certificate error\nRetry forever for query to mitigate 500 errors\n\n\n\nv1.4.5 (August 31, 2017)\n\nFixed regression in #34 by rewriting SAML 2.0 compliant service application support.\nCleaned up logger by moving instance to module.\n\n\n\nv1.4.4 (August 24, 2017)\n\nFixed Azure blob certificate issue. OCSP response structure bug fix\nAdded SAML 2.0 compliant service application support. preview feature.\nUpgraded SSL wrapper with the latest urllib3 pyopenssl glue module. It uses kqueue, epoll or poll in replacement of select to read data from socket if available.\n\n\n\nv1.4.3 (August 17, 2017)\n\nChanged the log levels for some messages from ERROR to DEBUG to address confusion as real incidents. In fact, they are not real issues but signals for connection retry.\nAdded certifi to the dependent component list to mitigate CA root certificate out of date issue.\nSet the maximum versions of dependent components boto3 and botocore.\nUpdated cryptography and pyOpenSSL version dependeny change.\nAdded a connection parameter validate_default_parameters to validate the default database, schema and warehouse. If the specified object doesn't exist, it raises an error.\n\n\n\nv1.4.2 (August 3, 2017)\n\nFixed retry HTTP 400 in upload file when AWS token expires\nRelaxed the version of dependent components pyasn1 and pyasn1-modules\n\n\n\nv1.4.1 (July 26, 2017)\n\nPinned pyasn1 and pyasn1-modules versions to 0.2.3 and 0.0.9, respectively\n\n\n\nv1.4.0 (July 6, 2017)\n\nRelaxed the versions of dependent components boto3, botocore, cffi and cryptography and pyOpenSSL\nMinor improvements in OCSP response file cache\n\n\n\nv1.3.18 (June 15, 2017)\n\nFixed OCSP response cache file not found issue on Windows. Drive letter was taken off\nUse less restrictive cryptography>=1.7,<1.8\nAdded ORC detection in PUT command\n\n\n\nv1.3.17 (June 1, 2017)\n\nTimeout OCSP request in 60 seconds and retry\nSet autocommit and abort_detached_query session parameters in authentication time if specified\nFixed cross region stage issue. Could not get files in us-west-2 region S3 bucket from us-east-1\n\n\n\nv1.3.16 (April 20, 2017)\n\nFixed issue in fetching DATE causing [Error 22] Invalid argument on Windows\nRetry on RuntimeError in requests\n\n\n\nv1.3.15 (March 30, 2017)\n\nRefactored data converters in fetch to improve performance\nFixed timestamp format FF to honor the scale of data type\nImproved the security of OKTA authentication with hostname verifications\nRetry PUT on the error OpenSSL.SSL.SysCallError 10053 with lower concurrency\nAdded raw_msg attribute to Error class\nRefactored session managements\n\n\n\nv1.3.14 (February 24, 2017)\n\nImproved PUT and GET error handler.\nAdded proxy support to OCSP checks.\nUse proxy parameters for PUT and GET commands.\nAdded sfqid and sqlstate to the results from query results.\nFixed the connection timeout calculation based on login_timeout and network_timeout.\nImproved error messages in case of 403, 502 and 504 HTTP reponse code.\nUpgraded cryptography to 1.7.2, boto3 to 1.4.4 and botocore to 1.5.14.\nRemoved explicit DNS lookup for OCSP URL.\n\n\n\nv1.3.13 (February 9, 2017)\n\nFixed AWS SQS connection error with OCSP checks\nAdded login_timeout and network_timeout parameters to the Connection objects.\nFixed forbidden access error handing\n\n\n\nv1.3.12 (February 2, 2017)\n\nFixed region parameter. One character was truncated from the tail of account name\nImproved performance of fetching data by refactoring fetchone method\n\n\n\nv1.3.11 (January 27, 2017)\n\nFixed the regression in 1.3.8 that caused intermittent 504 errors\n\n\n\nv1.3.10 (January 26, 2017)\n\nCompress data in HTTP requests at all times except empty data or OKTA request\nRefactored FIXED, REAL and TIMESTAMP data fetch to improve performance. This mainly impacts SnowSQL\nAdded region option to support EU deployments better\nIncreased the retry counter for OCSP servers to mitigate intermittent failure\nRefactored HTTP access retry logic\n\n\n\nv1.3.9 (January 16, 2017)\n\nUpgraded botocore to 1.4.93 to fix and boto3 to 1.4.3 to fix the HTTPS request failure in Python 3.6\nFixed python2 incomaptible import http.client\nRetry OCSP validation in case of non-200 HTTP code returned\n\n\n\nv1.3.8 (January 12, 2017)\n\nConvert non-UTF-8 data in the large result set chunk to Unicode replacement characters to avoid decode error.\nUpdated copyright year to 2017.\nUse six package to support both PY2 and PY3 for some functions\nUpgraded cryptography to 1.7.1 to address MacOS Python 3.6 build issue.\nFixed OverflowError caused by invalid range of timetamp data for SnowSQL.\n\n\n\nv1.3.7 (December 8, 2016)\n\nIncreased the validity date acceptance window to prevent OCSP returning invalid responses due to out-of-scope validity dates for certificates.\nEnabled OCSP response cache file by default.\n\n\n\nv1.3.6 (December 1, 2016)\n\nUpgraded cryptography to 1.5.3, pyOpenSSL to 16.2.0 and cffi to 1.9.1.\n\n\n\nv1.3.5 (November 17, 2016)\n\nFixed CA list cache race condition\nAdded retry intermittent 400 HTTP Bad Request error\n\n\n\nv1.3.4 (November 3, 2016)\n\nAdded quoted_name data type support for binding by SQLAlchemy\nNot to compress parquiet file in PUT command\n\n\n\nv1.3.3 (October 20, 2016)\n\nDowngraded botocore to 1.4.37 due to potential regression.\nIncreased the stability of PUT and GET commands\n\n\n\nv1.3.2 (October 12, 2016)\n\nUpgraded botocore to 1.4.52.\nSet the signature version to v4 to AWS client. This impacts PUT, GET commands and fetching large result set.\n\n\n\nv1.3.1 (September 30, 2016)\n\nAdded an account name including subdomain.\n\n\n\nv1.3.0 (September 26, 2016)\n\n\nAdded support for the BINARY data type, which enables support for more Python data types:\n\n\nPython 3:\n\nbytes and bytearray can be used for binding.\nbytes is also used for fetching BINARY data type.\n\n\n\nPython 2:\n\nbytearray can be used for binding\nstr is used for fetching BINARY data type.\n\n\n\n\n\nAdded proxy_user and proxy_password connection parameters for proxy servers that require authentication.\n\n\n\n\nv1.2.8 (August 16, 2016)\n\nUpgraded botocore to 1.4.37.\nAdded Connection.execute_string and Connection.execute_stream to run multiple statements in a string and stream.\nIncreased the stability of fetching data for Python 2.\nRefactored memory usage in fetching large result set (Work in Progress).\n\n\n\nv1.2.7 (July 31, 2016)\n\nFixed snowflake.cursor.rowcount for INSERT ALL.\nForce OCSP cache invalidation after 24 hours for better security.\nUse use_accelerate_endpoint in PUT and GET if Transfer acceleration is enabled for the S3 bucket.\nFixed the side effect of python-future that loads test.py in the current directory.\n\n\n\nv1.2.6 (July 13, 2016)\n\nFixed the AWS token renewal issue with PUT command when uploading uncompressed large files.\n\n\n\nv1.2.5 (July 8, 2016)\n\nAdded retry for errors S3UploadFailedError and RetriesExceededError in PUT and GET, respectively.\n\n\n\nv1.2.4 (July 6, 2016)\n\nAdded max_connection_pool parameter to Connection so that you can specify the maximum number of HTTP/HTTPS connections in the pool.\nMinor enhancements for SnowSQL.\n\n\n\nv1.2.3 (June 29, 2016)\n\nFixed 404 issue in GET command. An extra slash character changed the S3 path and failed to identify the file to download.\n\n\n\nv1.2.2 (June 21, 2016)\n\nUpgraded botocore to 1.4.26.\nAdded retry for 403 error when accessing S3.\n\n\n\nv1.2.1 (June 13, 2016)\n\nImproved fetch performance for data types (part 2): DATE, TIME, TIMESTAMP, TIMESTAMP_LTZ, TIMESTAMP_NTZ and TIMESTAMP_TZ.\n\n\n\nv1.2.0 (June 10, 2016)\n\nImproved fetch performance for data types (part 1): FIXED, REAL, STRING.\n\n\n\nv1.1.5 (June 2, 2016)\n\nUpgraded boto3 to 1.3.1 and botocore and 1.4.22.\nFixed snowflake.cursor.rowcount for DML by snowflake.cursor.executemany.\nAdded numpy data type binding support. numpy.intN, numpy.floatN and numpy.datetime64 can be bound and fetched.\n\n\n\nv1.1.4 (May 21, 2016)\n\nUpgraded cffi to 1.6.0.\nMinor enhancements to SnowSQL.\n\n\n\nv1.1.3 (May 5, 2016)\n\nUpgraded cryptography to 1.3.2.\n\n\n\nv1.1.2 (May 4, 2016)\n\nChanged the dependency of tzlocal optional.\nFixed charmap error in OCSP checks.\n\n\n\nv1.1.1 (Apr 11, 2016)\n\nFixed OCSP revocation check issue with the new certificate and AWS S3.\nUpgraded cryptography to 1.3.1 and pyOpenSSL to 16.0.0.\n\n\n\nv1.1.0 (Apr 4, 2016)\n\nAdded bzip2 support in PUT command. This feature requires a server upgrade.\nReplaced the self contained packages in snowflake._vendor with the dependency of boto3 1.3.0 and botocore 1.4.2.\n\n\n\nv1.0.7 (Mar 21, 2016)\n\nKeep pyOpenSSL at 0.15.1.\n\n\n\nv1.0.6 (Mar 15, 2016)\n\nUpgraded cryptography to 1.2.3.\nAdded support for TIME data type, which is now a Snowflake supported data type. This feature requires a server upgrade.\nAdded snowflake.connector.DistCursor to fetch the results in dict instead of tuple.\nAdded compression to the SQL text and commands.\n\n\n\nv1.0.5 (Mar 1, 2016)\n\nUpgraded cryptography to 1.2.2 and cffi to 1.5.2.\nFixed the conversion from TIMESTAMP_LTZ to datetime in queries.\n\n\n\nv1.0.4 (Feb 15, 2016)\n\nFixed the truncated parallel large result set.\nAdded retry OpenSSL low level errors ETIMEDOUT and ECONNRESET.\nTime out all HTTPS requests so that the Python Connector can retry the job or recheck the status.\nFixed the location of encrypted data for PUT command. They used to be in the same directory as the source data files.\nAdded support for renewing the AWS token used in PUT commands if the token expires.\n\n\n\nv1.0.3 (Jan 13, 2016)\n\n\nAdded support for the BOOLEAN data type (i.e. TRUE or FALSE). This changes the behavior of the binding for the bool type object:\n\nPreviously, bool was bound as a numeric value (i.e. 1 for True, 0 for False).\nNow, bool is bound as native SQL data (i.e. TRUE or FALSE).\n\n\n\nAdded the autocommit method to the Connection object:\n\nBy default, autocommit mode is ON (i.e. each DML statement commits the change).\nIf autocommit mode is OFF, the commit and rollback methods are enabled.\n\n\n\nAvoid segfault issue for cryptography 1.2 in Mac OSX by using 1.1 until resolved.\n\n\n\n\nv1.0.2 (Dec 15, 2015)\n\nUpgraded boto3 1.2.2, botocore 1.3.12.\nRemoved SSLv3 mapping from the initial table.\n\n\n\nv1.0.1 (Dec 8, 2015)\n\nMinor bug fixes.\n\n\n\nv1.0.0 (Dec 1, 2015)\n\nGeneral Availability release.\n\n\n\n"}, {"name": "smart-open", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsmart_open \u2014 utils for streaming large files in Python\nWhat?\nWhy?\nHow?\nDocumentation\nInstallation\nBuilt-in help\nMore examples\nCompression Handling\nTransport-specific Options\nS3 Credentials\nIterating Over an S3 Bucket's Contents\nGCS Credentials\nAzure Credentials\nDrop-in replacement of pathlib.Path.open\nHow do I ...?\nExtending smart_open\nTesting smart_open\nComments, bug reports\n\n\n\n\n\nREADME.rst\n\n\n\n\nsmart_open \u2014 utils for streaming large files in Python\n\n \n  \n\nWhat?\nsmart_open is a Python 3 library for efficient streaming of very large files from/to storages such as S3, GCS, Azure Blob Storage, HDFS, WebHDFS, HTTP, HTTPS, SFTP, or local filesystem. It supports transparent, on-the-fly (de-)compression for a variety of different formats.\nsmart_open is a drop-in replacement for Python's built-in open(): it can do anything open can (100% compatible, falls back to native open wherever possible), plus lots of nifty extra stuff on top.\nPython 2.7 is no longer supported. If you need Python 2.7, please use smart_open 1.10.1, the last version to support Python 2.\n\nWhy?\nWorking with large remote files, for example using Amazon's boto3 Python library, is a pain.\nboto3's Object.upload_fileobj() and Object.download_fileobj() methods require gotcha-prone boilerplate to use successfully, such as constructing file-like object wrappers.\nsmart_open shields you from that. It builds on boto3 and other remote storage libraries, but offers a clean unified Pythonic API. The result is less code for you to write and fewer bugs to make.\n\nHow?\nsmart_open is well-tested, well-documented, and has a simple Pythonic API:\n>>> from smart_open import open\n>>>\n>>> # stream lines from an S3 object\n>>> for line in open('s3://commoncrawl/robots.txt'):\n...    print(repr(line))\n...    break\n'User-Agent: *\\n'\n\n>>> # stream from/to compressed files, with transparent (de)compression:\n>>> for line in open('smart_open/tests/test_data/1984.txt.gz', encoding='utf-8'):\n...    print(repr(line))\n'It was a bright cold day in April, and the clocks were striking thirteen.\\n'\n'Winston Smith, his chin nuzzled into his breast in an effort to escape the vile\\n'\n'wind, slipped quickly through the glass doors of Victory Mansions, though not\\n'\n'quickly enough to prevent a swirl of gritty dust from entering along with him.\\n'\n\n>>> # can use context managers too:\n>>> with open('smart_open/tests/test_data/1984.txt.gz') as fin:\n...    with open('smart_open/tests/test_data/1984.txt.bz2', 'w') as fout:\n...        for line in fin:\n...           fout.write(line)\n74\n80\n78\n79\n\n>>> # can use any IOBase operations, like seek\n>>> with open('s3://commoncrawl/robots.txt', 'rb') as fin:\n...     for line in fin:\n...         print(repr(line.decode('utf-8')))\n...         break\n...     offset = fin.seek(0)  # seek to the beginning\n...     print(fin.read(4))\n'User-Agent: *\\n'\nb'User'\n\n>>> # stream from HTTP\n>>> for line in open('http://example.com/index.html'):\n...     print(repr(line))\n...     break\n'<!doctype html>\\n'\nOther examples of URLs that smart_open accepts:\ns3://my_bucket/my_key\ns3://my_key:my_secret@my_bucket/my_key\ns3://my_key:my_secret@my_server:my_port@my_bucket/my_key\ngs://my_bucket/my_blob\nazure://my_bucket/my_blob\nhdfs:///path/file\nhdfs://path/file\nwebhdfs://host:port/path/file\n./local/path/file\n~/local/path/file\nlocal/path/file\n./local/path/file.gz\nfile:///home/user/file\nfile:///home/user/file.bz2\n[ssh|scp|sftp]://username@host//path/file\n[ssh|scp|sftp]://username@host/path/file\n[ssh|scp|sftp]://username:password@host/path/file\n\n\nDocumentation\n\nInstallation\nsmart_open supports a wide range of storage solutions, including AWS S3, Google Cloud and Azure.\nEach individual solution has its own dependencies.\nBy default, smart_open does not install any dependencies, in order to keep the installation size small.\nYou can install these dependencies explicitly using:\npip install smart_open[azure] # Install Azure deps\npip install smart_open[gcs] # Install GCS deps\npip install smart_open[s3] # Install S3 deps\n\nOr, if you don't mind installing a large number of third party libraries, you can install all dependencies using:\npip install smart_open[all]\n\nBe warned that this option increases the installation size significantly, e.g. over 100MB.\nIf you're upgrading from smart_open versions 2.x and below, please check out the Migration Guide.\n\nBuilt-in help\nFor detailed API info, see the online help:\nhelp('smart_open')\nor click here to view the help in your browser.\n\nMore examples\nFor the sake of simplicity, the examples below assume you have all the dependencies installed, i.e. you have done:\npip install smart_open[all]\n\n>>> import os, boto3\n>>> from smart_open import open\n>>>\n>>> # stream content *into* S3 (write mode) using a custom session\n>>> session = boto3.Session(\n...     aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\n...     aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'],\n... )\n>>> url = 's3://smart-open-py37-benchmark-results/test.txt'\n>>> with open(url, 'wb', transport_params={'client': session.client('s3')}) as fout:\n...     bytes_written = fout.write(b'hello world!')\n...     print(bytes_written)\n12\n# stream from HDFS\nfor line in open('hdfs://user/hadoop/my_file.txt', encoding='utf8'):\n    print(line)\n\n# stream from WebHDFS\nfor line in open('webhdfs://host:port/user/hadoop/my_file.txt'):\n    print(line)\n\n# stream content *into* HDFS (write mode):\nwith open('hdfs://host:port/user/hadoop/my_file.txt', 'wb') as fout:\n    fout.write(b'hello world')\n\n# stream content *into* WebHDFS (write mode):\nwith open('webhdfs://host:port/user/hadoop/my_file.txt', 'wb') as fout:\n    fout.write(b'hello world')\n\n# stream from a completely custom s3 server, like s3proxy:\nfor line in open('s3u://user:secret@host:port@mybucket/mykey.txt'):\n    print(line)\n\n# Stream to Digital Ocean Spaces bucket providing credentials from boto3 profile\nsession = boto3.Session(profile_name='digitalocean')\nclient = session.client('s3', endpoint_url='https://ams3.digitaloceanspaces.com')\ntransport_params = {'client': client}\nwith open('s3://bucket/key.txt', 'wb', transport_params=transport_params) as fout:\n    fout.write(b'here we stand')\n\n# stream from GCS\nfor line in open('gs://my_bucket/my_file.txt'):\n    print(line)\n\n# stream content *into* GCS (write mode):\nwith open('gs://my_bucket/my_file.txt', 'wb') as fout:\n    fout.write(b'hello world')\n\n# stream from Azure Blob Storage\nconnect_str = os.environ['AZURE_STORAGE_CONNECTION_STRING']\ntransport_params = {\n    'client': azure.storage.blob.BlobServiceClient.from_connection_string(connect_str),\n}\nfor line in open('azure://mycontainer/myfile.txt', transport_params=transport_params):\n    print(line)\n\n# stream content *into* Azure Blob Storage (write mode):\nconnect_str = os.environ['AZURE_STORAGE_CONNECTION_STRING']\ntransport_params = {\n    'client': azure.storage.blob.BlobServiceClient.from_connection_string(connect_str),\n}\nwith open('azure://mycontainer/my_file.txt', 'wb', transport_params=transport_params) as fout:\n    fout.write(b'hello world')\n\nCompression Handling\nThe top-level compression parameter controls compression/decompression behavior when reading and writing.\nThe supported values for this parameter are:\n\ninfer_from_extension (default behavior)\ndisable\n.gz\n.bz2\n\nBy default, smart_open determines the compression algorithm to use based on the file extension.\n>>> from smart_open import open, register_compressor\n>>> with open('smart_open/tests/test_data/1984.txt.gz') as fin:\n...     print(fin.read(32))\nIt was a bright cold day in Apri\nYou can override this behavior to either disable compression, or explicitly specify the algorithm to use.\nTo disable compression:\n>>> from smart_open import open, register_compressor\n>>> with open('smart_open/tests/test_data/1984.txt.gz', 'rb', compression='disable') as fin:\n...     print(fin.read(32))\nb'\\x1f\\x8b\\x08\\x08\\x85F\\x94\\\\\\x00\\x031984.txt\\x005\\x8f=r\\xc3@\\x08\\x85{\\x9d\\xe2\\x1d@'\nTo specify the algorithm explicitly (e.g. for non-standard file extensions):\n>>> from smart_open import open, register_compressor\n>>> with open('smart_open/tests/test_data/1984.txt.gzip', compression='.gz') as fin:\n...     print(fin.read(32))\nIt was a bright cold day in Apri\nYou can also easily add support for other file extensions and compression formats.\nFor example, to open xz-compressed files:\n>>> import lzma, os\n>>> from smart_open import open, register_compressor\n\n>>> def _handle_xz(file_obj, mode):\n...      return lzma.LZMAFile(filename=file_obj, mode=mode, format=lzma.FORMAT_XZ)\n\n>>> register_compressor('.xz', _handle_xz)\n\n>>> with open('smart_open/tests/test_data/1984.txt.xz') as fin:\n...     print(fin.read(32))\nIt was a bright cold day in Apri\nlzma is in the standard library in Python 3.3 and greater.\nFor 2.7, use backports.lzma.\n\nTransport-specific Options\nsmart_open supports a wide range of transport options out of the box, including:\n\nS3\nHTTP, HTTPS (read-only)\nSSH, SCP and SFTP\nWebHDFS\nGCS\nAzure Blob Storage\n\nEach option involves setting up its own set of parameters.\nFor example, for accessing S3, you often need to set up authentication, like API keys or a profile name.\nsmart_open's open function accepts a keyword argument transport_params which accepts additional parameters for the transport layer.\nHere are some examples of using this parameter:\n>>> import boto3\n>>> fin = open('s3://commoncrawl/robots.txt', transport_params=dict(client=boto3.client('s3')))\n>>> fin = open('s3://commoncrawl/robots.txt', transport_params=dict(buffer_size=1024))\nFor the full list of keyword arguments supported by each transport option, see the documentation:\nhelp('smart_open.open')\n\nS3 Credentials\nsmart_open uses the boto3 library to talk to S3.\nboto3 has several mechanisms for determining the credentials to use.\nBy default, smart_open will defer to boto3 and let the latter take care of the credentials.\nThere are several ways to override this behavior.\nThe first is to pass a boto3.Client object as a transport parameter to the open function.\nYou can customize the credentials when constructing the session for the client.\nsmart_open will then use the session when talking to S3.\nsession = boto3.Session(\n    aws_access_key_id=ACCESS_KEY,\n    aws_secret_access_key=SECRET_KEY,\n    aws_session_token=SESSION_TOKEN,\n)\nclient = session.client('s3', endpoint_url=..., config=...)\nfin = open('s3://bucket/key', transport_params=dict(client=client))\nYour second option is to specify the credentials within the S3 URL itself:\nfin = open('s3://aws_access_key_id:aws_secret_access_key@bucket/key', ...)\nImportant: The two methods above are mutually exclusive. If you pass an AWS client and the URL contains credentials, smart_open will ignore the latter.\nImportant: smart_open ignores configuration files from the older boto library.\nPort your old boto settings to boto3 in order to use them with smart_open.\n\nIterating Over an S3 Bucket's Contents\nSince going over all (or select) keys in an S3 bucket is a very common operation, there's also an extra function smart_open.s3.iter_bucket() that does this efficiently, processing the bucket keys in parallel (using multiprocessing):\n>>> from smart_open import s3\n>>> # we use workers=1 for reproducibility; you should use as many workers as you have cores\n>>> bucket = 'silo-open-data'\n>>> prefix = 'Official/annual/monthly_rain/'\n>>> for key, content in s3.iter_bucket(bucket, prefix=prefix, accept_key=lambda key: '/201' in key, workers=1, key_limit=3):\n...     print(key, round(len(content) / 2**20))\nOfficial/annual/monthly_rain/2010.monthly_rain.nc 13\nOfficial/annual/monthly_rain/2011.monthly_rain.nc 13\nOfficial/annual/monthly_rain/2012.monthly_rain.nc 13\n\nGCS Credentials\nsmart_open uses the google-cloud-storage library to talk to GCS.\ngoogle-cloud-storage uses the google-cloud package under the hood to handle authentication.\nThere are several options to provide\ncredentials.\nBy default, smart_open will defer to google-cloud-storage and let it take care of the credentials.\nTo override this behavior, pass a google.cloud.storage.Client object as a transport parameter to the open function.\nYou can customize the credentials\nwhen constructing the client. smart_open will then use the client when talking to GCS. To follow allow with\nthe example below, refer to Google's guide\nto setting up GCS authentication with a service account.\nimport os\nfrom google.cloud.storage import Client\nservice_account_path = os.environ['GOOGLE_APPLICATION_CREDENTIALS']\nclient = Client.from_service_account_json(service_account_path)\nfin = open('gs://gcp-public-data-landsat/index.csv.gz', transport_params=dict(client=client))\nIf you need more credential options, you can create an explicit google.auth.credentials.Credentials object\nand pass it to the Client. To create an API token for use in the example below, refer to the\nGCS authentication guide.\nimport os\nfrom google.auth.credentials import Credentials\nfrom google.cloud.storage import Client\ntoken = os.environ['GOOGLE_API_TOKEN']\ncredentials = Credentials(token=token)\nclient = Client(credentials=credentials)\nfin = open('gs://gcp-public-data-landsat/index.csv.gz', transport_params=dict(client=client))\n\nAzure Credentials\nsmart_open uses the azure-storage-blob library to talk to Azure Blob Storage.\nBy default, smart_open will defer to azure-storage-blob and let it take care of the credentials.\nAzure Blob Storage does not have any ways of inferring credentials therefore, passing a azure.storage.blob.BlobServiceClient\nobject as a transport parameter to the open function is required.\nYou can customize the credentials\nwhen constructing the client. smart_open will then use the client when talking to. To follow allow with\nthe example below, refer to Azure's guide\nto setting up authentication.\nimport os\nfrom azure.storage.blob import BlobServiceClient\nazure_storage_connection_string = os.environ['AZURE_STORAGE_CONNECTION_STRING']\nclient = BlobServiceClient.from_connection_string(azure_storage_connection_string)\nfin = open('azure://my_container/my_blob.txt', transport_params=dict(client=client))\nIf you need more credential options, refer to the\nAzure Storage authentication guide.\n\nDrop-in replacement of pathlib.Path.open\nsmart_open.open can also be used with Path objects.\nThe built-in Path.open() is not able to read text from compressed files, so use patch_pathlib to replace it with smart_open.open() instead.\nThis can be helpful when e.g. working with compressed files.\n>>> from pathlib import Path\n>>> from smart_open.smart_open_lib import patch_pathlib\n>>>\n>>> _ = patch_pathlib()  # replace `Path.open` with `smart_open.open`\n>>>\n>>> path = Path(\"smart_open/tests/test_data/crime-and-punishment.txt.gz\")\n>>>\n>>> with path.open(\"r\") as infile:\n...     print(infile.readline()[:41])\n\u0412 \u043d\u0430\u0447\u0430\u043b\u0435 \u0438\u044e\u043b\u044f, \u0432 \u0447\u0440\u0435\u0437\u0432\u044b\u0447\u0430\u0439\u043d\u043e \u0436\u0430\u0440\u043a\u043e\u0435 \u0432\u0440\u0435\u043c\u044f\n\nHow do I ...?\nSee this document.\n\nExtending smart_open\nSee this document.\n\nTesting smart_open\nsmart_open comes with a comprehensive suite of unit tests.\nBefore you can run the test suite, install the test dependencies:\npip install -e .[test]\n\nNow, you can run the unit tests:\npytest smart_open\n\nThe tests are also run automatically with Travis CI on every commit push & pull request.\n\nComments, bug reports\nsmart_open lives on Github. You can file\nissues or pull requests there. Suggestions, pull requests and improvements welcome!\n\nsmart_open is open source software released under the MIT license.\nCopyright (c) 2015-now Radim \u0158eh\u016f\u0159ek.\n\n\n"}, {"name": "scikit-learn", "readme": "\n         \n\nscikit-learn is a Python module for machine learning built on top of\nSciPy and is distributed under the 3-Clause BSD license.\nThe project was started in 2007 by David Cournapeau as a Google Summer\nof Code project, and since then many volunteers have contributed. See\nthe About us page\nfor a list of core contributors.\nIt is currently maintained by a team of volunteers.\nWebsite: https://scikit-learn.org\n\nInstallation\n\nDependencies\nscikit-learn requires:\n\nPython (>= 3.8)\nNumPy (>= 1.17.3)\nSciPy (>= 1.5.0)\njoblib (>= 1.1.1)\nthreadpoolctl (>= 2.0.0)\n\n\nScikit-learn 0.20 was the last version to support Python 2.7 and Python 3.4.\nscikit-learn 1.0 and later require Python 3.7 or newer.\nscikit-learn 1.1 and later require Python 3.8 or newer.\nScikit-learn plotting capabilities (i.e., functions start with plot_ and\nclasses end with \u201cDisplay\u201d) require Matplotlib (>= 3.1.3).\nFor running the examples Matplotlib >= 3.1.3 is required.\nA few examples require scikit-image >= 0.16.2, a few examples\nrequire pandas >= 1.0.5, some examples require seaborn >=\n0.9.0 and plotly >= 5.14.0.\n\n\nUser installation\nIf you already have a working installation of numpy and scipy,\nthe easiest way to install scikit-learn is using pip:\npip install -U scikit-learn\nor conda:\nconda install -c conda-forge scikit-learn\nThe documentation includes more detailed installation instructions.\n\n\n\nChangelog\nSee the changelog\nfor a history of notable changes to scikit-learn.\n\n\nDevelopment\nWe welcome new contributors of all experience levels. The scikit-learn\ncommunity goals are to be helpful, welcoming, and effective. The\nDevelopment Guide\nhas detailed information about contributing code, documentation, tests, and\nmore. We\u2019ve included some basic information in this README.\n\nImportant links\n\nOfficial source code repo: https://github.com/scikit-learn/scikit-learn\nDownload releases: https://pypi.org/project/scikit-learn/\nIssue tracker: https://github.com/scikit-learn/scikit-learn/issues\n\n\n\nSource code\nYou can check the latest sources with the command:\ngit clone https://github.com/scikit-learn/scikit-learn.git\n\n\nContributing\nTo learn more about making a contribution to scikit-learn, please see our\nContributing guide.\n\n\nTesting\nAfter installation, you can launch the test suite from outside the source\ndirectory (you will need to have pytest >= 7.1.2 installed):\npytest sklearn\nSee the web page https://scikit-learn.org/dev/developers/contributing.html#testing-and-improving-test-coverage\nfor more information.\n\nRandom number generation can be controlled during testing by setting\nthe SKLEARN_SEED environment variable.\n\n\n\nSubmitting a Pull Request\nBefore opening a Pull Request, have a look at the\nfull Contributing page to make sure your code complies\nwith our guidelines: https://scikit-learn.org/stable/developers/index.html\n\n\n\nProject History\nThe project was started in 2007 by David Cournapeau as a Google Summer\nof Code project, and since then many volunteers have contributed. See\nthe About us page\nfor a list of core contributors.\nThe project is currently maintained by a team of volunteers.\nNote: scikit-learn was previously referred to as scikits.learn.\n\n\nHelp and Support\n\nDocumentation\n\nHTML documentation (stable release): https://scikit-learn.org\nHTML documentation (development version): https://scikit-learn.org/dev/\nFAQ: https://scikit-learn.org/stable/faq.html\n\n\n\nCommunication\n\nMailing list: https://mail.python.org/mailman/listinfo/scikit-learn\nGitter: https://gitter.im/scikit-learn/scikit-learn\nLogos & Branding: https://github.com/scikit-learn/scikit-learn/tree/main/doc/logos\nBlog: https://blog.scikit-learn.org\nCalendar: https://blog.scikit-learn.org/calendar/\nTwitter: https://twitter.com/scikit_learn\nStack Overflow: https://stackoverflow.com/questions/tagged/scikit-learn\nGithub Discussions: https://github.com/scikit-learn/scikit-learn/discussions\nWebsite: https://scikit-learn.org\nLinkedIn: https://www.linkedin.com/company/scikit-learn\nYouTube: https://www.youtube.com/channel/UCJosFjYm0ZYVUARxuOZqnnw/playlists\nFacebook: https://www.facebook.com/scikitlearnofficial/\nInstagram: https://www.instagram.com/scikitlearnofficial/\nTikTok: https://www.tiktok.com/@scikit.learn\n\n\n\nCitation\nIf you use scikit-learn in a scientific publication, we would appreciate citations: https://scikit-learn.org/stable/about.html#citing-scikit-learn\n\n\n"}, {"name": "scikit-image", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nscikit-image: Image processing in Python\nInstallation\nLicense\nCitation\n\n\n\n\n\nREADME.md\n\n\n\n\nscikit-image: Image processing in Python\n\n\n\n\nWebsite (including documentation): https://scikit-image.org/\nDocumentation: https://scikit-image.org/docs/stable/\nUser forum: https://forum.image.sc/tag/scikit-image\nDeveloper forum: https://discuss.scientific-python.org/c/contributor/skimage\nSource: https://github.com/scikit-image/scikit-image\n\nInstallation\n\npip: pip install scikit-image\nconda: conda install -c conda-forge scikit-image\n\nAlso see installing scikit-image.\nLicense\nSee LICENSE.txt.\nCitation\nIf you find this project useful, please cite:\n\nSt\u00e9fan van der Walt, Johannes L. Sch\u00f6nberger, Juan Nunez-Iglesias,\nFran\u00e7ois Boulogne, Joshua D. Warner, Neil Yager, Emmanuelle\nGouillart, Tony Yu, and the scikit-image contributors.\nscikit-image: Image processing in Python. PeerJ 2:e453 (2014)\nhttps://doi.org/10.7717/peerj.453\n\n\n\n"}, {"name": "rpds-py", "readme": "\n  \nPython bindings to the Rust rpds crate for persistent data structures.\nWhat\u2019s here is quite minimal (in transparency, it was written initially to support replacing pyrsistent in the referencing library).\nIf you see something missing (which is very likely), a PR is definitely welcome to add it.\n\nInstallation\nThe distribution on PyPI is named rpds.py (equivalently rpds-py), and thus can be installed via e.g.:\n$ pip install rpds-py\nNote that if you install rpds-py from source, you will need a Rust toolchain installed, as it is a build-time dependency.\nAn example of how to do so in a Dockerfile can be found here.\nIf you believe you are on a common platform which should have wheels built (i.e. and not need to compile from source), feel free to file an issue or pull request modifying the GitHub action used here to build wheels via maturin.\n\n\nUsage\nMethods in general are named similarly to their rpds counterparts (rather than pyrsistent\u2018s conventions, though probably a full drop-in pyrsistent-compatible wrapper module is a good addition at some point).\n>>> from rpds import HashTrieMap, HashTrieSet, List\n\n>>> m = HashTrieMap({\"foo\": \"bar\", \"baz\": \"quux\"})\n>>> m.insert(\"spam\", 37) == HashTrieMap({\"foo\": \"bar\", \"baz\": \"quux\", \"spam\": 37})\nTrue\n>>> m.remove(\"foo\") == HashTrieMap({\"baz\": \"quux\"})\nTrue\n\n>>> s = HashTrieSet({\"foo\", \"bar\", \"baz\", \"quux\"})\n>>> s.insert(\"spam\") == HashTrieSet({\"foo\", \"bar\", \"baz\", \"quux\", \"spam\"})\nTrue\n>>> s.remove(\"foo\") == HashTrieSet({\"bar\", \"baz\", \"quux\"})\nTrue\n\n>>> L = List([1, 3, 5])\n>>> L.push_front(-1) == List([-1, 1, 3, 5])\nTrue\n>>> L.rest == List([3, 5])\nTrue\n\n"}, {"name": "python-pptx", "readme": "\n\n\n\nREADME.rst\n\n\n\n\npython-pptx is a Python library for creating, reading, and updating PowerPoint (.pptx)\nfiles.\nA typical use would be generating a PowerPoint presentation from dynamic content such as\na database query, analytics output, or a JSON payload, perhaps in response to an HTTP\nrequest and downloading the generated PPTX file in response. It runs on any Python\ncapable platform, including macOS and Linux, and does not require the PowerPoint\napplication to be installed or licensed.\nIt can also be used to analyze PowerPoint files from a corpus, perhaps to extract search\nindexing text and images.\nIn can also be used to simply automate the production of a slide or two that would be\ntedious to get right by hand, which is how this all got started.\nMore information is available in the python-pptx documentation.\nBrowse examples with screenshots to get a quick idea what you can do with\npython-pptx.\n\n\n"}, {"name": "python-multipart", "readme": "\n\npython-multipart is an Apache2 licensed streaming multipart parser for Python.\nTest coverage is currently 100%.\nDocumentation is available here.\n\nWhy?\nBecause streaming uploads are awesome for large files.\n\n\nHow to Test\nIf you want to test:\n$ pip install .[dev]\n$ inv test\n\n"}, {"name": "python-dotenv", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython-dotenv\nGetting Started\nOther Use Cases\nLoad configuration without altering the environment\nParse configuration as a stream\nLoad .env files in IPython\nCommand-line Interface\nFile format\nMultiline values\nVariable without a value\nVariable expansion\nRelated Projects\nAcknowledgements\n\n\n\n\n\nREADME.md\n\n\n\n\npython-dotenv\n\n\nPython-dotenv reads key-value pairs from a .env file and can set them as environment\nvariables. It helps in the development of applications following the\n12-factor principles.\n\nGetting Started\nOther Use Cases\n\nLoad configuration without altering the environment\nParse configuration as a stream\nLoad .env files in IPython\n\n\nCommand-line Interface\nFile format\n\nMultiline values\nVariable expansion\n\n\nRelated Projects\nAcknowledgements\n\nGetting Started\npip install python-dotenv\nIf your application takes its configuration from environment variables, like a 12-factor\napplication, launching it in development is not very practical because you have to set\nthose environment variables yourself.\nTo help you with that, you can add Python-dotenv to your application to make it load the\nconfiguration from a .env file when it is present (e.g. in development) while remaining\nconfigurable via the environment:\nfrom dotenv import load_dotenv\n\nload_dotenv()  # take environment variables from .env.\n\n# Code of your application, which uses environment variables (e.g. from `os.environ` or\n# `os.getenv`) as if they came from the actual environment.\nBy default, load_dotenv doesn't override existing environment variables.\nTo configure the development environment, add a .env in the root directory of your\nproject:\n.\n\u251c\u2500\u2500 .env\n\u2514\u2500\u2500 foo.py\n\nThe syntax of .env files supported by python-dotenv is similar to that of Bash:\n# Development settings\nDOMAIN=example.org\nADMIN_EMAIL=admin@${DOMAIN}\nROOT_URL=${DOMAIN}/app\nIf you use variables in values, ensure they are surrounded with { and }, like\n${DOMAIN}, as bare variables such as $DOMAIN are not expanded.\nYou will probably want to add .env to your .gitignore, especially if it contains\nsecrets like a password.\nSee the section \"File format\" below for more information about what you can write in a\n.env file.\nOther Use Cases\nLoad configuration without altering the environment\nThe function dotenv_values works more or less the same way as load_dotenv, except it\ndoesn't touch the environment, it just returns a dict with the values parsed from the\n.env file.\nfrom dotenv import dotenv_values\n\nconfig = dotenv_values(\".env\")  # config = {\"USER\": \"foo\", \"EMAIL\": \"foo@example.org\"}\nThis notably enables advanced configuration management:\nimport os\nfrom dotenv import dotenv_values\n\nconfig = {\n    **dotenv_values(\".env.shared\"),  # load shared development variables\n    **dotenv_values(\".env.secret\"),  # load sensitive variables\n    **os.environ,  # override loaded values with environment variables\n}\nParse configuration as a stream\nload_dotenv and dotenv_values accept streams via their stream\nargument.  It is thus possible to load the variables from sources other than the\nfilesystem (e.g. the network).\nfrom io import StringIO\n\nfrom dotenv import load_dotenv\n\nconfig = StringIO(\"USER=foo\\nEMAIL=foo@example.org\")\nload_dotenv(stream=config)\nLoad .env files in IPython\nYou can use dotenv in IPython.  By default, it will use find_dotenv to search for a\n.env file:\n%load_ext dotenv\n%dotenv\nYou can also specify a path:\n%dotenv relative/or/absolute/path/to/.env\nOptional flags:\n\n-o to override existing variables.\n-v for increased verbosity.\n\nCommand-line Interface\nA CLI interface dotenv is also included, which helps you manipulate the .env file\nwithout manually opening it.\n$ pip install \"python-dotenv[cli]\"\n$ dotenv set USER foo\n$ dotenv set EMAIL foo@example.org\n$ dotenv list\nUSER=foo\nEMAIL=foo@example.org\n$ dotenv list --format=json\n{\n  \"USER\": \"foo\",\n  \"EMAIL\": \"foo@example.org\"\n}\n$ dotenv run -- python foo.py\nRun dotenv --help for more information about the options and subcommands.\nFile format\nThe format is not formally specified and still improves over time.  That being said,\n.env files should mostly look like Bash files.\nKeys can be unquoted or single-quoted. Values can be unquoted, single- or double-quoted.\nSpaces before and after keys, equal signs, and values are ignored. Values can be followed\nby a comment.  Lines can start with the export directive, which does not affect their\ninterpretation.\nAllowed escape sequences:\n\nin single-quoted values: \\\\, \\'\nin double-quoted values: \\\\, \\', \\\", \\a, \\b, \\f, \\n, \\r, \\t, \\v\n\nMultiline values\nIt is possible for single- or double-quoted values to span multiple lines.  The following\nexamples are equivalent:\nFOO=\"first line\nsecond line\"\nFOO=\"first line\\nsecond line\"\nVariable without a value\nA variable can have no value:\nFOO\nIt results in dotenv_values associating that variable name with the value None (e.g.\n{\"FOO\": None}. load_dotenv, on the other hand, simply ignores such variables.\nThis shouldn't be confused with FOO=, in which case the variable is associated with the\nempty string.\nVariable expansion\nPython-dotenv can interpolate variables using POSIX variable expansion.\nWith load_dotenv(override=True) or dotenv_values(), the value of a variable is the\nfirst of the values defined in the following list:\n\nValue of that variable in the .env file.\nValue of that variable in the environment.\nDefault value, if provided.\nEmpty string.\n\nWith load_dotenv(override=False), the value of a variable is the first of the values\ndefined in the following list:\n\nValue of that variable in the environment.\nValue of that variable in the .env file.\nDefault value, if provided.\nEmpty string.\n\nRelated Projects\n\nHoncho - For managing\nProcfile-based applications.\ndjango-dotenv\ndjango-environ\ndjango-environ-2\ndjango-configuration\ndump-env\nenvirons\ndynaconf\nparse_it\npython-decouple\n\nAcknowledgements\nThis project is currently maintained by Saurabh Kumar and\nBertrand Bonnefoy-Claudet and would not have been possible\nwithout the support of these awesome\npeople.\n\n\n"}, {"name": "python-docx", "readme": "\n\n\n\nREADME.rst\n\n\n\n\n\npython-docx is a Python library for creating and updating Microsoft Word\n(.docx) files.\nMore information is available in the python-docx documentation.\n\n\n"}, {"name": "python-dateutil", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndateutil - powerful extensions to datetime\nInstallation\nDownload\nCode\nFeatures\nQuick example\nContributing\nAuthor\nContact\nLicense\n\n\n\n\n\nREADME.rst\n\n\n\n\n\ndateutil - powerful extensions to datetime\n  \n\n \n   \nThe dateutil module provides powerful extensions to\nthe standard datetime module, available in Python.\n\nInstallation\ndateutil can be installed from PyPI using pip (note that the package name is\ndifferent from the importable name):\npip install python-dateutil\n\n\nDownload\ndateutil is available on PyPI\nhttps://pypi.org/project/python-dateutil/\nThe documentation is hosted at:\nhttps://dateutil.readthedocs.io/en/stable/\n\nCode\nThe code and issue tracker are hosted on GitHub:\nhttps://github.com/dateutil/dateutil/\n\nFeatures\n\nComputing of relative deltas (next month, next year,\nnext Monday, last week of month, etc);\nComputing of relative deltas between two given\ndate and/or datetime objects;\nComputing of dates based on very flexible recurrence rules,\nusing a superset of the iCalendar\nspecification. Parsing of RFC strings is supported as well.\nGeneric parsing of dates in almost any string format;\nTimezone (tzinfo) implementations for tzfile(5) format\nfiles (/etc/localtime, /usr/share/zoneinfo, etc), TZ\nenvironment string (in all known formats), iCalendar\nformat files, given ranges (with help from relative deltas),\nlocal machine timezone, fixed offset timezone, UTC timezone,\nand Windows registry-based time zones.\nInternal up-to-date world timezone information based on\nOlson's database.\nComputing of Easter Sunday dates for any given year,\nusing Western, Orthodox or Julian algorithms;\nA comprehensive test suite.\n\n\nQuick example\nHere's a snapshot, just to give an idea about the power of the\npackage. For more examples, look at the documentation.\nSuppose you want to know how much time is left, in\nyears/months/days/etc, before the next easter happening on a\nyear with a Friday 13th in August, and you want to get today's\ndate out of the \"date\" unix system command. Here is the code:\n>>> from dateutil.relativedelta import *\n>>> from dateutil.easter import *\n>>> from dateutil.rrule import *\n>>> from dateutil.parser import *\n>>> from datetime import *\n>>> now = parse(\"Sat Oct 11 17:13:46 UTC 2003\")\n>>> today = now.date()\n>>> year = rrule(YEARLY,dtstart=now,bymonth=8,bymonthday=13,byweekday=FR)[0].year\n>>> rdelta = relativedelta(easter(year), today)\n>>> print(\"Today is: %s\" % today)\nToday is: 2003-10-11\n>>> print(\"Year with next Aug 13th on a Friday is: %s\" % year)\nYear with next Aug 13th on a Friday is: 2004\n>>> print(\"How far is the Easter of that year: %s\" % rdelta)\nHow far is the Easter of that year: relativedelta(months=+6)\n>>> print(\"And the Easter of that year is: %s\" % (today+rdelta))\nAnd the Easter of that year is: 2004-04-11\nBeing exactly 6 months ahead was really a coincidence :)\n\nContributing\nWe welcome many types of contributions - bug reports, pull requests (code, infrastructure or documentation fixes). For more information about how to contribute to the project, see the CONTRIBUTING.md file in the repository.\n\nAuthor\nThe dateutil module was written by Gustavo Niemeyer <gustavo@niemeyer.net>\nin 2003.\nIt is maintained by:\n\nGustavo Niemeyer <gustavo@niemeyer.net> 2003-2011\nTomi Pievil\u00e4inen <tomi.pievilainen@iki.fi> 2012-2014\nYaron de Leeuw <me@jarondl.net> 2014-2016\nPaul Ganssle <paul@ganssle.io> 2015-\n\nStarting with version 2.4.1 and running until 2.8.2, all source and binary\ndistributions will be signed by a PGP key that has, at the very least, been\nsigned by the key which made the previous release. A table of release signing\nkeys can be found below:\n\n\nReleases\nSigning key fingerprint\n\n\n\n2.4.1-2.8.2\n6B49 ACBA DCF6 BD1C A206 67AB CD54 FCE3 D964 BEFB\n\n\n\nNew releases may have signed tags, but binary and source distributions\nuploaded to PyPI will no longer have GPG signatures attached.\n\nContact\nOur mailing list is available at dateutil@python.org. As it is hosted by the PSF, it is subject to the PSF code of\nconduct.\n\nLicense\nAll contributions after December 1, 2017 released under dual license - either Apache 2.0 License or the BSD 3-Clause License. Contributions before December 1, 2017 - except those those explicitly relicensed - are released only under the BSD 3-Clause License.\n\n\n"}, {"name": "pure-eval", "readme": "\n\n\n\nREADME.md\n\n\n\n\npure_eval\n  \nThis is a Python package that lets you safely evaluate certain AST nodes without triggering arbitrary code that may have unwanted side effects.\nIt can be installed from PyPI:\npip install pure_eval\n\nTo demonstrate usage, suppose we have an object defined as follows:\nclass Rectangle:\n    def __init__(self, width, height):\n        self.width = width\n        self.height = height\n\n    @property\n    def area(self):\n        print(\"Calculating area...\")\n        return self.width * self.height\n\n\nrect = Rectangle(3, 5)\nGiven the rect object, we want to evaluate whatever expressions we can in this source code:\nsource = \"(rect.width, rect.height, rect.area)\"\nThis library works with the AST, so let's parse the source code and peek inside:\nimport ast\n\ntree = ast.parse(source)\nthe_tuple = tree.body[0].value\nfor node in the_tuple.elts:\n    print(ast.dump(node))\nOutput:\nAttribute(value=Name(id='rect', ctx=Load()), attr='width', ctx=Load())\nAttribute(value=Name(id='rect', ctx=Load()), attr='height', ctx=Load())\nAttribute(value=Name(id='rect', ctx=Load()), attr='area', ctx=Load())\nNow to actually use the library. First construct an Evaluator:\nfrom pure_eval import Evaluator\n\nevaluator = Evaluator({\"rect\": rect})\nThe argument to Evaluator should be a mapping from variable names to their values. Or if you have access to the stack frame where rect is defined, you can instead use:\nevaluator = Evaluator.from_frame(frame)\nNow to evaluate some nodes, using evaluator[node]:\nprint(\"rect.width:\", evaluator[the_tuple.elts[0]])\nprint(\"rect:\", evaluator[the_tuple.elts[0].value])\nOutput:\nrect.width: 3\nrect: <__main__.Rectangle object at 0x105b0dd30>\n\nOK, but you could have done the same thing with eval. The useful part is that it will refuse to evaluate the property rect.area because that would trigger unknown code. If we try, it'll raise a CannotEval exception.\nfrom pure_eval import CannotEval\n\ntry:\n    print(\"rect.area:\", evaluator[the_tuple.elts[2]])  # fails\nexcept CannotEval as e:\n    print(e)  # prints CannotEval\nTo find all the expressions that can be evaluated in a tree:\nfor node, value in evaluator.find_expressions(tree):\n    print(ast.dump(node), value)\nOutput:\nAttribute(value=Name(id='rect', ctx=Load()), attr='width', ctx=Load()) 3\nAttribute(value=Name(id='rect', ctx=Load()), attr='height', ctx=Load()) 5\nName(id='rect', ctx=Load()) <__main__.Rectangle object at 0x105568d30>\nName(id='rect', ctx=Load()) <__main__.Rectangle object at 0x105568d30>\nName(id='rect', ctx=Load()) <__main__.Rectangle object at 0x105568d30>\nNote that this includes rect three times, once for each appearance in the source code. Since all these nodes are equivalent, we can group them together:\nfrom pure_eval import group_expressions\n\nfor nodes, values in group_expressions(evaluator.find_expressions(tree)):\n    print(len(nodes), \"nodes with value:\", values)\nOutput:\n1 nodes with value: 3\n1 nodes with value: 5\n3 nodes with value: <__main__.Rectangle object at 0x10d374d30>\n\nIf we want to list all the expressions in a tree, we may want to filter out certain expressions whose values are obvious. For example, suppose we have a function foo:\ndef foo():\n    pass\nIf we refer to foo by its name as usual, then that's not interesting:\nfrom pure_eval import is_expression_interesting\n\nnode = ast.parse('foo').body[0].value\nprint(ast.dump(node))\nprint(is_expression_interesting(node, foo))\nOutput:\nName(id='foo', ctx=Load())\nFalse\nBut if we refer to it by a different name, then it's interesting:\nnode = ast.parse('bar').body[0].value\nprint(ast.dump(node))\nprint(is_expression_interesting(node, foo))\nOutput:\nName(id='bar', ctx=Load())\nTrue\nIn general is_expression_interesting returns False for the following values:\n\nLiterals (e.g. 123, 'abc', [1, 2, 3], {'a': (), 'b': ([1, 2], [3])})\nVariables or attributes whose name is equal to the value's __name__, such as foo above or self.foo if it was a method.\nBuiltins (e.g. len) referred to by their usual name.\n\nTo make things easier, you can combine finding expressions, grouping them, and filtering out the obvious ones with:\nevaluator.interesting_expressions_grouped(root)\nTo get the source code of an AST node, I recommend asttokens.\nHere's a complete example that brings it all together:\nfrom asttokens import ASTTokens\nfrom pure_eval import Evaluator\n\nsource = \"\"\"\nx = 1\nd = {x: 2}\ny = d[x]\n\"\"\"\n\nnames = {}\nexec(source, names)\natok = ASTTokens(source, parse=True)\nfor nodes, value in Evaluator(names).interesting_expressions_grouped(atok.tree):\n    print(atok.get_text(nodes[0]), \"=\", value)\nOutput:\nx = 1\nd = {1: 2}\ny = 2\nd[x] = 2\n\n\n"}, {"name": "prompt-toolkit", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython Prompt Toolkit\nGallery\nprompt_toolkit features\nInstallation\nAbout Windows support\nGetting started\nPhilosophy\nProjects using prompt_toolkit\nSpecial thanks to\n\n\n\n\n\nREADME.rst\n\n\n\n\nPython Prompt Toolkit\n \n  \n \n\nprompt_toolkit is a library for building powerful interactive command line applications in Python.\nRead the documentation on readthedocs.\n\nGallery\nptpython is an interactive\nPython Shell, build on top of prompt_toolkit.\n\nMore examples\n\nprompt_toolkit features\nprompt_toolkit could be a replacement for GNU readline, but it can be much\nmore than that.\nSome features:\n\nPure Python.\nSyntax highlighting of the input while typing. (For instance, with a Pygments lexer.)\nMulti-line input editing.\nAdvanced code completion.\nBoth Emacs and Vi key bindings. (Similar to readline.)\nEven some advanced Vi functionality, like named registers and digraphs.\nReverse and forward incremental search.\nWorks well with Unicode double width characters. (Chinese input.)\nSelecting text for copy/paste. (Both Emacs and Vi style.)\nSupport for bracketed paste.\nMouse support for cursor positioning and scrolling.\nAuto suggestions. (Like fish shell.)\nMultiple input buffers.\nNo global state.\nLightweight, the only dependencies are Pygments and wcwidth.\nRuns on Linux, OS X, FreeBSD, OpenBSD and Windows systems.\nAnd much more...\n\nFeel free to create tickets for bugs and feature requests, and create pull\nrequests if you have nice patches that you would like to share with others.\n\nInstallation\npip install prompt_toolkit\n\nFor Conda, do:\nconda install -c https://conda.anaconda.org/conda-forge prompt_toolkit\n\n\nAbout Windows support\nprompt_toolkit is cross platform, and everything that you build on top\nshould run fine on both Unix and Windows systems. Windows support is best on\nrecent Windows 10 builds, for which the command line window supports vt100\nescape sequences. (If not supported, we fall back to using Win32 APIs for color\nand cursor movements).\nIt's worth noting that the implementation is a \"best effort of what is\npossible\". Both Unix and Windows terminals have their limitations. But in\ngeneral, the Unix experience will still be a little better.\nFor Windows, it's recommended to use either cmder or conemu.\n\nGetting started\nThe most simple example of the library would look like this:\nfrom prompt_toolkit import prompt\n\nif __name__ == '__main__':\n    answer = prompt('Give me some input: ')\n    print('You said: %s' % answer)\nFor more complex examples, have a look in the examples directory. All\nexamples are chosen to demonstrate only one thing. Also, don't be afraid to\nlook at the source code. The implementation of the prompt function could be\na good start.\n\nPhilosophy\nThe source code of prompt_toolkit should be readable, concise and\nefficient. We prefer short functions focusing each on one task and for which\nthe input and output types are clearly specified. We mostly prefer composition\nover inheritance, because inheritance can result in too much functionality in\nthe same object. We prefer immutable objects where possible (objects don't\nchange after initialization). Reusability is important. We absolutely refrain\nfrom having a changing global state, it should be possible to have multiple\nindependent instances of the same code in the same process. The architecture\nshould be layered: the lower levels operate on primitive operations and data\nstructures giving -- when correctly combined -- all the possible flexibility;\nwhile at the higher level, there should be a simpler API, ready-to-use and\nsufficient for most use cases. Thinking about algorithms and efficiency is\nimportant, but avoid premature optimization.\n\nProjects using prompt_toolkit\n\nSpecial thanks to\n\nPygments: Syntax highlighter.\nwcwidth: Determine columns needed for a wide characters.\n\n\n\n"}, {"name": "prometheus-client", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrometheus Python Client\nThree Step Demo\nInstallation\nInstrumenting\nCounter\nGauge\nSummary\nHistogram\nInfo\nEnum\nLabels\nExemplars\nDisabling _created metrics\nProcess Collector\nPlatform Collector\nDisabling Default Collector metrics\nExporting\nHTTP\nTwisted\nWSGI\nASGI\nFlask\nFastAPI + Gunicorn\nNode exporter textfile collector\nExporting to a Pushgateway\nHandlers for authentication\nBridges\nGraphite\nCustom Collectors\nMultiprocess Mode (E.g. Gunicorn)\nParser\nLinks\n\n\n\n\n\nREADME.md\n\n\n\n\nPrometheus Python Client\nThe official Python client for Prometheus.\nThree Step Demo\nOne: Install the client:\npip install prometheus-client\n\nTwo: Paste the following into a Python interpreter:\nfrom prometheus_client import start_http_server, Summary\nimport random\nimport time\n\n# Create a metric to track time spent and requests made.\nREQUEST_TIME = Summary('request_processing_seconds', 'Time spent processing request')\n\n# Decorate function with metric.\n@REQUEST_TIME.time()\ndef process_request(t):\n    \"\"\"A dummy function that takes some time.\"\"\"\n    time.sleep(t)\n\nif __name__ == '__main__':\n    # Start up the server to expose the metrics.\n    start_http_server(8000)\n    # Generate some requests.\n    while True:\n        process_request(random.random())\nThree: Visit http://localhost:8000/ to view the metrics.\nFrom one easy to use decorator you get:\n\nrequest_processing_seconds_count: Number of times this function was called.\nrequest_processing_seconds_sum: Total amount of time spent in this function.\n\nPrometheus's rate function allows calculation of both requests per second,\nand latency over time from this data.\nIn addition if you're on Linux the process metrics expose CPU, memory and\nother information about the process for free!\nInstallation\npip install prometheus-client\n\nThis package can be found on\nPyPI.\nInstrumenting\nFour types of metric are offered: Counter, Gauge, Summary and Histogram.\nSee the documentation on metric types\nand instrumentation best practices\non how to use them.\nCounter\nCounters go up, and reset when the process restarts.\nfrom prometheus_client import Counter\nc = Counter('my_failures', 'Description of counter')\nc.inc()     # Increment by 1\nc.inc(1.6)  # Increment by given value\nIf there is a suffix of _total on the metric name, it will be removed. When\nexposing the time series for counter, a _total suffix will be added. This is\nfor compatibility between OpenMetrics and the Prometheus text format, as OpenMetrics\nrequires the _total suffix.\nThere are utilities to count exceptions raised:\n@c.count_exceptions()\ndef f():\n  pass\n\nwith c.count_exceptions():\n  pass\n\n# Count only one type of exception\nwith c.count_exceptions(ValueError):\n  pass\nGauge\nGauges can go up and down.\nfrom prometheus_client import Gauge\ng = Gauge('my_inprogress_requests', 'Description of gauge')\ng.inc()      # Increment by 1\ng.dec(10)    # Decrement by given value\ng.set(4.2)   # Set to a given value\nThere are utilities for common use cases:\ng.set_to_current_time()   # Set to current unixtime\n\n# Increment when entered, decrement when exited.\n@g.track_inprogress()\ndef f():\n  pass\n\nwith g.track_inprogress():\n  pass\nA Gauge can also take its value from a callback:\nd = Gauge('data_objects', 'Number of objects')\nmy_dict = {}\nd.set_function(lambda: len(my_dict))\nSummary\nSummaries track the size and number of events.\nfrom prometheus_client import Summary\ns = Summary('request_latency_seconds', 'Description of summary')\ns.observe(4.7)    # Observe 4.7 (seconds in this case)\nThere are utilities for timing code:\n@s.time()\ndef f():\n  pass\n\nwith s.time():\n  pass\nThe Python client doesn't store or expose quantile information at this time.\nHistogram\nHistograms track the size and number of events in buckets.\nThis allows for aggregatable calculation of quantiles.\nfrom prometheus_client import Histogram\nh = Histogram('request_latency_seconds', 'Description of histogram')\nh.observe(4.7)    # Observe 4.7 (seconds in this case)\nThe default buckets are intended to cover a typical web/rpc request from milliseconds to seconds.\nThey can be overridden by passing buckets keyword argument to Histogram.\nThere are utilities for timing code:\n@h.time()\ndef f():\n  pass\n\nwith h.time():\n  pass\nInfo\nInfo tracks key-value information, usually about a whole target.\nfrom prometheus_client import Info\ni = Info('my_build_version', 'Description of info')\ni.info({'version': '1.2.3', 'buildhost': 'foo@bar'})\nEnum\nEnum tracks which of a set of states something is currently in.\nfrom prometheus_client import Enum\ne = Enum('my_task_state', 'Description of enum',\n        states=['starting', 'running', 'stopped'])\ne.state('running')\nLabels\nAll metrics can have labels, allowing grouping of related time series.\nSee the best practices on naming\nand labels.\nTaking a counter as an example:\nfrom prometheus_client import Counter\nc = Counter('my_requests_total', 'HTTP Failures', ['method', 'endpoint'])\nc.labels('get', '/').inc()\nc.labels('post', '/submit').inc()\nLabels can also be passed as keyword-arguments:\nfrom prometheus_client import Counter\nc = Counter('my_requests_total', 'HTTP Failures', ['method', 'endpoint'])\nc.labels(method='get', endpoint='/').inc()\nc.labels(method='post', endpoint='/submit').inc()\nMetrics with labels are not initialized when declared, because the client can't\nknow what values the label can have. It is recommended to initialize the label\nvalues by calling the .labels() method alone:\nfrom prometheus_client import Counter\nc = Counter('my_requests_total', 'HTTP Failures', ['method', 'endpoint'])\nc.labels('get', '/')\nc.labels('post', '/submit')\nExemplars\nExemplars can be added to counter and histogram metrics. Exemplars can be\nspecified by passing a dict of label value pairs to be exposed as the exemplar.\nFor example with a counter:\nfrom prometheus_client import Counter\nc = Counter('my_requests_total', 'HTTP Failures', ['method', 'endpoint'])\nc.labels('get', '/').inc(exemplar={'trace_id': 'abc123'})\nc.labels('post', '/submit').inc(1.0, {'trace_id': 'def456'})\nAnd with a histogram:\nfrom prometheus_client import Histogram\nh = Histogram('request_latency_seconds', 'Description of histogram')\nh.observe(4.7, {'trace_id': 'abc123'})\nExemplars are only rendered in the OpenMetrics exposition format. If using the\nHTTP server or apps in this library, content negotiation can be used to specify\nOpenMetrics (which is done by default in Prometheus). Otherwise it will be\nnecessary to use generate_latest from\nprometheus_client.openmetrics.exposition to view exemplars.\nTo view exemplars in Prometheus it is also necessary to enable the the\nexemplar-storage feature flag:\n--enable-feature=exemplar-storage\n\nAdditional information is available in the Prometheus\ndocumentation.\nDisabling _created metrics\nBy default counters, histograms, and summaries export an additional series\nsuffixed with _created and a value of the unix timestamp for when the metric\nwas created. If this information is not helpful, it can be disabled by setting\nthe environment variable PROMETHEUS_DISABLE_CREATED_SERIES=True.\nProcess Collector\nThe Python client automatically exports metrics about process CPU usage, RAM,\nfile descriptors and start time. These all have the prefix process, and\nare only currently available on Linux.\nThe namespace and pid constructor arguments allows for exporting metrics about\nother processes, for example:\nProcessCollector(namespace='mydaemon', pid=lambda: open('/var/run/daemon.pid').read())\n\nPlatform Collector\nThe client also automatically exports some metadata about Python. If using Jython,\nmetadata about the JVM in use is also included. This information is available as\nlabels on the python_info metric. The value of the metric is 1, since it is the\nlabels that carry information.\nDisabling Default Collector metrics\nBy default the collected process, gc, and platform collector metrics are exported.\nIf this information is not helpful, it can be disabled using the following:\nimport prometheus_client\n\nprometheus_client.REGISTRY.unregister(prometheus_client.GC_COLLECTOR)\nprometheus_client.REGISTRY.unregister(prometheus_client.PLATFORM_COLLECTOR)\nprometheus_client.REGISTRY.unregister(prometheus_client.PROCESS_COLLECTOR)\nExporting\nThere are several options for exporting metrics.\nHTTP\nMetrics are usually exposed over HTTP, to be read by the Prometheus server.\nThe easiest way to do this is via start_http_server, which will start a HTTP\nserver in a daemon thread on the given port:\nfrom prometheus_client import start_http_server\n\nstart_http_server(8000)\nVisit http://localhost:8000/ to view the metrics.\nTo add Prometheus exposition to an existing HTTP server, see the MetricsHandler class\nwhich provides a BaseHTTPRequestHandler. It also serves as a simple example of how\nto write a custom endpoint.\nTwisted\nTo use prometheus with twisted, there is MetricsResource which exposes metrics as a twisted resource.\nfrom prometheus_client.twisted import MetricsResource\nfrom twisted.web.server import Site\nfrom twisted.web.resource import Resource\nfrom twisted.internet import reactor\n\nroot = Resource()\nroot.putChild(b'metrics', MetricsResource())\n\nfactory = Site(root)\nreactor.listenTCP(8000, factory)\nreactor.run()\nWSGI\nTo use Prometheus with WSGI, there is\nmake_wsgi_app which creates a WSGI application.\nfrom prometheus_client import make_wsgi_app\nfrom wsgiref.simple_server import make_server\n\napp = make_wsgi_app()\nhttpd = make_server('', 8000, app)\nhttpd.serve_forever()\nSuch an application can be useful when integrating Prometheus metrics with WSGI\napps.\nThe method start_wsgi_server can be used to serve the metrics through the\nWSGI reference implementation in a new thread.\nfrom prometheus_client import start_wsgi_server\n\nstart_wsgi_server(8000)\nBy default, the WSGI application will respect Accept-Encoding:gzip headers used by Prometheus\nand compress the response if such a header is present. This behaviour can be disabled by passing\ndisable_compression=True when creating the app, like this:\napp = make_wsgi_app(disable_compression=True)\nASGI\nTo use Prometheus with ASGI, there is\nmake_asgi_app which creates an ASGI application.\nfrom prometheus_client import make_asgi_app\n\napp = make_asgi_app()\nSuch an application can be useful when integrating Prometheus metrics with ASGI\napps.\nBy default, the WSGI application will respect Accept-Encoding:gzip headers used by Prometheus\nand compress the response if such a header is present. This behaviour can be disabled by passing\ndisable_compression=True when creating the app, like this:\napp = make_asgi_app(disable_compression=True)\nFlask\nTo use Prometheus with Flask we need to serve metrics through a Prometheus WSGI application. This can be achieved using Flask's application dispatching. Below is a working example.\nSave the snippet below in a myapp.py file\nfrom flask import Flask\nfrom werkzeug.middleware.dispatcher import DispatcherMiddleware\nfrom prometheus_client import make_wsgi_app\n\n# Create my app\napp = Flask(__name__)\n\n# Add prometheus wsgi middleware to route /metrics requests\napp.wsgi_app = DispatcherMiddleware(app.wsgi_app, {\n    '/metrics': make_wsgi_app()\n})\nRun the example web application like this\n# Install uwsgi if you do not have it\npip install uwsgi\nuwsgi --http 127.0.0.1:8000 --wsgi-file myapp.py --callable app\nVisit http://localhost:8000/metrics to see the metrics\nFastAPI + Gunicorn\nTo use Prometheus with FastAPI and Gunicorn we need to serve metrics through a Prometheus ASGI application.\nSave the snippet below in a myapp.py file\nfrom fastapi import FastAPI\nfrom prometheus_client import make_asgi_app\n\n# Create app\napp = FastAPI(debug=False)\n\n# Add prometheus asgi middleware to route /metrics requests\nmetrics_app = make_asgi_app()\napp.mount(\"/metrics\", metrics_app)\nFor Multiprocessing support, use this modified code snippet. Full multiprocessing instructions are provided here.\nfrom fastapi import FastAPI\nfrom prometheus_client import make_asgi_app\n\napp = FastAPI(debug=False)\n\n# Using multiprocess collector for registry\ndef make_metrics_app():\n    registry = CollectorRegistry()\n    multiprocess.MultiProcessCollector(registry)\n    return make_asgi_app(registry=registry)\n\n\nmetrics_app = make_metrics_app()\napp.mount(\"/metrics\", metrics_app)\nRun the example web application like this\n# Install gunicorn if you do not have it\npip install gunicorn\n# If using multiple workers, add `--workers n` parameter to the line below\ngunicorn -b 127.0.0.1:8000 myapp:app -k uvicorn.workers.UvicornWorker\nVisit http://localhost:8000/metrics to see the metrics\nNode exporter textfile collector\nThe textfile collector\nallows machine-level statistics to be exported out via the Node exporter.\nThis is useful for monitoring cronjobs, or for writing cronjobs to expose metrics\nabout a machine system that the Node exporter does not support or would not make sense\nto perform at every scrape (for example, anything involving subprocesses).\nfrom prometheus_client import CollectorRegistry, Gauge, write_to_textfile\n\nregistry = CollectorRegistry()\ng = Gauge('raid_status', '1 if raid array is okay', registry=registry)\ng.set(1)\nwrite_to_textfile('/configured/textfile/path/raid.prom', registry)\nA separate registry is used, as the default registry may contain other metrics\nsuch as those from the Process Collector.\nExporting to a Pushgateway\nThe Pushgateway\nallows ephemeral and batch jobs to expose their metrics to Prometheus.\nfrom prometheus_client import CollectorRegistry, Gauge, push_to_gateway\n\nregistry = CollectorRegistry()\ng = Gauge('job_last_success_unixtime', 'Last time a batch job successfully finished', registry=registry)\ng.set_to_current_time()\npush_to_gateway('localhost:9091', job='batchA', registry=registry)\nA separate registry is used, as the default registry may contain other metrics\nsuch as those from the Process Collector.\nPushgateway functions take a grouping key. push_to_gateway replaces metrics\nwith the same grouping key, pushadd_to_gateway only replaces metrics with the\nsame name and grouping key and delete_from_gateway deletes metrics with the\ngiven job and grouping key. See the\nPushgateway documentation\nfor more information.\ninstance_ip_grouping_key returns a grouping key with the instance label set\nto the host's IP address.\nHandlers for authentication\nIf the push gateway you are connecting to is protected with HTTP Basic Auth,\nyou can use a special handler to set the Authorization header.\nfrom prometheus_client import CollectorRegistry, Gauge, push_to_gateway\nfrom prometheus_client.exposition import basic_auth_handler\n\ndef my_auth_handler(url, method, timeout, headers, data):\n    username = 'foobar'\n    password = 'secret123'\n    return basic_auth_handler(url, method, timeout, headers, data, username, password)\nregistry = CollectorRegistry()\ng = Gauge('job_last_success_unixtime', 'Last time a batch job successfully finished', registry=registry)\ng.set_to_current_time()\npush_to_gateway('localhost:9091', job='batchA', registry=registry, handler=my_auth_handler)\nTLS Auth is also supported when using the push gateway with a special handler.\nfrom prometheus_client import CollectorRegistry, Gauge, push_to_gateway\nfrom prometheus_client.exposition import tls_auth_handler\n\n\ndef my_auth_handler(url, method, timeout, headers, data):\n    certfile = 'client-crt.pem'\n    keyfile = 'client-key.pem'\n    return tls_auth_handler(url, method, timeout, headers, data, certfile, keyfile)\n\nregistry = CollectorRegistry()\ng = Gauge('job_last_success_unixtime', 'Last time a batch job successfully finished', registry=registry)\ng.set_to_current_time()\npush_to_gateway('localhost:9091', job='batchA', registry=registry, handler=my_auth_handler)\nBridges\nIt is also possible to expose metrics to systems other than Prometheus.\nThis allows you to take advantage of Prometheus instrumentation even\nif you are not quite ready to fully transition to Prometheus yet.\nGraphite\nMetrics are pushed over TCP in the Graphite plaintext format.\nfrom prometheus_client.bridge.graphite import GraphiteBridge\n\ngb = GraphiteBridge(('graphite.your.org', 2003))\n# Push once.\ngb.push()\n# Push every 10 seconds in a daemon thread.\ngb.start(10.0)\nGraphite tags are also supported.\nfrom prometheus_client.bridge.graphite import GraphiteBridge\n\ngb = GraphiteBridge(('graphite.your.org', 2003), tags=True)\nc = Counter('my_requests_total', 'HTTP Failures', ['method', 'endpoint'])\nc.labels('get', '/').inc()\ngb.push()\nCustom Collectors\nSometimes it is not possible to directly instrument code, as it is not\nin your control. This requires you to proxy metrics from other systems.\nTo do so you need to create a custom collector, for example:\nfrom prometheus_client.core import GaugeMetricFamily, CounterMetricFamily, REGISTRY\n\nclass CustomCollector(object):\n    def collect(self):\n        yield GaugeMetricFamily('my_gauge', 'Help text', value=7)\n        c = CounterMetricFamily('my_counter_total', 'Help text', labels=['foo'])\n        c.add_metric(['bar'], 1.7)\n        c.add_metric(['baz'], 3.8)\n        yield c\n\nREGISTRY.register(CustomCollector())\nSummaryMetricFamily, HistogramMetricFamily and InfoMetricFamily work similarly.\nA collector may implement a describe method which returns metrics in the same\nformat as collect (though you don't have to include the samples). This is\nused to predetermine the names of time series a CollectorRegistry exposes and\nthus to detect collisions and duplicate registrations.\nUsually custom collectors do not have to implement describe. If describe is\nnot implemented and the CollectorRegistry was created with auto_describe=True\n(which is the case for the default registry) then collect will be called at\nregistration time instead of describe. If this could cause problems, either\nimplement a proper describe, or if that's not practical have describe\nreturn an empty list.\nMultiprocess Mode (E.g. Gunicorn)\nPrometheus client libraries presume a threaded model, where metrics are shared\nacross workers. This doesn't work so well for languages such as Python where\nit's common to have processes rather than threads to handle large workloads.\nTo handle this the client library can be put in multiprocess mode.\nThis comes with a number of limitations:\n\nRegistries can not be used as normal, all instantiated metrics are exported\n\nRegistering metrics to a registry later used by a MultiProcessCollector\nmay cause duplicate metrics to be exported\n\n\nCustom collectors do not work (e.g. cpu and memory metrics)\nInfo and Enum metrics do not work\nThe pushgateway cannot be used\nGauges cannot use the pid label\nExemplars are not supported\n\nThere's several steps to getting this working:\n1. Deployment:\nThe PROMETHEUS_MULTIPROC_DIR environment variable must be set to a directory\nthat the client library can use for metrics. This directory must be wiped\nbetween process/Gunicorn runs (before startup is recommended).\nThis environment variable should be set from a start-up shell script,\nand not directly from Python (otherwise it may not propagate to child processes).\n2. Metrics collector:\nThe application must initialize a new CollectorRegistry, and store the\nmulti-process collector inside. It is a best practice to create this registry\ninside the context of a request to avoid metrics registering themselves to a\ncollector used by a MultiProcessCollector. If a registry with metrics\nregistered is used by a MultiProcessCollector duplicate metrics may be\nexported, one for multiprocess, and one for the process serving the request.\nfrom prometheus_client import multiprocess\nfrom prometheus_client import generate_latest, CollectorRegistry, CONTENT_TYPE_LATEST, Counter\n\nMY_COUNTER = Counter('my_counter', 'Description of my counter')\n\n# Expose metrics.\ndef app(environ, start_response):\n    registry = CollectorRegistry()\n    multiprocess.MultiProcessCollector(registry)\n    data = generate_latest(registry)\n    status = '200 OK'\n    response_headers = [\n        ('Content-type', CONTENT_TYPE_LATEST),\n        ('Content-Length', str(len(data)))\n    ]\n    start_response(status, response_headers)\n    return iter([data])\n3. Gunicorn configuration:\nThe gunicorn configuration file needs to include the following function:\nfrom prometheus_client import multiprocess\n\ndef child_exit(server, worker):\n    multiprocess.mark_process_dead(worker.pid)\n4. Metrics tuning (Gauge):\nWhen Gauges are used in multiprocess applications,\nyou must decide how to handle the metrics reported by each process.\nGauges have several modes they can run in, which can be selected with the multiprocess_mode parameter.\n\n'all': Default. Return a timeseries per process (alive or dead), labelled by the process's pid (the label is added internally).\n'min': Return a single timeseries that is the minimum of the values of all processes (alive or dead).\n'max': Return a single timeseries that is the maximum of the values of all processes (alive or dead).\n'sum': Return a single timeseries that is the sum of the values of all processes (alive or dead).\n\nPrepend 'live' to the beginning of the mode to return the same result but only considering living processes\n(e.g., 'liveall, 'livesum', 'livemax', 'livemin').\nfrom prometheus_client import Gauge\n\n# Example gauge\nIN_PROGRESS = Gauge(\"inprogress_requests\", \"help\", multiprocess_mode='livesum')\nParser\nThe Python client supports parsing the Prometheus text format.\nThis is intended for advanced use cases where you have servers\nexposing Prometheus metrics and need to get them into some other\nsystem.\nfrom prometheus_client.parser import text_string_to_metric_families\nfor family in text_string_to_metric_families(u\"my_gauge 1.0\\n\"):\n  for sample in family.samples:\n    print(\"Name: {0} Labels: {1} Value: {2}\".format(*sample))\nLinks\n\nReleases: The releases page shows the history of the project and acts as a changelog.\nPyPI\n\n\n\n"}, {"name": "pkgutil-resolve-name", "readme": "\n\n\n\nREADME.rst\n\n\n\n\npkgutil-resolve-name\nA backport of Python 3.9's pkgutil.resolve_name.\nSee the Python 3.9 documentation.\n\n\n"}, {"name": "oscrypto", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noscrypto\nSupported Operating Systems\nFeatures\nWhy Another Python Crypto Library?\nRelated Crypto Libraries\nCurrent Release\nDependencies\nInstallation\nLicense\nDocumentation\nContinuous Integration\nTesting\nGit Repository\nBackend Options\nInternet Tests\nPyPi Source Distribution\nTest Options\nForce OpenSSL Shared Library Paths\nForce Use of ctypes\nForce Use of Legacy Windows Crypto APIs\nSkip Tests Requiring an Internet Connection\nPackage\nDevelopment\nCI Tasks\n\n\n\n\n\nreadme.md\n\n\n\n\noscrypto\nA compilation-free, always up-to-date encryption library for Python that works\non Windows, OS X, Linux and BSD. Supports the following versions of Python:\n2.6, 2.7, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 3.10 and pypy.\n\nSupported Operating Systems\nFeatures\nWhy Another Python Crypto Library?\nRelated Crypto Libraries\nCurrent Release\nDependencies\nInstallation\nLicense\nDocumentation\nContinuous Integration\nTesting\nDevelopment\nCI Tasks\n\n\n\n\nSupported Operating Systems\nThe library integrates with the encryption library that is part of the operating\nsystem. This means that a compiler is never needed, and OS security updates take\ncare of patching vulnerabilities. Supported operating systems include:\n\nWindows XP or newer\n\nUses:\n\nCryptography API: Next Generation (CNG)\nSecure Channel for TLS\nCryptoAPI for trust lists and XP support\n\n\nTested on:\n\nWindows XP (no SNI)\nWindows 7\nWindows 8.1\nWindows Server 2012\nWindows 10\n\n\n\n\nOS X 10.7 or newer\n\nUses:\n\nSecurity.framework\nSecure Transport for TLS\nCommonCrypto for PBKDF2\nOpenSSL (or LibreSSL on macOS 10.13) for the PKCS #12 KDF\n\n\nTested on:\n\nOS X 10.7\nOS X 10.8\nOS X 10.9\nOS X 10.10\nOS X 10.11\nOS X 10.11 with OpenSSL 1.1.0\nmacOS 10.12\nmacOS 10.13 with LibreSSL 2.2.7\nmacOS 10.14\nmacOS 10.15\nmacOS 10.15 with OpenSSL 3.0\nmacOS 11\nmacOS 12\n\n\n\n\nLinux or BSD\n\nUses one of:\n\nOpenSSL 0.9.8\nOpenSSL 1.0.x\nOpenSSL 1.1.0\nOpenSSL 3.0\nLibreSSL\n\n\nTested on:\n\nArch Linux with OpenSSL 1.0.2\nOpenBSD 5.7 with LibreSSL\nUbuntu 10.04 with OpenSSL 0.9.8\nUbuntu 12.04 with OpenSSL 1.0.1\nUbuntu 15.04 with OpenSSL 1.0.1\nUbuntu 16.04 with OpenSSL 1.0.2 on Raspberry Pi 3 (armhf)\nUbuntu 18.04 with OpenSSL 1.1.x (amd64, arm64, ppc64el)\nUbuntu 22.04 with OpenSSL 3.0 (amd64)\n\n\n\n\n\nOS X 10.6 will not be supported due to a lack of available\ncryptographic primitives and due to lack of vendor support.\nFeatures\nCurrently the following features are implemented. Many of these should only be\nused for integration with existing/legacy systems. If you don't know which you\nshould, or should not use, please see Learning.\n\nTLSv1.x socket wrappers\n\nCertificate verification performed by OS trust roots\nCustom CA certificate support\nSNI support (except Windows XP)\nSession reuse via IDs/tickets\nModern cipher suites (RC4, DES, anon and NULL ciphers disabled)\nWeak DH parameters and certificate signatures rejected\nSSLv3 disabled by default, SSLv2 unimplemented\nCRL/OCSP revocation checks consistenty disabled\n\n\nExporting OS trust roots\n\nPEM-formatted CA certs from the OS for OpenSSL-based code\n\n\nEncryption/decryption\n\nAES (128, 192, 256), CBC mode, PKCS7 padding\nAES (128, 192, 256), CBC mode, no padding\nTripleDES 3-key, CBC mode, PKCS5 padding\nTripleDes 2-key, CBC mode, PKCS5 padding\nDES, CBC mode, PKCS5 padding\nRC2 (40-128), CBC mode, PKCS5 padding\nRC4 (40-128)\nRSA PKCSv1.5\nRSA OAEP (SHA1 only)\n\n\nGenerating public/private key pairs\n\nRSA (1024, 2048, 3072, 4096 bit)\nDSA (1024 bit on all platforms - 2048, 3072 bit with OpenSSL 1.x or\nWindows 8)\nEC (secp256r1, secp384r1, secp521r1 curves)\n\n\nGenerating DH parameters\nSigning and verification\n\nRSA PKCSv1.5\nRSA PSS\nDSA\nEC\n\n\nLoading and normalizing DER and PEM formatted keys\n\nRSA public and private keys\nDSA public and private keys\nEC public and private keys\nX.509 Certificates\nPKCS#12 archives (.pfx/.p12)\n\n\nKey derivation\n\nPBKDF2\nPBKDF1\nPKCS#12 KDF\n\n\nRandom byte generation\n\nThe feature set was largely driven by the technologies used related to\ngenerating and validating X.509 certificates. The various CBC encryption schemes\nand KDFs are used to load encrypted private keys, and the various RSA padding\nschemes are part of X.509 signatures.\nFor modern cryptography not tied to an existing system, please see the\nModern Cryptography section of the docs.\nPlease note that this library does not include modern block modes such as CTR\nand GCM due to lack of support from both OS X and OpenSSL 0.9.8.\nWhy Another Python Crypto Library?\nIn short, the existing cryptography libraries for Python didn't fit the needs of\na couple of projects I was working on. Primarily these are applications\ndistributed to end-users who aren't programmers, that need to handle TLS and\nvarious technologies related to X.509 certificates.\nIf your system is not tied to AES, TLS, X.509, or related technologies, you\nprobably want more modern cryptography.\nDepending on your needs, the cryptography package may\nbe a good (or better) fit.\nSome things that make oscrypto unique:\n\nNo compiler needed, ever. No need to pre-compile shared libraries. Just\ndistribute the Python source files, any way you want.\nUses the operating system's crypto library - does not require OpenSSL on\nWindows or OS X.\nRelies on the operating system for security patching. You don't need to\nrebuild all of your apps every time there is a new TLS vulnerability.\nIntentionally limited in scope to crypto primitives. Other libraries\nbuilt upon it deal with certificate path validation, creating certificates\nand CSRs, constructing CMS structures.\nBuilt on top of a fast, pure-Python ASN.1 parser,\nasn1crypto.\nTLS functionality uses the operating system's trust list/CA certs and is\npre-configured with sane defaults\nPublic APIs are simple and use strict type checks to avoid errors\n\nSome downsides include:\n\nDoes not currently implement:\n\nstandalone DH key exchange\nvarious encryption modes such as GCM, CCM, CTR, CFB, OFB, ECB\nkey wrapping\nCMAC\nHKDF\n\n\nNon-TLS functionality is architected for dealing with data that fits in\nmemory and is available all at once\nDeveloped by a single developer\n\nRelated Crypto Libraries\noscrypto is part of the modularcrypto family of Python packages:\n\nasn1crypto\noscrypto\ncsrbuilder\ncertbuilder\ncrlbuilder\nocspbuilder\ncertvalidator\n\nCurrent Release\n1.3.0 - changelog\nDependencies\n\nasn1crypto\nPython 2.6, 2.7, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 3.10, 3.11 or pypy\nOpenSSL/LibreSSL if on Linux\u00b9\n\n\u00b9 On Linux, ctypes.util.find_library() is used to located OpenSSL. Alpine Linux does not have an appropriate install by default for find_library() to work properly. Instead, oscrypto.use_openssl() must be called with the path to the OpenSSL shared libraries.\nInstallation\npip install oscrypto\nLicense\noscrypto is licensed under the terms of the MIT license. See the\nLICENSE file for the exact license text.\nDocumentation\noscrypto documentation\nContinuous Integration\nVarious combinations of platforms and versions of Python are tested via:\n\nmacOS, Linux, Windows via GitHub Actions\narm64 via CircleCI\n\nTesting\nTests are written using unittest and require no third-party packages.\nDepending on what type of source is available for the package, the following\ncommands can be used to run the test suite.\nGit Repository\nWhen working within a Git working copy, or an archive of the Git repository,\nthe full test suite is run via:\npython run.py tests\nTo run only some tests, pass a regular expression as a parameter to tests.\npython run.py tests aes\nTo run tests multiple times, in order to catch edge-case bugs, pass an integer\nto tests. If combined with a regular expression for filtering, pass the\nrepeat count after the regular expression.\npython run.py tests 20\npython run.py tests aes 20\nBackend Options\nTo run tests using a custom build of OpenSSL, or to use OpenSSL on Windows or\nMac, add use_openssl after run.py, like:\npython run.py use_openssl=/path/to/libcrypto.so,/path/to/libssl.so tests\nTo run tests forcing the use of ctypes, even if cffi is installed, add\nuse_ctypes after run.py:\npython run.py use_ctypes=true tests\nTo run tests using the legacy Windows crypto functions on Windows 7+, add\nuse_winlegacy after run.py:\npython run.py use_winlegacy=true tests\nInternet Tests\nTo skip tests that require an internet connection, add skip_internet after\nrun.py:\npython run.py skip_internet=true tests\nPyPi Source Distribution\nWhen working within an extracted source distribution (aka .tar.gz) from\nPyPi, the full test suite is run via:\npython setup.py test\nTest Options\nThe following env vars can control aspects of running tests:\nForce OpenSSL Shared Library Paths\nSetting the env var OSCRYPTO_USE_OPENSSL to a string in the form:\n/path/to/libcrypto.so,/path/to/libssl.so\n\nwill force use of specific OpenSSL shared libraries.\nThis also works on Mac and Windows to force use of OpenSSL instead of using\nnative crypto libraries.\nForce Use of ctypes\nBy default, oscrypto will use the cffi module for FFI if it is installed.\nTo use the slightly slower, but more widely-tested, ctypes FFI layer, set\nthe env var OSCRYPTO_USE_CTYPES=true.\nForce Use of Legacy Windows Crypto APIs\nOn Windows 7 and newer, oscrypto will use the CNG backend by default.\nTo force use of the older CryptoAPI, set the env var\nOSCRYPTO_USE_WINLEGACY=true.\nSkip Tests Requiring an Internet Connection\nSome of the TLS tests require an active internet connection to ensure that\nvarious \"bad\" server certificates are rejected.\nTo skip tests requiring an internet connection, set the env var\nOSCRYPTO_SKIP_INTERNET_TESTS=true.\nPackage\nWhen the package has been installed via pip (or another method), the package\noscrypto_tests may be installed and invoked to run the full test suite:\npip install oscrypto_tests\npython -m oscrypto_tests\nDevelopment\nTo install the package used for linting, execute:\npip install --user -r requires/lint\nThe following command will run the linter:\npython run.py lint\nSupport for code coverage can be installed via:\npip install --user -r requires/coverage\nCoverage is measured by running:\npython run.py coverage\nTo install the packages requires to generate the API documentation, run:\npip install --user -r requires/api_docs\nThe documentation can then be generated by running:\npython run.py api_docs\nTo install the necessary packages for releasing a new version on PyPI, run:\npip install --user -r requires/release\nReleases are created by:\n\n\nMaking a git tag in semver format\n\n\nRunning the command:\npython run.py release\n\n\nExisting releases can be found at https://pypi.python.org/pypi/oscrypto.\nCI Tasks\nA task named deps exists to download and stage all necessary testing\ndependencies. On posix platforms, curl is used for downloads and on Windows\nPowerShell with Net.WebClient is used. This configuration sidesteps issues\nrelated to getting pip to work properly and messing with site-packages for\nthe version of Python being used.\nThe ci task runs lint (if flake8 is available for the version of Python) and\ncoverage (or tests if coverage is not available for the version of Python).\nIf the current directory is a clean git working copy, the coverage data is\nsubmitted to codecov.io.\npython run.py deps\npython run.py ci\n\n\n"}, {"name": "orjson", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norjson\nUsage\nInstall\nQuickstart\nMigrating\nSerialize\ndefault\noption\nOPT_APPEND_NEWLINE\nOPT_INDENT_2\nOPT_NAIVE_UTC\nOPT_NON_STR_KEYS\nOPT_OMIT_MICROSECONDS\nOPT_PASSTHROUGH_DATACLASS\nOPT_PASSTHROUGH_DATETIME\nOPT_PASSTHROUGH_SUBCLASS\nOPT_SERIALIZE_DATACLASS\nOPT_SERIALIZE_NUMPY\nOPT_SERIALIZE_UUID\nOPT_SORT_KEYS\nOPT_STRICT_INTEGER\nOPT_UTC_Z\nFragment\nDeserialize\nTypes\ndataclass\ndatetime\nenum\nfloat\nint\nnumpy\nstr\nuuid\nTesting\nPerformance\nLatency\ntwitter.json serialization\ntwitter.json deserialization\ngithub.json serialization\ngithub.json deserialization\ncitm_catalog.json serialization\ncitm_catalog.json deserialization\ncanada.json serialization\ncanada.json deserialization\nMemory\ntwitter.json\ngithub.json\ncitm_catalog.json\ncanada.json\nReproducing\nQuestions\nWhy can't I install it from PyPI?\n\"Cargo, the Rust package manager, is not installed or is not on PATH.\"\nWill it deserialize to dataclasses, UUIDs, decimals, etc or support object_hook?\nWill it serialize to str?\nWill it support PyPy?\nPackaging\nLicense\n\n\n\n\n\nREADME.md\n\n\n\n\norjson\norjson is a fast, correct JSON library for Python. It\nbenchmarks as the fastest Python\nlibrary for JSON and is more correct than the standard json library or other\nthird-party libraries. It serializes\ndataclass,\ndatetime,\nnumpy, and\nUUID instances natively.\nIts features and drawbacks compared to other Python JSON libraries:\n\nserializes dataclass instances 40-50x as fast as other libraries\nserializes datetime, date, and time instances to RFC 3339 format,\ne.g., \"1970-01-01T00:00:00+00:00\"\nserializes numpy.ndarray instances 4-12x as fast with 0.3x the memory\nusage of other libraries\npretty prints 10x to 20x as fast as the standard library\nserializes to bytes rather than str, i.e., is not a drop-in replacement\nserializes str without escaping unicode to ASCII, e.g., \"\u597d\" rather than\n\"\\\\u597d\"\nserializes float 10x as fast and deserializes twice as fast as other\nlibraries\nserializes subclasses of str, int, list, and dict natively,\nrequiring default to specify how to serialize others\nserializes arbitrary types using a default hook\nhas strict UTF-8 conformance, more correct than the standard library\nhas strict JSON conformance in not supporting Nan/Infinity/-Infinity\nhas an option for strict JSON conformance on 53-bit integers with default\nsupport for 64-bit\ndoes not provide load() or dump() functions for reading from/writing to\nfile-like objects\n\norjson supports CPython 3.7, 3.8, 3.9, 3.10, 3.11, and 3.12. It distributes\namd64/x86_64, aarch64/armv8, arm7, POWER/ppc64le, and s390x wheels for Linux,\namd64 and aarch64 wheels for macOS, and amd64 and i686/x86 wheels for Windows.\norjson  does not support PyPy. Releases follow semantic versioning and\nserializing a new object type without an opt-in flag is considered a\nbreaking change.\norjson is licensed under both the Apache 2.0 and MIT licenses. The\nrepository and issue tracker is\ngithub.com/ijl/orjson, and patches may be\nsubmitted there. There is a\nCHANGELOG\navailable in the repository.\n\nUsage\n\nInstall\nQuickstart\nMigrating\nSerialize\n\ndefault\noption\nFragment\n\n\nDeserialize\n\n\nTypes\n\ndataclass\ndatetime\nenum\nfloat\nint\nnumpy\nstr\nuuid\n\n\nTesting\nPerformance\n\nLatency\nMemory\nReproducing\n\n\nQuestions\nPackaging\nLicense\n\nUsage\nInstall\nTo install a wheel from PyPI:\npip install --upgrade \"pip>=20.3\" # manylinux_x_y, universal2 wheel support\npip install --upgrade orjson\nTo build a wheel, see packaging.\nQuickstart\nThis is an example of serializing, with options specified, and deserializing:\n>>> import orjson, datetime, numpy\n>>> data = {\n    \"type\": \"job\",\n    \"created_at\": datetime.datetime(1970, 1, 1),\n    \"status\": \"\ud83c\udd97\",\n    \"payload\": numpy.array([[1, 2], [3, 4]]),\n}\n>>> orjson.dumps(data, option=orjson.OPT_NAIVE_UTC | orjson.OPT_SERIALIZE_NUMPY)\nb'{\"type\":\"job\",\"created_at\":\"1970-01-01T00:00:00+00:00\",\"status\":\"\\xf0\\x9f\\x86\\x97\",\"payload\":[[1,2],[3,4]]}'\n>>> orjson.loads(_)\n{'type': 'job', 'created_at': '1970-01-01T00:00:00+00:00', 'status': '\ud83c\udd97', 'payload': [[1, 2], [3, 4]]}\nMigrating\norjson version 3 serializes more types than version 2. Subclasses of str,\nint, dict, and list are now serialized. This is faster and more similar\nto the standard library. It can be disabled with\norjson.OPT_PASSTHROUGH_SUBCLASS.dataclasses.dataclass instances\nare now serialized by default and cannot be customized in a\ndefault function unless option=orjson.OPT_PASSTHROUGH_DATACLASS is\nspecified. uuid.UUID instances are serialized by default.\nFor any type that is now serialized,\nimplementations in a default function and options enabling them can be\nremoved but do not need to be. There was no change in deserialization.\nTo migrate from the standard library, the largest difference is that\norjson.dumps returns bytes and json.dumps returns a str. Users with\ndict objects using non-str keys should specify\noption=orjson.OPT_NON_STR_KEYS. sort_keys is replaced by\noption=orjson.OPT_SORT_KEYS. indent is replaced by\noption=orjson.OPT_INDENT_2 and other levels of indentation are not\nsupported.\nSerialize\ndef dumps(\n    __obj: Any,\n    default: Optional[Callable[[Any], Any]] = ...,\n    option: Optional[int] = ...,\n) -> bytes: ...\ndumps() serializes Python objects to JSON.\nIt natively serializes\nstr, dict, list, tuple, int, float, bool, None,\ndataclasses.dataclass, typing.TypedDict, datetime.datetime,\ndatetime.date, datetime.time, uuid.UUID, numpy.ndarray, and\norjson.Fragment instances. It supports arbitrary types through default. It\nserializes subclasses of str, int, dict, list,\ndataclasses.dataclass, and enum.Enum. It does not serialize subclasses\nof tuple to avoid serializing namedtuple objects as arrays. To avoid\nserializing subclasses, specify the option orjson.OPT_PASSTHROUGH_SUBCLASS.\nThe output is a bytes object containing UTF-8.\nThe global interpreter lock (GIL) is held for the duration of the call.\nIt raises JSONEncodeError on an unsupported type. This exception message\ndescribes the invalid object with the error message\nType is not JSON serializable: .... To fix this, specify\ndefault.\nIt raises JSONEncodeError on a str that contains invalid UTF-8.\nIt raises JSONEncodeError on an integer that exceeds 64 bits by default or,\nwith OPT_STRICT_INTEGER, 53 bits.\nIt raises JSONEncodeError if a dict has a key of a type other than str,\nunless OPT_NON_STR_KEYS is specified.\nIt raises JSONEncodeError if the output of default recurses to handling by\ndefault more than 254 levels deep.\nIt raises JSONEncodeError on circular references.\nIt raises JSONEncodeError  if a tzinfo on a datetime object is\nunsupported.\nJSONEncodeError is a subclass of TypeError. This is for compatibility\nwith the standard library.\nIf the failure was caused by an exception in default then\nJSONEncodeError chains the original exception as __cause__.\ndefault\nTo serialize a subclass or arbitrary types, specify default as a\ncallable that returns a supported type. default may be a function,\nlambda, or callable class instance. To specify that a type was not\nhandled by default, raise an exception such as TypeError.\n>>> import orjson, decimal\n>>>\ndef default(obj):\n    if isinstance(obj, decimal.Decimal):\n        return str(obj)\n    raise TypeError\n\n>>> orjson.dumps(decimal.Decimal(\"0.0842389659712649442845\"))\nJSONEncodeError: Type is not JSON serializable: decimal.Decimal\n>>> orjson.dumps(decimal.Decimal(\"0.0842389659712649442845\"), default=default)\nb'\"0.0842389659712649442845\"'\n>>> orjson.dumps({1, 2}, default=default)\norjson.JSONEncodeError: Type is not JSON serializable: set\nThe default callable may return an object that itself\nmust be handled by default up to 254 times before an exception\nis raised.\nIt is important that default raise an exception if a type cannot be handled.\nPython otherwise implicitly returns None, which appears to the caller\nlike a legitimate value and is serialized:\n>>> import orjson, json, rapidjson\n>>>\ndef default(obj):\n    if isinstance(obj, decimal.Decimal):\n        return str(obj)\n\n>>> orjson.dumps({\"set\":{1, 2}}, default=default)\nb'{\"set\":null}'\n>>> json.dumps({\"set\":{1, 2}}, default=default)\n'{\"set\":null}'\n>>> rapidjson.dumps({\"set\":{1, 2}}, default=default)\n'{\"set\":null}'\noption\nTo modify how data is serialized, specify option. Each option is an integer\nconstant in orjson. To specify multiple options, mask them together, e.g.,\noption=orjson.OPT_STRICT_INTEGER | orjson.OPT_NAIVE_UTC.\nOPT_APPEND_NEWLINE\nAppend \\n to the output. This is a convenience and optimization for the\npattern of dumps(...) + \"\\n\". bytes objects are immutable and this\npattern copies the original contents.\n>>> import orjson\n>>> orjson.dumps([])\nb\"[]\"\n>>> orjson.dumps([], option=orjson.OPT_APPEND_NEWLINE)\nb\"[]\\n\"\nOPT_INDENT_2\nPretty-print output with an indent of two spaces. This is equivalent to\nindent=2 in the standard library. Pretty printing is slower and the output\nlarger. orjson is the fastest compared library at pretty printing and has\nmuch less of a slowdown to pretty print than the standard library does. This\noption is compatible with all other options.\n>>> import orjson\n>>> orjson.dumps({\"a\": \"b\", \"c\": {\"d\": True}, \"e\": [1, 2]})\nb'{\"a\":\"b\",\"c\":{\"d\":true},\"e\":[1,2]}'\n>>> orjson.dumps(\n    {\"a\": \"b\", \"c\": {\"d\": True}, \"e\": [1, 2]},\n    option=orjson.OPT_INDENT_2\n)\nb'{\\n  \"a\": \"b\",\\n  \"c\": {\\n    \"d\": true\\n  },\\n  \"e\": [\\n    1,\\n    2\\n  ]\\n}'\nIf displayed, the indentation and linebreaks appear like this:\n{\n  \"a\": \"b\",\n  \"c\": {\n    \"d\": true\n  },\n  \"e\": [\n    1,\n    2\n  ]\n}\nThis measures serializing the github.json fixture as compact (52KiB) or\npretty (64KiB):\n\n\n\nLibrary\ncompact (ms)\npretty (ms)\nvs. orjson\n\n\n\n\norjson\n0.03\n0.04\n1\n\n\nujson\n0.18\n0.19\n4.6\n\n\nrapidjson\n0.1\n0.12\n2.9\n\n\nsimplejson\n0.25\n0.89\n21.4\n\n\njson\n0.18\n0.71\n17\n\n\n\nThis measures serializing the citm_catalog.json fixture, more of a worst\ncase due to the amount of nesting and newlines, as compact (489KiB) or\npretty (1.1MiB):\n\n\n\nLibrary\ncompact (ms)\npretty (ms)\nvs. orjson\n\n\n\n\norjson\n0.59\n0.71\n1\n\n\nujson\n2.9\n3.59\n5\n\n\nrapidjson\n1.81\n2.8\n3.9\n\n\nsimplejson\n10.43\n42.13\n59.1\n\n\njson\n4.16\n33.42\n46.9\n\n\n\nThis can be reproduced using the pyindent script.\nOPT_NAIVE_UTC\nSerialize datetime.datetime objects without a tzinfo as UTC. This\nhas no effect on datetime.datetime objects that have tzinfo set.\n>>> import orjson, datetime\n>>> orjson.dumps(\n        datetime.datetime(1970, 1, 1, 0, 0, 0),\n    )\nb'\"1970-01-01T00:00:00\"'\n>>> orjson.dumps(\n        datetime.datetime(1970, 1, 1, 0, 0, 0),\n        option=orjson.OPT_NAIVE_UTC,\n    )\nb'\"1970-01-01T00:00:00+00:00\"'\nOPT_NON_STR_KEYS\nSerialize dict keys of type other than str. This allows dict keys\nto be one of str, int, float, bool, None, datetime.datetime,\ndatetime.date, datetime.time, enum.Enum, and uuid.UUID. For comparison,\nthe standard library serializes str, int, float, bool or None by\ndefault. orjson benchmarks as being faster at serializing non-str keys\nthan other libraries. This option is slower for str keys than the default.\n>>> import orjson, datetime, uuid\n>>> orjson.dumps(\n        {uuid.UUID(\"7202d115-7ff3-4c81-a7c1-2a1f067b1ece\"): [1, 2, 3]},\n        option=orjson.OPT_NON_STR_KEYS,\n    )\nb'{\"7202d115-7ff3-4c81-a7c1-2a1f067b1ece\":[1,2,3]}'\n>>> orjson.dumps(\n        {datetime.datetime(1970, 1, 1, 0, 0, 0): [1, 2, 3]},\n        option=orjson.OPT_NON_STR_KEYS | orjson.OPT_NAIVE_UTC,\n    )\nb'{\"1970-01-01T00:00:00+00:00\":[1,2,3]}'\nThese types are generally serialized how they would be as\nvalues, e.g., datetime.datetime is still an RFC 3339 string and respects\noptions affecting it. The exception is that int serialization does not\nrespect OPT_STRICT_INTEGER.\nThis option has the risk of creating duplicate keys. This is because non-str\nobjects may serialize to the same str as an existing key, e.g.,\n{\"1\": true, 1: false}. The last key to be inserted to the dict will be\nserialized last and a JSON deserializer will presumably take the last\noccurrence of a key (in the above, false). The first value will be lost.\nThis option is compatible with orjson.OPT_SORT_KEYS. If sorting is used,\nnote the sort is unstable and will be unpredictable for duplicate keys.\n>>> import orjson, datetime\n>>> orjson.dumps(\n    {\"other\": 1, datetime.date(1970, 1, 5): 2, datetime.date(1970, 1, 3): 3},\n    option=orjson.OPT_NON_STR_KEYS | orjson.OPT_SORT_KEYS\n)\nb'{\"1970-01-03\":3,\"1970-01-05\":2,\"other\":1}'\nThis measures serializing 589KiB of JSON comprising a list of 100 dict\nin which each dict has both 365 randomly-sorted int keys representing epoch\ntimestamps as well as one str key and the value for each key is a\nsingle integer. In \"str keys\", the keys were converted to str before\nserialization, and orjson still specifes option=orjson.OPT_NON_STR_KEYS\n(which is always somewhat slower).\n\n\n\nLibrary\nstr keys (ms)\nint keys (ms)\nint keys sorted (ms)\n\n\n\n\norjson\n1.53\n2.16\n4.29\n\n\nujson\n3.07\n5.65\n\n\n\nrapidjson\n4.29\n\n\n\n\nsimplejson\n11.24\n14.50\n21.86\n\n\njson\n7.17\n8.49\n\n\n\n\nujson is blank for sorting because it segfaults. json is blank because it\nraises TypeError on attempting to sort before converting all keys to str.\nrapidjson is blank because it does not support non-str keys. This can\nbe reproduced using the pynonstr script.\nOPT_OMIT_MICROSECONDS\nDo not serialize the microsecond field on datetime.datetime and\ndatetime.time instances.\n>>> import orjson, datetime\n>>> orjson.dumps(\n        datetime.datetime(1970, 1, 1, 0, 0, 0, 1),\n    )\nb'\"1970-01-01T00:00:00.000001\"'\n>>> orjson.dumps(\n        datetime.datetime(1970, 1, 1, 0, 0, 0, 1),\n        option=orjson.OPT_OMIT_MICROSECONDS,\n    )\nb'\"1970-01-01T00:00:00\"'\nOPT_PASSTHROUGH_DATACLASS\nPassthrough dataclasses.dataclass instances to default. This allows\ncustomizing their output but is much slower.\n>>> import orjson, dataclasses\n>>>\n@dataclasses.dataclass\nclass User:\n    id: str\n    name: str\n    password: str\n\ndef default(obj):\n    if isinstance(obj, User):\n        return {\"id\": obj.id, \"name\": obj.name}\n    raise TypeError\n\n>>> orjson.dumps(User(\"3b1\", \"asd\", \"zxc\"))\nb'{\"id\":\"3b1\",\"name\":\"asd\",\"password\":\"zxc\"}'\n>>> orjson.dumps(User(\"3b1\", \"asd\", \"zxc\"), option=orjson.OPT_PASSTHROUGH_DATACLASS)\nTypeError: Type is not JSON serializable: User\n>>> orjson.dumps(\n        User(\"3b1\", \"asd\", \"zxc\"),\n        option=orjson.OPT_PASSTHROUGH_DATACLASS,\n        default=default,\n    )\nb'{\"id\":\"3b1\",\"name\":\"asd\"}'\nOPT_PASSTHROUGH_DATETIME\nPassthrough datetime.datetime, datetime.date, and datetime.time instances\nto default. This allows serializing datetimes to a custom format, e.g.,\nHTTP dates:\n>>> import orjson, datetime\n>>>\ndef default(obj):\n    if isinstance(obj, datetime.datetime):\n        return obj.strftime(\"%a, %d %b %Y %H:%M:%S GMT\")\n    raise TypeError\n\n>>> orjson.dumps({\"created_at\": datetime.datetime(1970, 1, 1)})\nb'{\"created_at\":\"1970-01-01T00:00:00\"}'\n>>> orjson.dumps({\"created_at\": datetime.datetime(1970, 1, 1)}, option=orjson.OPT_PASSTHROUGH_DATETIME)\nTypeError: Type is not JSON serializable: datetime.datetime\n>>> orjson.dumps(\n        {\"created_at\": datetime.datetime(1970, 1, 1)},\n        option=orjson.OPT_PASSTHROUGH_DATETIME,\n        default=default,\n    )\nb'{\"created_at\":\"Thu, 01 Jan 1970 00:00:00 GMT\"}'\nThis does not affect datetimes in dict keys if using OPT_NON_STR_KEYS.\nOPT_PASSTHROUGH_SUBCLASS\nPassthrough subclasses of builtin types to default.\n>>> import orjson\n>>>\nclass Secret(str):\n    pass\n\ndef default(obj):\n    if isinstance(obj, Secret):\n        return \"******\"\n    raise TypeError\n\n>>> orjson.dumps(Secret(\"zxc\"))\nb'\"zxc\"'\n>>> orjson.dumps(Secret(\"zxc\"), option=orjson.OPT_PASSTHROUGH_SUBCLASS)\nTypeError: Type is not JSON serializable: Secret\n>>> orjson.dumps(Secret(\"zxc\"), option=orjson.OPT_PASSTHROUGH_SUBCLASS, default=default)\nb'\"******\"'\nThis does not affect serializing subclasses as dict keys if using\nOPT_NON_STR_KEYS.\nOPT_SERIALIZE_DATACLASS\nThis is deprecated and has no effect in version 3. In version 2 this was\nrequired to serialize  dataclasses.dataclass instances. For more, see\ndataclass.\nOPT_SERIALIZE_NUMPY\nSerialize numpy.ndarray instances. For more, see\nnumpy.\nOPT_SERIALIZE_UUID\nThis is deprecated and has no effect in version 3. In version 2 this was\nrequired to serialize uuid.UUID instances. For more, see\nUUID.\nOPT_SORT_KEYS\nSerialize dict keys in sorted order. The default is to serialize in an\nunspecified order. This is equivalent to sort_keys=True in the standard\nlibrary.\nThis can be used to ensure the order is deterministic for hashing or tests.\nIt has a substantial performance penalty and is not recommended in general.\n>>> import orjson\n>>> orjson.dumps({\"b\": 1, \"c\": 2, \"a\": 3})\nb'{\"b\":1,\"c\":2,\"a\":3}'\n>>> orjson.dumps({\"b\": 1, \"c\": 2, \"a\": 3}, option=orjson.OPT_SORT_KEYS)\nb'{\"a\":3,\"b\":1,\"c\":2}'\nThis measures serializing the twitter.json fixture unsorted and sorted:\n\n\n\nLibrary\nunsorted (ms)\nsorted (ms)\nvs. orjson\n\n\n\n\norjson\n0.32\n0.54\n1\n\n\nujson\n1.6\n2.07\n3.8\n\n\nrapidjson\n1.12\n1.65\n3.1\n\n\nsimplejson\n2.25\n3.13\n5.8\n\n\njson\n1.78\n2.32\n4.3\n\n\n\nThe benchmark can be reproduced using the pysort script.\nThe sorting is not collation/locale-aware:\n>>> import orjson\n>>> orjson.dumps({\"a\": 1, \"\u00e4\": 2, \"A\": 3}, option=orjson.OPT_SORT_KEYS)\nb'{\"A\":3,\"a\":1,\"\\xc3\\xa4\":2}'\nThis is the same sorting behavior as the standard library, rapidjson,\nsimplejson, and ujson.\ndataclass also serialize as maps but this has no effect on them.\nOPT_STRICT_INTEGER\nEnforce 53-bit limit on integers. The limit is otherwise 64 bits, the same as\nthe Python standard library. For more, see int.\nOPT_UTC_Z\nSerialize a UTC timezone on datetime.datetime instances as Z instead\nof +00:00.\n>>> import orjson, datetime, zoneinfo\n>>> orjson.dumps(\n        datetime.datetime(1970, 1, 1, 0, 0, 0, tzinfo=zoneinfo.ZoneInfo(\"UTC\")),\n    )\nb'\"1970-01-01T00:00:00+00:00\"'\n>>> orjson.dumps(\n        datetime.datetime(1970, 1, 1, 0, 0, 0, tzinfo=zoneinfo.ZoneInfo(\"UTC\")),\n        option=orjson.OPT_UTC_Z\n    )\nb'\"1970-01-01T00:00:00Z\"'\nFragment\norjson.Fragment includes already-serialized JSON in a document. This is an\nefficient way to include JSON blobs from a cache, JSONB field, or separately\nserialized object without first deserializing to Python objects via loads().\n>>> import orjson\n>>> orjson.dumps({\"key\": \"zxc\", \"data\": orjson.Fragment(b'{\"a\": \"b\", \"c\": 1}')})\nb'{\"key\":\"zxc\",\"data\":{\"a\": \"b\", \"c\": 1}}'\nIt does no reformatting: orjson.OPT_INDENT_2 will not affect a\ncompact blob nor will a pretty-printed JSON blob be rewritten as compact.\nThe input must be bytes or str and given as a positional argument.\nThis raises orjson.JSONEncodeError if a str is given and the input is\nnot valid UTF-8. It otherwise does no validation and it is possible to\nwrite invalid JSON. This does not escape characters. The implementation is\ntested to not crash if given invalid strings or invalid JSON.\nThis is similar to RawJSON in rapidjson.\nDeserialize\ndef loads(__obj: Union[bytes, bytearray, memoryview, str]) -> Any: ...\nloads() deserializes JSON to Python objects. It deserializes to dict,\nlist, int, float, str, bool, and None objects.\nbytes, bytearray, memoryview, and str input are accepted. If the input\nexists as a memoryview, bytearray, or bytes object, it is recommended to\npass these directly rather than creating an unnecessary str object. That is,\norjson.loads(b\"{}\") instead of orjson.loads(b\"{}\".decode(\"utf-8\")). This\nhas lower memory usage and lower latency.\nThe input must be valid UTF-8.\norjson maintains a cache of map keys for the duration of the process. This\ncauses a net reduction in memory usage by avoiding duplicate strings. The\nkeys must be at most 64 bytes to be cached and 1024 entries are stored.\nThe global interpreter lock (GIL) is held for the duration of the call.\nIt raises JSONDecodeError if given an invalid type or invalid\nJSON. This includes if the input contains NaN, Infinity, or -Infinity,\nwhich the standard library allows, but is not valid JSON.\nJSONDecodeError is a subclass of json.JSONDecodeError and ValueError.\nThis is for compatibility with the standard library.\nTypes\ndataclass\norjson serializes instances of dataclasses.dataclass natively. It serializes\ninstances 40-50x as fast as other libraries and avoids a severe slowdown seen\nin other libraries compared to serializing dict.\nIt is supported to pass all variants of dataclasses, including dataclasses\nusing __slots__, frozen dataclasses, those with optional or default\nattributes, and subclasses. There is a performance benefit to not\nusing __slots__.\n\n\n\nLibrary\ndict (ms)\ndataclass (ms)\nvs. orjson\n\n\n\n\norjson\n1.40\n1.60\n1\n\n\nujson\n\n\n\n\n\nrapidjson\n3.64\n68.48\n42\n\n\nsimplejson\n14.21\n92.18\n57\n\n\njson\n13.28\n94.90\n59\n\n\n\nThis measures serializing 555KiB of JSON, orjson natively and other libraries\nusing default to serialize the output of dataclasses.asdict(). This can be\nreproduced using the pydataclass script.\nDataclasses are serialized as maps, with every attribute serialized and in\nthe order given on class definition:\n>>> import dataclasses, orjson, typing\n\n@dataclasses.dataclass\nclass Member:\n    id: int\n    active: bool = dataclasses.field(default=False)\n\n@dataclasses.dataclass\nclass Object:\n    id: int\n    name: str\n    members: typing.List[Member]\n\n>>> orjson.dumps(Object(1, \"a\", [Member(1, True), Member(2)]))\nb'{\"id\":1,\"name\":\"a\",\"members\":[{\"id\":1,\"active\":true},{\"id\":2,\"active\":false}]}'\ndatetime\norjson serializes datetime.datetime objects to\nRFC 3339 format,\ne.g., \"1970-01-01T00:00:00+00:00\". This is a subset of ISO 8601 and is\ncompatible with isoformat() in the standard library.\n>>> import orjson, datetime, zoneinfo\n>>> orjson.dumps(\n    datetime.datetime(2018, 12, 1, 2, 3, 4, 9, tzinfo=zoneinfo.ZoneInfo(\"Australia/Adelaide\"))\n)\nb'\"2018-12-01T02:03:04.000009+10:30\"'\n>>> orjson.dumps(\n    datetime.datetime(2100, 9, 1, 21, 55, 2).replace(tzinfo=zoneinfo.ZoneInfo(\"UTC\"))\n)\nb'\"2100-09-01T21:55:02+00:00\"'\n>>> orjson.dumps(\n    datetime.datetime(2100, 9, 1, 21, 55, 2)\n)\nb'\"2100-09-01T21:55:02\"'\ndatetime.datetime supports instances with a tzinfo that is None,\ndatetime.timezone.utc, a timezone instance from the python3.9+ zoneinfo\nmodule, or a timezone instance from the third-party pendulum, pytz, or\ndateutil/arrow libraries.\nIt is fastest to use the standard library's zoneinfo.ZoneInfo for timezones.\ndatetime.time objects must not have a tzinfo.\n>>> import orjson, datetime\n>>> orjson.dumps(datetime.time(12, 0, 15, 290))\nb'\"12:00:15.000290\"'\ndatetime.date objects will always serialize.\n>>> import orjson, datetime\n>>> orjson.dumps(datetime.date(1900, 1, 2))\nb'\"1900-01-02\"'\nErrors with tzinfo result in JSONEncodeError being raised.\nTo disable serialization of datetime objects specify the option\norjson.OPT_PASSTHROUGH_DATETIME.\nTo use \"Z\" suffix instead of \"+00:00\" to indicate UTC (\"Zulu\") time, use the option\norjson.OPT_UTC_Z.\nTo assume datetimes without timezone are UTC, use the option orjson.OPT_NAIVE_UTC.\nenum\norjson serializes enums natively. Options apply to their values.\n>>> import enum, datetime, orjson\n>>>\nclass DatetimeEnum(enum.Enum):\n    EPOCH = datetime.datetime(1970, 1, 1, 0, 0, 0)\n>>> orjson.dumps(DatetimeEnum.EPOCH)\nb'\"1970-01-01T00:00:00\"'\n>>> orjson.dumps(DatetimeEnum.EPOCH, option=orjson.OPT_NAIVE_UTC)\nb'\"1970-01-01T00:00:00+00:00\"'\nEnums with members that are not supported types can be serialized using\ndefault:\n>>> import enum, orjson\n>>>\nclass Custom:\n    def __init__(self, val):\n        self.val = val\n\ndef default(obj):\n    if isinstance(obj, Custom):\n        return obj.val\n    raise TypeError\n\nclass CustomEnum(enum.Enum):\n    ONE = Custom(1)\n\n>>> orjson.dumps(CustomEnum.ONE, default=default)\nb'1'\nfloat\norjson serializes and deserializes double precision floats with no loss of\nprecision and consistent rounding.\norjson.dumps() serializes Nan, Infinity, and -Infinity, which are not\ncompliant JSON, as null:\n>>> import orjson, ujson, rapidjson, json\n>>> orjson.dumps([float(\"NaN\"), float(\"Infinity\"), float(\"-Infinity\")])\nb'[null,null,null]'\n>>> ujson.dumps([float(\"NaN\"), float(\"Infinity\"), float(\"-Infinity\")])\nOverflowError: Invalid Inf value when encoding double\n>>> rapidjson.dumps([float(\"NaN\"), float(\"Infinity\"), float(\"-Infinity\")])\n'[NaN,Infinity,-Infinity]'\n>>> json.dumps([float(\"NaN\"), float(\"Infinity\"), float(\"-Infinity\")])\n'[NaN, Infinity, -Infinity]'\nint\norjson serializes and deserializes 64-bit integers by default. The range\nsupported is a signed 64-bit integer's minimum (-9223372036854775807) to\nan unsigned 64-bit integer's maximum (18446744073709551615). This\nis widely compatible, but there are implementations\nthat only support 53-bits for integers, e.g.,\nweb browsers. For those implementations, dumps() can be configured to\nraise a JSONEncodeError on values exceeding the 53-bit range.\n>>> import orjson\n>>> orjson.dumps(9007199254740992)\nb'9007199254740992'\n>>> orjson.dumps(9007199254740992, option=orjson.OPT_STRICT_INTEGER)\nJSONEncodeError: Integer exceeds 53-bit range\n>>> orjson.dumps(-9007199254740992, option=orjson.OPT_STRICT_INTEGER)\nJSONEncodeError: Integer exceeds 53-bit range\nnumpy\norjson natively serializes numpy.ndarray and individual\nnumpy.float64, numpy.float32,\nnumpy.int64, numpy.int32, numpy.int16, numpy.int8,\nnumpy.uint64, numpy.uint32, numpy.uint16, numpy.uint8,\nnumpy.uintp, numpy.intp, numpy.datetime64, and numpy.bool\ninstances.\norjson is faster than all compared libraries at serializing\nnumpy instances. Serializing numpy data requires specifying\noption=orjson.OPT_SERIALIZE_NUMPY.\n>>> import orjson, numpy\n>>> orjson.dumps(\n        numpy.array([[1, 2, 3], [4, 5, 6]]),\n        option=orjson.OPT_SERIALIZE_NUMPY,\n)\nb'[[1,2,3],[4,5,6]]'\nThe array must be a contiguous C array (C_CONTIGUOUS) and one of the\nsupported datatypes.\nNote a difference between serializing numpy.float32 using ndarray.tolist()\nor orjson.dumps(..., option=orjson.OPT_SERIALIZE_NUMPY): tolist() converts\nto a double before serializing and orjson's native path does not. This\ncan result in different rounding.\nnumpy.datetime64 instances are serialized as RFC 3339 strings and\ndatetime options affect them.\n>>> import orjson, numpy\n>>> orjson.dumps(\n        numpy.datetime64(\"2021-01-01T00:00:00.172\"),\n        option=orjson.OPT_SERIALIZE_NUMPY,\n)\nb'\"2021-01-01T00:00:00.172000\"'\n>>> orjson.dumps(\n        numpy.datetime64(\"2021-01-01T00:00:00.172\"),\n        option=(\n            orjson.OPT_SERIALIZE_NUMPY |\n            orjson.OPT_NAIVE_UTC |\n            orjson.OPT_OMIT_MICROSECONDS\n        ),\n)\nb'\"2021-01-01T00:00:00+00:00\"'\nIf an array is not a contiguous C array, contains an unsupported datatype,\nor contains a numpy.datetime64 using an unsupported representation\n(e.g., picoseconds), orjson falls through to default. In default,\nobj.tolist() can be specified. If an array is malformed, which\nis not expected, orjson.JSONEncodeError is raised.\nThis measures serializing 92MiB of JSON from an numpy.ndarray with\ndimensions of (50000, 100) and numpy.float64 values:\n\n\n\nLibrary\nLatency (ms)\nRSS diff (MiB)\nvs. orjson\n\n\n\n\norjson\n194\n99\n1.0\n\n\nujson\n\n\n\n\n\nrapidjson\n3,048\n309\n15.7\n\n\nsimplejson\n3,023\n297\n15.6\n\n\njson\n3,133\n297\n16.1\n\n\n\nThis measures serializing 100MiB of JSON from an numpy.ndarray with\ndimensions of (100000, 100) and numpy.int32 values:\n\n\n\nLibrary\nLatency (ms)\nRSS diff (MiB)\nvs. orjson\n\n\n\n\norjson\n178\n115\n1.0\n\n\nujson\n\n\n\n\n\nrapidjson\n1,512\n551\n8.5\n\n\nsimplejson\n1,606\n504\n9.0\n\n\njson\n1,506\n503\n8.4\n\n\n\nThis measures serializing 105MiB of JSON from an numpy.ndarray with\ndimensions of (100000, 200) and numpy.bool values:\n\n\n\nLibrary\nLatency (ms)\nRSS diff (MiB)\nvs. orjson\n\n\n\n\norjson\n157\n120\n1.0\n\n\nujson\n\n\n\n\n\nrapidjson\n710\n327\n4.5\n\n\nsimplejson\n931\n398\n5.9\n\n\njson\n996\n400\n6.3\n\n\n\nIn these benchmarks, orjson serializes natively, ujson is blank because it\ndoes not support a default parameter, and the other libraries serialize\nndarray.tolist() via default. The RSS column measures peak memory\nusage during serialization. This can be reproduced using the pynumpy script.\norjson does not have an installation or compilation dependency on numpy. The\nimplementation is independent, reading numpy.ndarray using\nPyArrayInterface.\nstr\norjson is strict about UTF-8 conformance. This is stricter than the standard\nlibrary's json module, which will serialize and deserialize UTF-16 surrogates,\ne.g., \"\\ud800\", that are invalid UTF-8.\nIf orjson.dumps() is given a str that does not contain valid UTF-8,\norjson.JSONEncodeError is raised. If loads() receives invalid UTF-8,\norjson.JSONDecodeError is raised.\norjson and rapidjson are the only compared JSON libraries to consistently\nerror on bad input.\n>>> import orjson, ujson, rapidjson, json\n>>> orjson.dumps('\\ud800')\nJSONEncodeError: str is not valid UTF-8: surrogates not allowed\n>>> ujson.dumps('\\ud800')\nUnicodeEncodeError: 'utf-8' codec ...\n>>> rapidjson.dumps('\\ud800')\nUnicodeEncodeError: 'utf-8' codec ...\n>>> json.dumps('\\ud800')\n'\"\\\\ud800\"'\n>>> orjson.loads('\"\\\\ud800\"')\nJSONDecodeError: unexpected end of hex escape at line 1 column 8: line 1 column 1 (char 0)\n>>> ujson.loads('\"\\\\ud800\"')\n''\n>>> rapidjson.loads('\"\\\\ud800\"')\nValueError: Parse error at offset 1: The surrogate pair in string is invalid.\n>>> json.loads('\"\\\\ud800\"')\n'\\ud800'\nTo make a best effort at deserializing bad input, first decode bytes using\nthe replace or lossy argument for errors:\n>>> import orjson\n>>> orjson.loads(b'\"\\xed\\xa0\\x80\"')\nJSONDecodeError: str is not valid UTF-8: surrogates not allowed\n>>> orjson.loads(b'\"\\xed\\xa0\\x80\"'.decode(\"utf-8\", \"replace\"))\n'\ufffd\ufffd\ufffd'\nuuid\norjson serializes uuid.UUID instances to\nRFC 4122 format, e.g.,\n\"f81d4fae-7dec-11d0-a765-00a0c91e6bf6\".\n>>> import orjson, uuid\n>>> orjson.dumps(uuid.UUID('f81d4fae-7dec-11d0-a765-00a0c91e6bf6'))\nb'\"f81d4fae-7dec-11d0-a765-00a0c91e6bf6\"'\n>>> orjson.dumps(uuid.uuid5(uuid.NAMESPACE_DNS, \"python.org\"))\nb'\"886313e1-3b8a-5372-9b90-0c9aee199e5d\"'\nTesting\nThe library has comprehensive tests. There are tests against fixtures in the\nJSONTestSuite and\nnativejson-benchmark\nrepositories. It is tested to not crash against the\nBig List of Naughty Strings.\nIt is tested to not leak memory. It is tested to not crash\nagainst and not accept invalid UTF-8. There are integration tests\nexercising the library's use in web servers (gunicorn using multiprocess/forked\nworkers) and when\nmultithreaded. It also uses some tests from the ultrajson library.\norjson is the most correct of the compared libraries. This graph shows how each\nlibrary handles a combined 342 JSON fixtures from the\nJSONTestSuite and\nnativejson-benchmark tests:\n\n\n\nLibrary\nInvalid JSON documents not rejected\nValid JSON documents not deserialized\n\n\n\n\norjson\n0\n0\n\n\nujson\n38\n0\n\n\nrapidjson\n6\n0\n\n\nsimplejson\n13\n0\n\n\njson\n17\n0\n\n\n\nThis shows that all libraries deserialize valid JSON but only orjson\ncorrectly rejects the given invalid JSON fixtures. Errors are largely due to\naccepting invalid strings and numbers.\nThe graph above can be reproduced using the pycorrectness script.\nPerformance\nSerialization and deserialization performance of orjson is better than\nultrajson, rapidjson, simplejson, or json. The benchmarks are done on\nfixtures of real data:\n\n\ntwitter.json, 631.5KiB, results of a search on Twitter for \"\u4e00\", containing\nCJK strings, dictionaries of strings and arrays of dictionaries, indented.\n\n\ngithub.json, 55.8KiB, a GitHub activity feed, containing dictionaries of\nstrings and arrays of dictionaries, not indented.\n\n\ncitm_catalog.json, 1.7MiB, concert data, containing nested dictionaries of\nstrings and arrays of integers, indented.\n\n\ncanada.json, 2.2MiB, coordinates of the Canadian border in GeoJSON\nformat, containing floats and arrays, indented.\n\n\nLatency\ntwitter.json serialization\n\n\n\nLibrary\nMedian latency (milliseconds)\nOperations per second\nRelative (latency)\n\n\n\n\norjson\n0.33\n3069.4\n1\n\n\nujson\n1.68\n592.8\n5.15\n\n\nrapidjson\n1.12\n891\n3.45\n\n\nsimplejson\n2.29\n436.2\n7.03\n\n\njson\n1.8\n556.6\n5.52\n\n\n\ntwitter.json deserialization\n\n\n\nLibrary\nMedian latency (milliseconds)\nOperations per second\nRelative (latency)\n\n\n\n\norjson\n0.81\n1237.6\n1\n\n\nujson\n1.87\n533.9\n2.32\n\n\nrapidjson\n2.97\n335.8\n3.67\n\n\nsimplejson\n2.15\n463.8\n2.66\n\n\njson\n2.45\n408.2\n3.03\n\n\n\ngithub.json serialization\n\n\n\nLibrary\nMedian latency (milliseconds)\nOperations per second\nRelative (latency)\n\n\n\n\norjson\n0.03\n28817.3\n1\n\n\nujson\n0.18\n5478.2\n5.26\n\n\nrapidjson\n0.1\n9686.4\n2.98\n\n\nsimplejson\n0.26\n3901.3\n7.39\n\n\njson\n0.18\n5437\n5.27\n\n\n\ngithub.json deserialization\n\n\n\nLibrary\nMedian latency (milliseconds)\nOperations per second\nRelative (latency)\n\n\n\n\norjson\n0.07\n15270\n1\n\n\nujson\n0.19\n5374.8\n2.84\n\n\nrapidjson\n0.17\n5854.9\n2.59\n\n\nsimplejson\n0.15\n6707.4\n2.27\n\n\njson\n0.16\n6397.3\n2.39\n\n\n\ncitm_catalog.json serialization\n\n\n\nLibrary\nMedian latency (milliseconds)\nOperations per second\nRelative (latency)\n\n\n\n\norjson\n0.58\n1722.5\n1\n\n\nujson\n2.89\n345.6\n4.99\n\n\nrapidjson\n1.83\n546.4\n3.15\n\n\nsimplejson\n10.39\n95.9\n17.89\n\n\njson\n3.93\n254.6\n6.77\n\n\n\ncitm_catalog.json deserialization\n\n\n\nLibrary\nMedian latency (milliseconds)\nOperations per second\nRelative (latency)\n\n\n\n\norjson\n1.76\n569.2\n1\n\n\nujson\n3.5\n284.3\n1.99\n\n\nrapidjson\n5.77\n173.2\n3.28\n\n\nsimplejson\n5.13\n194.7\n2.92\n\n\njson\n4.99\n200.5\n2.84\n\n\n\ncanada.json serialization\n\n\n\nLibrary\nMedian latency (milliseconds)\nOperations per second\nRelative (latency)\n\n\n\n\norjson\n3.62\n276.3\n1\n\n\nujson\n14.16\n70.6\n3.91\n\n\nrapidjson\n33.64\n29.7\n9.29\n\n\nsimplejson\n57.46\n17.4\n15.88\n\n\njson\n35.7\n28\n9.86\n\n\n\ncanada.json deserialization\n\n\n\nLibrary\nMedian latency (milliseconds)\nOperations per second\nRelative (latency)\n\n\n\n\norjson\n3.89\n256.6\n1\n\n\nujson\n8.73\n114.3\n2.24\n\n\nrapidjson\n23.33\n42.8\n5.99\n\n\nsimplejson\n23.99\n41.7\n6.16\n\n\njson\n21.1\n47.4\n5.42\n\n\n\nMemory\norjson as of 3.7.0 has higher baseline memory usage than other libraries\ndue to a persistent buffer used for parsing. Incremental memory usage when\ndeserializing is similar to the standard library and other third-party\nlibraries.\nThis measures, in the first column, RSS after importing a library and reading\nthe fixture, and in the second column, increases in RSS after repeatedly\ncalling loads() on the fixture.\ntwitter.json\n\n\n\nLibrary\nimport, read() RSS (MiB)\nloads() increase in RSS (MiB)\n\n\n\n\norjson\n21.8\n2.8\n\n\nujson\n14.3\n4.8\n\n\nrapidjson\n14.9\n4.6\n\n\nsimplejson\n13.4\n2.4\n\n\njson\n13.1\n2.3\n\n\n\ngithub.json\n\n\n\nLibrary\nimport, read() RSS (MiB)\nloads() increase in RSS (MiB)\n\n\n\n\norjson\n21.2\n0.5\n\n\nujson\n13.6\n0.6\n\n\nrapidjson\n14.1\n0.5\n\n\nsimplejson\n12.5\n0.3\n\n\njson\n12.4\n0.3\n\n\n\ncitm_catalog.json\n\n\n\nLibrary\nimport, read() RSS (MiB)\nloads() increase in RSS (MiB)\n\n\n\n\norjson\n23\n10.6\n\n\nujson\n15.2\n11.2\n\n\nrapidjson\n15.8\n29.7\n\n\nsimplejson\n14.4\n24.7\n\n\njson\n13.9\n24.7\n\n\n\ncanada.json\n\n\n\nLibrary\nimport, read() RSS (MiB)\nloads() increase in RSS (MiB)\n\n\n\n\norjson\n23.2\n21.3\n\n\nujson\n15.6\n19.2\n\n\nrapidjson\n16.3\n23.4\n\n\nsimplejson\n15\n21.1\n\n\njson\n14.3\n20.9\n\n\n\nReproducing\nThe above was measured using Python 3.10.5 on Linux (amd64) with\norjson 3.7.9, ujson 5.4.0, python-rapidson 1.8, and simplejson 3.17.6.\nThe latency results can be reproduced using the pybench and graph\nscripts. The memory results can be reproduced using the pymem script.\nQuestions\nWhy can't I install it from PyPI?\nProbably pip needs to be upgraded to version 20.3 or later to support\nthe latest manylinux_x_y or universal2 wheel formats.\n\"Cargo, the Rust package manager, is not installed or is not on PATH.\"\nThis happens when there are no binary wheels (like manylinux) for your\nplatform on PyPI. You can install Rust through\nrustup or a package manager and then it will compile.\nWill it deserialize to dataclasses, UUIDs, decimals, etc or support object_hook?\nNo. This requires a schema specifying what types are expected and how to\nhandle errors etc. This is addressed by data validation libraries a\nlevel above this.\nWill it serialize to str?\nNo. bytes is the correct type for a serialized blob.\nWill it support PyPy?\nProbably not.\nPackaging\nTo package orjson requires at least Rust 1.60\nand the maturin build tool. The recommended\nbuild command is:\nmaturin build --release --strip\nIt benefits from also having a C build environment to compile a faster\ndeserialization backend. See this project's manylinux_2_28 builds for an\nexample using clang and LTO.\nThe project's own CI tests against nightly-2023-06-30 and stable 1.60. It\nis prudent to pin the nightly version because that channel can introduce\nbreaking changes.\norjson is tested for amd64, aarch64, arm7, ppc64le, and s390x on Linux. It\nis tested for amd64 on macOS and cross-compiles for aarch64. For Windows\nit is tested on amd64 and i686.\nThere are no runtime dependencies other than libc.\nThe source distribution on PyPI contains all dependencies' source and can be\nbuilt without network access. The file can be downloaded from\nhttps://files.pythonhosted.org/packages/source/o/orjson/orjson-${version}.tar.gz.\norjson's tests are included in the source distribution on PyPI. The\nrequirements to run the tests are specified in test/requirements.txt. The\ntests should be run as part of the build. It can be run with\npytest -q test.\nLicense\norjson was written by ijl <ijl@mailbox.org>, copyright 2018 - 2023, licensed\nunder both the Apache 2 and MIT licenses.\n\n\n"}, {"name": "opt-einsum", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nOptimized Einsum\nOptimized Einsum: A tensor contraction order optimizer\nExample usage\nFeatures\nInstallation\nCitation\nContributing\n\n\n\n\n\nREADME.md\n\n\n\n\nOptimized Einsum\n\n\n\n\n\n\n\nOptimized Einsum: A tensor contraction order optimizer\nOptimized einsum can significantly reduce the overall execution time of einsum-like expressions (e.g.,\nnp.einsum,\ndask.array.einsum,\npytorch.einsum,\ntensorflow.einsum,\n)\nby optimizing the expression's contraction order and dispatching many\noperations to canonical BLAS, cuBLAS, or other specialized routines.\nOptimized\neinsum is agnostic to the backend and can handle NumPy, Dask, PyTorch,\nTensorflow, CuPy, Sparse, Theano, JAX, and Autograd arrays as well as potentially\nany library which conforms to a standard API. See the\ndocumentation for more\ninformation.\nExample usage\nThe opt_einsum.contract\nfunction can often act as a drop-in replacement for einsum\nfunctions without further changes to the code while providing superior performance.\nHere, a tensor contraction is performed with and without optimization:\nimport numpy as np\nfrom opt_einsum import contract\n\nN = 10\nC = np.random.rand(N, N)\nI = np.random.rand(N, N, N, N)\n\n%timeit np.einsum('pi,qj,ijkl,rk,sl->pqrs', C, C, I, C, C)\n1 loops, best of 3: 934 ms per loop\n\n%timeit contract('pi,qj,ijkl,rk,sl->pqrs', C, C, I, C, C)\n1000 loops, best of 3: 324 us per loop\nIn this particular example, we see a ~3000x performance improvement which is\nnot uncommon when compared against unoptimized contractions. See the backend\nexamples\nfor more information on using other backends.\nFeatures\nThe algorithms found in this repository often power the einsum optimizations\nin many of the above projects. For example, the optimization of np.einsum\nhas been passed upstream and most of the same features that can be found in\nthis repository can be enabled with np.einsum(..., optimize=True). However,\nthis repository often has more up to date algorithms for complex contractions.\nThe following capabilities are enabled by opt_einsum:\n\nInspect detailed information about the path chosen.\nPerform contractions with numerous backends, including on the GPU and with libraries such as TensorFlow and PyTorch.\nGenerate reusable expressions, potentially with constant tensors, that can be compiled for greater performance.\nUse an arbitrary number of indices to find contractions for hundreds or even thousands of tensors.\nShare intermediate computations among multiple contractions.\nCompute gradients of tensor contractions using autograd or jax\n\nPlease see the documentation for more features!\nInstallation\nopt_einsum can either be installed via pip install opt_einsum or from conda conda install opt_einsum -c conda-forge.\nSee the installation documentation for further methods.\nCitation\nIf this code has benefited your research, please support us by citing:\nDaniel G. A. Smith and Johnnie Gray, opt_einsum - A Python package for optimizing contraction order for einsum-like expressions. Journal of Open Source Software, 2018, 3(26), 753\nDOI: https://doi.org/10.21105/joss.00753\nContributing\nAll contributions, bug reports, bug fixes, documentation improvements, enhancements, and ideas are welcome.\nA detailed overview on how to contribute can be found in the contributing guide.\n\n\n"}, {"name": "opencv-python", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpenCV on Wheels\nInstallation and Usage\nFrequently Asked Questions\nDocumentation for opencv-python\nCI build process\nManual builds\nManual debug builds\nSource distributions\nLicensing\nVersioning\nReleases\nDevelopment builds\nManylinux wheels\nSupported Python versions\nBackward compatibility\n\n\n\n\n\nREADME.md\n\n\n\n\n\n\nOpenCV on Wheels\n\nInstallation and Usage\n\n\nFrequently Asked Questions\nDocumentation for opencv-python\n\nCI build process\nManual builds\n\nManual debug builds\nSource distributions\n\n\nLicensing\nVersioning\nReleases\nDevelopment builds\nManylinux wheels\nSupported Python versions\nBackward compatibility\n\n\n\nOpenCV on Wheels\nPre-built CPU-only OpenCV packages for Python.\nCheck the manual build section if you wish to compile the bindings from source to enable additional modules such as CUDA.\nInstallation and Usage\n\n\nIf you have previous/other manually installed (= not installed via pip) version of OpenCV installed (e.g. cv2 module in the root of Python's site-packages), remove it before installation to avoid conflicts.\n\n\nMake sure that your pip version is up-to-date (19.3 is the minimum supported version): pip install --upgrade pip. Check version with pip -V. For example Linux distributions ship usually with very old pip versions which cause a lot of unexpected problems especially with the manylinux format.\n\n\nSelect the correct package for your environment:\nThere are four different packages (see options 1, 2, 3 and 4 below) and you should SELECT ONLY ONE OF THEM. Do not install multiple different packages in the same environment. There is no plugin architecture: all the packages use the same namespace (cv2). If you installed multiple different packages in the same environment, uninstall them all with pip uninstall and reinstall only one package.\na. Packages for standard desktop environments (Windows, macOS, almost any GNU/Linux distribution)\n\nOption 1 - Main modules package: pip install opencv-python\nOption 2 - Full package (contains both main modules and contrib/extra modules): pip install opencv-contrib-python (check contrib/extra modules listing from OpenCV documentation)\n\nb. Packages for server (headless) environments (such as Docker, cloud environments etc.), no GUI library dependencies\nThese packages are smaller than the two other packages above because they do not contain any GUI functionality (not compiled with Qt / other GUI components). This means that the packages avoid a heavy dependency chain to X11 libraries and you will have for example smaller Docker images as a result. You should always use these packages if you do not use cv2.imshow et al. or you are using some other package (such as PyQt) than OpenCV to create your GUI.\n\nOption 3 - Headless main modules package: pip install opencv-python-headless\nOption 4 - Headless full package (contains both main modules and contrib/extra modules): pip install opencv-contrib-python-headless (check contrib/extra modules listing from OpenCV documentation)\n\n\n\nImport the package:\nimport cv2\nAll packages contain Haar cascade files. cv2.data.haarcascades can be used as a shortcut to the data folder. For example:\ncv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n\n\nRead OpenCV documentation\n\n\nBefore opening a new issue, read the FAQ below and have a look at the other issues which are already open.\n\n\nFrequently Asked Questions\nQ: Do I need to install also OpenCV separately?\nA: No, the packages are special wheel binary packages and they already contain statically built OpenCV binaries.\nQ: Pip install fails with ModuleNotFoundError: No module named 'skbuild'?\nSince opencv-python version 4.3.0.*, manylinux1 wheels were replaced by manylinux2014 wheels. If your pip is too old, it will try to use the new source distribution introduced in 4.3.0.38 to manually build OpenCV because it does not know how to install manylinux2014 wheels. However, source build will also fail because of too old pip because it does not understand build dependencies in pyproject.toml. To use the new manylinux2014 pre-built wheels (or to build from source), your pip version must be >= 19.3. Please upgrade pip with pip install --upgrade pip.\nQ: Import fails on Windows: ImportError: DLL load failed: The specified module could not be found.?\nA: If the import fails on Windows, make sure you have Visual C++ redistributable 2015 installed. If you are using older Windows version than Windows 10 and latest system updates are not installed, Universal C Runtime might be also required.\nWindows N and KN editions do not include Media Feature Pack which is required by OpenCV. If you are using Windows N or KN edition, please install also Windows Media Feature Pack.\nIf you have Windows Server 2012+, media DLLs are probably missing too; please install the Feature called \"Media Foundation\" in the Server Manager. Beware, some posts advise to install \"Windows Server Essentials Media Pack\", but this one requires the \"Windows Server Essentials Experience\" role, and this role will deeply affect your Windows Server configuration (by enforcing active directory integration etc.); so just installing the \"Media Foundation\" should be a safer choice.\nIf the above does not help, check if you are using Anaconda. Old Anaconda versions have a bug which causes the error, see this issue for a manual fix.\nIf you still encounter the error after you have checked all the previous solutions, download Dependencies and open the cv2.pyd (located usually at C:\\Users\\username\\AppData\\Local\\Programs\\Python\\PythonXX\\Lib\\site-packages\\cv2) file with it to debug missing DLL issues.\nQ: I have some other import errors?\nA: Make sure you have removed old manual installations of OpenCV Python bindings (cv2.so or cv2.pyd in site-packages).\nQ: Function foo() or method bar() returns wrong result, throws exception or crashes interpreter. What should I do?\nA: The repository contains only OpenCV-Python package build scripts, but not OpenCV itself. Python bindings for OpenCV are developed in official OpenCV repository and it's the best place to report issues. Also please check OpenCV wiki and the official OpenCV forum before file new bugs.\nQ: Why the packages do not include non-free algorithms?\nA: Non-free algorithms such as SURF are not included in these packages because they are patented / non-free and therefore cannot be distributed as built binaries. Note that SIFT is included in the builds due to patent expiration since OpenCV versions 4.3.0 and 3.4.10. See this issue for more info: #126\nQ: Why the package and import are different (opencv-python vs. cv2)?\nA: It's easier for users to understand opencv-python than cv2 and it makes it easier to find the package with search engines. cv2 (old interface in old OpenCV versions was named as cv) is the name that OpenCV developers chose when they created the binding generators. This is kept as the import name to be consistent with different kind of tutorials around the internet. Changing the import name or behaviour would be also confusing to experienced users who are accustomed to the import cv2.\nDocumentation for opencv-python\n\n\n\nThe aim of this repository is to provide means to package each new OpenCV release for the most used Python versions and platforms.\nCI build process\nThe project is structured like a normal Python package with a standard setup.py file.\nThe build process for a single entry in the build matrices is as follows (see for example .github/workflows/build_wheels_linux.yml file):\n\n\nIn Linux and MacOS build: get OpenCV's optional C dependencies that we compile against\n\n\nCheckout repository and submodules\n\nOpenCV is included as submodule and the version is updated\nmanually by maintainers when a new OpenCV release has been made\nContrib modules are also included as a submodule\n\n\n\nFind OpenCV version from the sources\n\n\nBuild OpenCV\n\ntests are disabled, otherwise build time increases too much\nthere are 4 build matrix entries for each build combination: with and without contrib modules, with and without GUI (headless)\nLinux builds run in manylinux Docker containers (CentOS 5)\nsource distributions are separate entries in the build matrix\n\n\n\nRearrange OpenCV's build result, add our custom files and generate wheel\n\n\nLinux and macOS wheels are transformed with auditwheel and delocate, correspondingly\n\n\nInstall the generated wheel\n\n\nTest that Python can import the library and run some sanity checks\n\n\nUse twine to upload the generated wheel to PyPI (only in release builds)\n\n\nSteps 1--4 are handled by pip wheel.\nThe build can be customized with environment variables. In addition to any variables that OpenCV's build accepts, we recognize:\n\nCI_BUILD. Set to 1 to emulate the CI environment build behaviour. Used only in CI builds to force certain build flags on in setup.py. Do not use this unless you know what you are doing.\nENABLE_CONTRIB and ENABLE_HEADLESS. Set to 1 to build the contrib and/or headless version\nENABLE_JAVA, Set to 1 to enable the Java client build.  This is disabled by default.\nCMAKE_ARGS. Additional arguments for OpenCV's CMake invocation. You can use this to make a custom build.\n\nSee the next section for more info about manual builds outside the CI environment.\nManual builds\nIf some dependency is not enabled in the pre-built wheels, you can also run the build locally to create a custom wheel.\n\nClone this repository: git clone --recursive https://github.com/opencv/opencv-python.git\ncd opencv-python\n\nyou can use git to checkout some other version of OpenCV in the opencv and opencv_contrib submodules if needed\n\n\nAdd custom Cmake flags if needed, for example: export CMAKE_ARGS=\"-DSOME_FLAG=ON -DSOME_OTHER_FLAG=OFF\" (in Windows you need to set environment variables differently depending on Command Line or PowerShell)\nSelect the package flavor which you wish to build with ENABLE_CONTRIB and ENABLE_HEADLESS: i.e. export ENABLE_CONTRIB=1 if you wish to build opencv-contrib-python\nRun pip wheel . --verbose. NOTE: make sure you have the latest pip version, the pip wheel command replaces the old python setup.py bdist_wheel command which does not support pyproject.toml.\n\nthis might take anything from 5 minutes to over 2 hours depending on your hardware\n\n\nPip will print fresh will location at the end of build procedure. If you use old approach with setup.py file wheel package will be placed in dist folder. Package is ready and you can do with that whatever you wish.\n\nOptional: on Linux use some of the manylinux images as a build hosts if maximum portability is needed and run auditwheel for the wheel after build\nOptional: on macOS use delocate (same as auditwheel but for macOS) for better portability\n\n\n\nManual debug builds\nIn order to build opencv-python in an unoptimized debug build, you need to side-step the normal process a bit.\n\nInstall the packages scikit-build and numpy via pip.\nRun the command python setup.py bdist_wheel --build-type=Debug.\nInstall the generated wheel file in the dist/ folder with pip install dist/wheelname.whl.\n\nIf you would like the build produce all compiler commands, then the following combination of flags and environment variables has been tested to work on Linux:\nexport CMAKE_ARGS='-DCMAKE_VERBOSE_MAKEFILE=ON'\nexport VERBOSE=1\n\npython3 setup.py bdist_wheel --build-type=Debug\n\nSee this issue for more discussion: #424\nSource distributions\nSince OpenCV version 4.3.0, also source distributions are provided in PyPI. This means that if your system is not compatible with any of the wheels in PyPI, pip will attempt to build OpenCV from sources. If you need a OpenCV version which is not available in PyPI as a source distribution, please follow the manual build guidance above instead of this one.\nYou can also force pip to build the wheels from the source distribution. Some examples:\n\npip install --no-binary opencv-python opencv-python\npip install --no-binary :all: opencv-python\n\nIf you need contrib modules or headless version, just change the package name (step 4 in the previous section is not needed). However, any additional CMake flags can be provided via environment variables as described in step 3 of the manual build section. If none are provided, OpenCV's CMake scripts will attempt to find and enable any suitable dependencies. Headless distributions have hard coded CMake flags which disable all possible GUI dependencies.\nOn slow systems such as Raspberry Pi the full build may take several hours. On a 8-core Ryzen 7 3700X the build takes about 6 minutes.\nLicensing\nOpencv-python package (scripts in this repository) is available under MIT license.\nOpenCV itself is available under Apache 2 license.\nThird party package licenses are at LICENSE-3RD-PARTY.txt.\nAll wheels ship with FFmpeg licensed under the LGPLv2.1.\nNon-headless Linux wheels ship with Qt 5 licensed under the LGPLv3.\nThe packages include also other binaries. Full list of licenses can be found from LICENSE-3RD-PARTY.txt.\nVersioning\nfind_version.py script searches for the version information from OpenCV sources and appends also a revision number specific to this repository to the version string. It saves the version information to version.py file under cv2 in addition to some other flags.\nReleases\nA release is made and uploaded to PyPI when a new tag is pushed to master branch. These tags differentiate packages (this repo might have modifications but OpenCV version stays same) and should be incremented sequentially. In practice, release version numbers look like this:\ncv_major.cv_minor.cv_revision.package_revision e.g. 3.1.0.0\nThe master branch follows OpenCV master branch releases. 3.4 branch follows OpenCV 3.4 bugfix releases.\nDevelopment builds\nEvery commit to the master branch of this repo will be built. Possible build artifacts use local version identifiers:\ncv_major.cv_minor.cv_revision+git_hash_of_this_repo e.g. 3.1.0+14a8d39\nThese artifacts can't be and will not be uploaded to PyPI.\nManylinux wheels\nLinux wheels are built using manylinux2014. These wheels should work out of the box for most of the distros (which use GNU C standard library) out there since they are built against an old version of glibc.\nThe default manylinux2014 images have been extended with some OpenCV dependencies. See Docker folder for more info.\nSupported Python versions\nPython 3.x compatible pre-built wheels are provided for the officially supported Python versions (not in EOL):\n\n3.7\n3.8\n3.9\n3.10\n3.11\n\nBackward compatibility\nStarting from 4.2.0 and 3.4.9 builds the macOS Travis build environment was updated to XCode 9.4. The change effectively dropped support for older than 10.13 macOS versions.\nStarting from 4.3.0 and 3.4.10 builds the Linux build environment was updated from manylinux1 to manylinux2014. This dropped support for old Linux distributions.\nStarting from version 4.7.0 the Mac OS GitHub Actions build environment was update to version 11. Mac OS 10.x support depricated. See actions/runner-images#5583\n\n\n"}, {"name": "numpy-financial", "readme": "\nNumPy Financial\nThe numpy-financial package contains a collection of elementary financial\nfunctions.\nThe financial functions in NumPy\nare deprecated and eventually will be removed from NumPy; see\nNEP-32\nfor more information.  This package is the replacement for the original\nNumPy financial functions.\nThe source code for this package is available at https://github.com/numpy/numpy-financial.\nThe importable name of the package is numpy_financial.  The recommended\nalias is npf.  For example,\n>>> import numpy_financial as npf\n>>> npf.irr([-250000, 100000, 150000, 200000, 250000, 300000])\n0.5672303344358536\n\n"}, {"name": "notebook-shim", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJupyter Server\nInstallation and Basic usage\nVersioning and Branches\nUsage - Running Jupyter Server\nRunning in a local installation\nTesting\nContributing\nTeam Meetings and Roadmap\nAbout the Jupyter Development Team\nOur Copyright Policy\n\n\n\n\n\nREADME.md\n\n\n\n\nJupyter Server\n\n\nThe Jupyter Server provides the backend (i.e. the core services, APIs, and REST endpoints) for Jupyter web applications like Jupyter notebook, JupyterLab, and Voila.\nFor more information, read our documentation here.\nInstallation and Basic usage\nTo install the latest release locally, make sure you have\npip installed and run:\npip install jupyter_server\n\nJupyter Server currently supports Python>=3.6 on Linux, OSX and Windows.\nVersioning and Branches\nIf Jupyter Server is a dependency of your project/application, it is important that you pin it to a version that works for your application. Currently, Jupyter Server only has minor and patch versions. Different minor versions likely include API-changes while patch versions do not change API.\nWhen a new minor version is released on PyPI, a branch for that version will be created in this repository, and the version of the main branch will be bumped to the next minor version number. That way, the main branch always reflects the latest un-released version.\nTo see the changes between releases, checkout the CHANGELOG.\nUsage - Running Jupyter Server\nRunning in a local installation\nLaunch with:\njupyter server\n\nTesting\nSee CONTRIBUTING.\nContributing\nIf you are interested in contributing to the project, see CONTRIBUTING.rst.\nTeam Meetings and Roadmap\n\nWhen: Thursdays 8:00am, Pacific time\nWhere: Jovyan Zoom\nWhat: Meeting notes\n\nSee our tentative roadmap here.\nAbout the Jupyter Development Team\nThe Jupyter Development Team is the set of all contributors to the Jupyter project.\nThis includes all of the Jupyter subprojects.\nThe core team that coordinates development on GitHub can be found here:\nhttps://github.com/jupyter/.\nOur Copyright Policy\nJupyter uses a shared copyright model. Each contributor maintains copyright\nover their contributions to Jupyter. But, it is important to note that these\ncontributions are typically only changes to the repositories. Thus, the Jupyter\nsource code, in its entirety is not the copyright of any single person or\ninstitution. Instead, it is the collective copyright of the entire Jupyter\nDevelopment Team. If individual contributors want to maintain a record of what\nchanges/contributions they have specific copyright on, they should indicate\ntheir copyright in the commit message of the change, when they commit the\nchange to one of the Jupyter repositories.\nWith this in mind, the following banner should be used in any source code file\nto indicate the copyright and license terms:\n# Copyright (c) Jupyter Development Team.\n# Distributed under the terms of the Modified BSD License.\n\n\n\n"}, {"name": "nest-asyncio", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\nInstallation\nUsage\n\n\n\n\n\nREADME.rst\n\n\n\n\n \n\n \n\n\nIntroduction\nBy design asyncio does not allow\nits event loop to be nested. This presents a practical problem:\nWhen in an environment where the event loop is\nalready running it's impossible to run tasks and wait\nfor the result. Trying to do so will give the error\n\"RuntimeError: This event loop is already running\".\nThe issue pops up in various environments, such as web servers,\nGUI applications and in Jupyter notebooks.\nThis module patches asyncio to allow nested use of asyncio.run and\nloop.run_until_complete.\n\nInstallation\npip3 install nest_asyncio\n\nPython 3.5 or higher is required.\n\nUsage\nimport nest_asyncio\nnest_asyncio.apply()\nOptionally the specific loop that needs patching can be given\nas argument to apply, otherwise the current event loop is used.\nAn event loop can be patched whether it is already running\nor not. Only event loops from asyncio can be patched;\nLoops from other projects, such as uvloop or quamash,\ngenerally can't be patched.\n\n\n"}, {"name": "matplotlib-venn", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nVenn diagram plotting routines for Python/Matplotlib\nInstallation\nDependencies\nUsage\nQuestions\nSee also\n\n\n\n\n\nREADME.rst\n\n\n\n\nVenn diagram plotting routines for Python/Matplotlib\n\nRoutines for plotting area-weighted two- and three-circle venn diagrams.\n\nInstallation\nThe simplest way to install the package is via easy_install or\npip:\n$ easy_install matplotlib-venn\n\n\nDependencies\n\nnumpy,\nscipy,\nmatplotlib.\n\n\nUsage\nThe package provides four main functions: venn2,\nvenn2_circles, venn3 and venn3_circles.\nThe functions venn2 and venn2_circles accept as their only\nrequired argument a 3-element list (Ab, aB, AB) of subset sizes,\ne.g.:\nvenn2(subsets = (3, 2, 1))\n\nand draw a two-circle venn diagram with respective region areas. In\nthe particular example, the region, corresponding to subset A and\nnot B will be three times larger in area than the region,\ncorresponding to subset A and B. Alternatively, you can simply\nprovide a list of two set or Counter (i.e. multi-set) objects instead (new in version 0.7),\ne.g.:\nvenn2([set(['A', 'B', 'C', 'D']), set(['D', 'E', 'F'])])\n\nSimilarly, the functions venn3 and venn3_circles take a\n7-element list of subset sizes (Abc, aBc, ABc, abC, AbC, aBC,\nABC), and draw a three-circle area-weighted venn\ndiagram. Alternatively, you can provide a list of three set or Counter objects\n(rather than counting sizes for all 7 subsets).\nThe functions venn2_circles and venn3_circles draw just the\ncircles, whereas the functions venn2 and venn3 draw the\ndiagrams as a collection of colored patches, annotated with text\nlabels. In addition (version 0.7+), functions venn2_unweighted and\nvenn3_unweighted draw the Venn diagrams without area-weighting.\nNote that for a three-circle venn diagram it is not in general\npossible to achieve exact correspondence between the required set\nsizes and region areas, however in most cases the picture will still\nprovide a decent indication.\nThe functions venn2_circles and venn3_circles return the list of matplotlib.patch.Circle objects that may be tuned further\nto your liking. The functions venn2 and venn3 return an object of class VennDiagram,\nwhich gives access to constituent patches, text elements, and (since\nversion 0.7) the information about the centers and radii of the\ncircles.\nBasic Example:\nfrom matplotlib_venn import venn2\nvenn2(subsets = (3, 2, 1))\n\nFor the three-circle case:\nfrom matplotlib_venn import venn3\nvenn3(subsets = (1, 1, 1, 2, 1, 2, 2), set_labels = ('Set1', 'Set2', 'Set3'))\n\nA more elaborate example:\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom matplotlib_venn import venn3, venn3_circles\nplt.figure(figsize=(4,4))\nv = venn3(subsets=(1, 1, 1, 1, 1, 1, 1), set_labels = ('A', 'B', 'C'))\nv.get_patch_by_id('100').set_alpha(1.0)\nv.get_patch_by_id('100').set_color('white')\nv.get_label_by_id('100').set_text('Unknown')\nv.get_label_by_id('A').set_text('Set \"A\"')\nc = venn3_circles(subsets=(1, 1, 1, 1, 1, 1, 1), linestyle='dashed')\nc[0].set_lw(1.0)\nc[0].set_ls('dotted')\nplt.title(\"Sample Venn diagram\")\nplt.annotate('Unknown set', xy=v.get_label_by_id('100').get_position() - np.array([0, 0.05]), xytext=(-70,-70),\n             ha='center', textcoords='offset points', bbox=dict(boxstyle='round,pad=0.5', fc='gray', alpha=0.1),\n             arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.5',color='gray'))\nplt.show()\n\nAn example with multiple subplots (new in version 0.6):\nfrom matplotlib_venn import venn2, venn2_circles\nfigure, axes = plt.subplots(2, 2)\nvenn2(subsets={'10': 1, '01': 1, '11': 1}, set_labels = ('A', 'B'), ax=axes[0][0])\nvenn2_circles((1, 2, 3), ax=axes[0][1])\nvenn3(subsets=(1, 1, 1, 1, 1, 1, 1), set_labels = ('A', 'B', 'C'), ax=axes[1][0])\nvenn3_circles({'001': 10, '100': 20, '010': 21, '110': 13, '011': 14}, ax=axes[1][1])\nplt.show()\n\nPerhaps the most common use case is generating a Venn diagram given\nthree sets of objects:\nset1 = set(['A', 'B', 'C', 'D'])\nset2 = set(['B', 'C', 'D', 'E'])\nset3 = set(['C', 'D',' E', 'F', 'G'])\n\nvenn3([set1, set2, set3], ('Set1', 'Set2', 'Set3'))\nplt.show()\n\n\nQuestions\n\nIf you ask your questions at StackOverflow and tag them matplotlib-venn, chances are high you'll get an answer from the maintainer of this package.\n\n\nSee also\n\nReport issues and submit fixes at Github:\nhttps://github.com/konstantint/matplotlib-venn\nCheck out the DEVELOPER-README.rst for development-related notes.\n\nSome alternative means of plotting a Venn diagram (as of\nOctober 2012) are reviewed in the blog post:\nhttp://fouryears.eu/2012/10/13/venn-diagrams-in-python/\n\nThe matplotlib-subsets package\nvisualizes a hierarchy of sets as a tree of rectangles.\n\nThe matplotlib_venn_wordcloud package\ncombines Venn diagrams with word clouds for a pretty amazing (and amusing) result.\n\n\n\n\n"}, {"name": "matplotlib-inline", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nMatplotlib Inline Back-end for IPython and Jupyter\nInstallation\nUsage\nLicense\n\n\n\n\n\nREADME.md\n\n\n\n\nMatplotlib Inline Back-end for IPython and Jupyter\nThis package provides support for matplotlib to display figures directly inline in the Jupyter notebook and related clients, as shown below.\nInstallation\nWith conda:\nconda install -c conda-forge matplotlib-inline\nWith pip:\npip install matplotlib-inline\nUsage\nNote that in current versions of JupyterLab and Jupyter Notebook, the explicit use of the %matplotlib inline directive is not needed anymore, though other third-party clients may still require it.\nThis will produce a figure immediately below:\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0, 3*np.pi, 500)\nplt.plot(x, np.sin(x**2))\nplt.title('A simple chirp');\nLicense\nLicensed under the terms of the BSD 3-Clause License, by the IPython Development Team (see LICENSE file).\n\n\n"}, {"name": "korean-lunar-calendar", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nkorean_lunar_calendar\nOverview\nDocs\nInstall\nImport\nExample\nValidation\nOther languages\n\n\n\n\n\nREADME.md\n\n\n\n\nkorean_lunar_calendar\n\nLibrary to convert Korean lunar-calendar to Gregorian calendar.\n\nOverview\nKorean calendar and Chinese calendar is same lunar calendar but have different date.\nThis follow the KARI(Korea Astronomy and Space Science Institute)\n\ud55c\uad6d \uc591\uc74c\ub825 \ubcc0\ud658 (\ud55c\uad6d\ucc9c\ubb38\uc5f0\uad6c\uc6d0 \uae30\uc900) - \ub124\ud2b8\uc6cc\ud06c \uc5f0\uacb0 \ubd88\ud544\uc694\n\uc74c\ub825 \uc9c0\uc6d0 \ubc94\uc704 (1000\ub144 01\uc6d4 01\uc77c ~ 2050\ub144 11\uc6d4 18\uc77c)\nKorean Lunar Calendar (1000-01-01 ~ 2050-11-18)\n\n\uc591\ub825 \uc9c0\uc6d0 \ubc94\uc704 (1000\ub144 02\uc6d4 13\uc77c ~ 2050\ub144 12\uc6d4 31\uc77c)\nGregorian Calendar (1000-02-13 ~ 2050-12-31)\n\nExample Site\nDocs\n\nInstall\nImport\nExample\nValidation\nOther languages\n\nInstall\npip install korean_lunar_calendar\nImport\nfrom korean_lunar_calendar import KoreanLunarCalendar\nExample\nKorean Solar Date -> Korean Lunar Date (\uc591\ub825 -> \uc74c\ub825)\ncalendar = KoreanLunarCalendar()\n\n# params : year(\ub144), month(\uc6d4), day(\uc77c)\ncalendar.setSolarDate(2017, 6, 24)\n\n# Lunar Date (ISO Format)\nprint(calendar.LunarIsoFormat())\n\n# Korean GapJa String\nprint(calendar.getGapJaString())\n\n# Chinese GapJa String\nprint(calendar.getChineseGapJaString())\n[Result]\n2017-05-01 Intercalation\n\uc815\uc720\ub144 \ubcd1\uc624\uc6d4 \uc784\uc624\uc77c (\uc724\uc6d4)\n\u4e01\u9149\u5e74 \u4e19\u5348\u6708 \u58ec\u5348\u65e5 (\u958f\u6708)\n\nKorean Lunar Date -> Korean Solar Date (\uc74c\ub825 -> \uc591\ub825)\ncalendar = KoreanLunarCalendar()\n\n# params : year(\ub144), month(\uc6d4), day(\uc77c), intercalation(\uc724\ub2ec\uc5ec\ubd80)\ncalendar.setLunarDate(1956, 1, 21, False)\n\n# Solar Date (ISO Format)\nprint(calendar.SolarIsoFormat())\n\n# Korean GapJa String\nprint(calendar.getGapJaString())\n\n# Chinese GapJa String\nprint(calendar.getChineseGapJaString())\n[Result]\n1956-03-03\n\ubcd1\uc2e0\ub144 \uacbd\uc778\uc6d4 \uae30\uc0ac\uc77c\n\u4e19\u7533\u5e74 \u5e9a\u5bc5\u6708 \u5df1\u5df3\u65e5\n\nValidation\nCheck for invalid date input\ncalendar = KoreanLunarCalendar()\n\n# invald date\ncalendar.setLunarDate(99, 1, 1, False) # => return False\ncalendar.setSolarDate(2051, 1, 1) # => return False\n\n# OK\ncalendar.setLunarDate(1000, 1, 1, False) # => return True\ncalendar.setSolarDate(2050, 12, 31) # => return True\nOther languages\n\nJava : https://github.com/usingsky/KoreanLunarCalendar\nPython : https://github.com/usingsky/korean_lunar_calendar_py\nJavascript : https://github.com/usingsky/korean_lunar_calendar_js\n\n\n\n"}, {"name": "jupyterlab-server", "readme": "\n\n\n\n\n\n\n\n\n\n\n\njupyterlab server\nMotivation\nInstall\nUsage\nExtending the Application\nContribution\n\n\n\n\n\nREADME.md\n\n\n\n\njupyterlab server\n\n\nMotivation\nJupyterLab Server sits between JupyterLab and Jupyter Server, and provides a\nset of REST API handlers and utilities that are used by JupyterLab. It is a separate project in order to\naccommodate creating JupyterLab-like applications from a more limited scope.\nInstall\npip install jupyterlab_server\nTo include optional openapi dependencies, use:\npip install jupyterlab_server[openapi]\nTo include optional pytest_plugin dependencies, use:\npip install jupyterlab_server[test]\nUsage\nSee the full documentation for API docs and REST endpoint descriptions.\nExtending the Application\nSubclass the LabServerApp and provide additional traits and handlers as appropriate for your application.\nContribution\nPlease see CONTRIBUTING.md for details.\n\n\n"}, {"name": "jupyterlab-pygments", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nJupyterLab Pygments Theme\nScreencast\nInstallation\nDependencies\nLimitations\nLicense\n\n\n\n\n\nREADME.md\n\n\n\n\nJupyterLab Pygments Theme\nThis package contains a syntax coloring theme for pygments making use of\nthe JupyterLab CSS variables.\nThe goal is to enable the use of JupyterLab's themes with pygments-generated HTML.\nScreencast\nIn the following screencast, we demonstrate how Pygments-highlighted code can make use of the JupyterLab theme.\n\nInstallation\njupyterlab_pygments can be installed with the conda package manager\nconda install -c conda-forge jupyterlab_pygments\n\nor from pypi\npip install jupyterlab_pygments\n\nDependencies\n\njupyterlab_pygments requires pygments version 2.4.1.\nThe CSS variables used by the theme correspond to the CodeMirror syntex coloring\ntheme defined in the NPM package @jupyterlab/codemirror. Supported versions for @jupyterlab/codemirror's CSS include 0.19.1, ^1.0, and, ^2.0.\n\nLimitations\nPygments-generated HTML and CSS classes are not granular enough to reproduce\nall of the details of codemirror (the JavaScript text editor used by JupyterLab).\nThis includes the ability to differentiate properties from general names.\nLicense\njupyterlab_pygments uses a shared copyright model that enables all contributors to maintain the\ncopyright on their contributions. All code is licensed under the terms of the revised BSD license.\n\n\n"}, {"name": "jupyter-server", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJupyter Server\nInstallation and Basic usage\nVersioning and Branches\nUsage - Running Jupyter Server\nRunning in a local installation\nTesting\nContributing\nTeam Meetings and Roadmap\nAbout the Jupyter Development Team\nOur Copyright Policy\n\n\n\n\n\nREADME.md\n\n\n\n\nJupyter Server\n\n\nThe Jupyter Server provides the backend (i.e. the core services, APIs, and REST endpoints) for Jupyter web applications like Jupyter notebook, JupyterLab, and Voila.\nFor more information, read our documentation here.\nInstallation and Basic usage\nTo install the latest release locally, make sure you have\npip installed and run:\npip install jupyter_server\n\nJupyter Server currently supports Python>=3.6 on Linux, OSX and Windows.\nVersioning and Branches\nIf Jupyter Server is a dependency of your project/application, it is important that you pin it to a version that works for your application. Currently, Jupyter Server only has minor and patch versions. Different minor versions likely include API-changes while patch versions do not change API.\nWhen a new minor version is released on PyPI, a branch for that version will be created in this repository, and the version of the main branch will be bumped to the next minor version number. That way, the main branch always reflects the latest un-released version.\nTo see the changes between releases, checkout the CHANGELOG.\nUsage - Running Jupyter Server\nRunning in a local installation\nLaunch with:\njupyter server\n\nTesting\nSee CONTRIBUTING.\nContributing\nIf you are interested in contributing to the project, see CONTRIBUTING.rst.\nTeam Meetings and Roadmap\n\nWhen: Thursdays 8:00am, Pacific time\nWhere: Jovyan Zoom\nWhat: Meeting notes\n\nSee our tentative roadmap here.\nAbout the Jupyter Development Team\nThe Jupyter Development Team is the set of all contributors to the Jupyter project.\nThis includes all of the Jupyter subprojects.\nThe core team that coordinates development on GitHub can be found here:\nhttps://github.com/jupyter/.\nOur Copyright Policy\nJupyter uses a shared copyright model. Each contributor maintains copyright\nover their contributions to Jupyter. But, it is important to note that these\ncontributions are typically only changes to the repositories. Thus, the Jupyter\nsource code, in its entirety is not the copyright of any single person or\ninstitution. Instead, it is the collective copyright of the entire Jupyter\nDevelopment Team. If individual contributors want to maintain a record of what\nchanges/contributions they have specific copyright on, they should indicate\ntheir copyright in the commit message of the change, when they commit the\nchange to one of the Jupyter repositories.\nWith this in mind, the following banner should be used in any source code file\nto indicate the copyright and license terms:\n# Copyright (c) Jupyter Development Team.\n# Distributed under the terms of the Modified BSD License.\n\n\n\n"}, {"name": "jupyter-core", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nJupyter Core\nDevelopment Setup\nCoding\nCode Styling\nDocumentation\nAbout the Jupyter Development Team\nOur Copyright Policy\n\n\n\n\n\nREADME.md\n\n\n\n\nJupyter Core\n\n\nCore common functionality of Jupyter projects.\nThis package contains base application classes and configuration inherited by other projects.\nIt doesn't do much on its own.\nDevelopment Setup\nThe Jupyter Contributor Guides provide extensive information on contributing code or documentation to Jupyter projects. The limited instructions below for setting up a development environment are for your convenience.\nCoding\nYou'll need Python and pip on the search path. Clone the Jupyter Core git repository to your computer, for example in /my/projects/jupyter_core.\nNow create an editable install\nand download the dependencies of code and test suite by executing:\ncd /my/projects/jupyter_core/\npip install -e \".[test]\"\npy.test\n\nThe last command runs the test suite to verify the setup. During development, you can pass filenames to py.test, and it will execute only those tests.\nCode Styling\njupyter_core has adopted automatic code formatting so you shouldn't\nneed to worry too much about your code style.\nAs long as your code is valid,\nthe pre-commit hook should take care of how it should look.\npre-commit and its associated hooks will automatically be installed when\nyou run pip install -e \".[test]\"\nTo install pre-commit manually, run the following:\n    pip install pre-commit\n    pre-commit install\nYou can invoke the pre-commit hook by hand at any time with:\n    pre-commit run\nwhich should run any autoformatting on your code\nand tell you about any errors it couldn't fix automatically.\nYou may also install black integration\ninto your text editor to format code automatically.\nIf you have already committed files before setting up the pre-commit\nhook with pre-commit install, you can fix everything up using\npre-commit run --all-files. You need to make the fixing commit\nyourself after that.\nDocumentation\nThe documentation of Jupyter Core is generated from the files in docs/ using Sphinx. Instructions for setting up Sphinx with a selection of optional modules are in the Documentation Guide. You'll also need the make command.\nFor a minimal Sphinx installation to process the Jupyter Core docs, execute:\npip install sphinx\n\nThe following commands build the documentation in HTML format and check for broken links:\ncd /my/projects/jupyter_core/docs/\nmake html linkcheck\n\nPoint your browser to the following URL to access the generated documentation:\nfile:///my/projects/jupyter_core/docs/_build/html/index.html\nAbout the Jupyter Development Team\nThe Jupyter Development Team is the set of all contributors to the Jupyter\nproject. This includes all of the Jupyter subprojects. A full list with\ndetails is kept in the documentation directory, in the file\nabout/credits.txt.\nThe core team that coordinates development on GitHub can be found here:\nhttps://github.com/ipython/.\nOur Copyright Policy\nJupyter uses a shared copyright model. Each contributor maintains copyright\nover their contributions to Jupyter. It is important to note that these\ncontributions are typically only changes to the repositories. Thus, the Jupyter\nsource code in its entirety is not the copyright of any single person or\ninstitution. Instead, it is the collective copyright of the entire Jupyter\nDevelopment Team. If individual contributors want to maintain a record of what\nchanges/contributions they have specific copyright on, they should indicate\ntheir copyright in the commit message of the change, when they commit the\nchange to one of the Jupyter repositories.\nWith this in mind, the following banner should be used in any source code file\nto indicate the copyright and license terms:\n# Copyright (c) Jupyter Development Team.\n# Distributed under the terms of the Modified BSD License.\n\n\n\n"}, {"name": "jupyter-client", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nJupyter Client\nDevelopment Setup\nCoding\nDocumentation\nContributing\nAbout the Jupyter Development Team\nOur Copyright Policy\n\n\n\n\n\nREADME.md\n\n\n\n\nJupyter Client\n\n\njupyter_client contains the reference implementation of the Jupyter protocol.\nIt also provides client and kernel management APIs for working with kernels.\nIt also provides the jupyter kernelspec entrypoint\nfor installing kernelspecs for use with Jupyter frontends.\nDevelopment Setup\nThe Jupyter Contributor Guides provide extensive information on contributing code or documentation to Jupyter projects. The limited instructions below for setting up a development environment are for your convenience.\nCoding\nYou'll need Python and pip on the search path. Clone the Jupyter Client git repository to your computer, for example in /my/project/jupyter_client\ncd /my/projects/\ngit clone git@github.com:jupyter/jupyter_client.git\nNow create an editable install\nand download the dependencies of code and test suite by executing:\ncd /my/projects/jupyter_client/\npip install -e \".[test]\"\npytest\nThe last command runs the test suite to verify the setup. During development, you can pass filenames to pytest, and it will execute only those tests.\nDocumentation\nThe documentation of Jupyter Client is generated from the files in docs/ using Sphinx. Instructions for setting up Sphinx with a selection of optional modules are in the Documentation Guide. You'll also need the make command.\nFor a minimal Sphinx installation to process the Jupyter Client docs, execute:\npip install \".[doc]\"\nThe following commands build the documentation in HTML format and check for broken links:\ncd /my/projects/jupyter_client/docs/\nmake html linkcheck\nPoint your browser to the following URL to access the generated documentation:\nfile:///my/projects/jupyter_client/docs/_build/html/index.html\nContributing\njupyter-client has adopted automatic code formatting so you shouldn't\nneed to worry too much about your code style.\nAs long as your code is valid,\nthe pre-commit hook should take care of how it should look.\nYou can invoke the pre-commit hook by hand at any time with:\npre-commit run\nwhich should run any autoformatting on your code\nand tell you about any errors it couldn't fix automatically.\nYou may also install black integration\ninto your text editor to format code automatically.\nIf you have already committed files before setting up the pre-commit\nhook with pre-commit install, you can fix everything up using\npre-commit run --all-files. You need to make the fixing commit\nyourself after that.\nSome of the hooks only run on CI by default, but you can invoke them by\nrunning with the --hook-stage manual argument.\nAbout the Jupyter Development Team\nThe Jupyter Development Team is the set of all contributors to the Jupyter project.\nThis includes all of the Jupyter subprojects.\nThe core team that coordinates development on GitHub can be found here:\nhttps://github.com/jupyter/.\nOur Copyright Policy\nJupyter uses a shared copyright model. Each contributor maintains copyright\nover their contributions to Jupyter. But, it is important to note that these\ncontributions are typically only changes to the repositories. Thus, the Jupyter\nsource code, in its entirety is not the copyright of any single person or\ninstitution. Instead, it is the collective copyright of the entire Jupyter\nDevelopment Team. If individual contributors want to maintain a record of what\nchanges/contributions they have specific copyright on, they should indicate\ntheir copyright in the commit message of the change, when they commit the\nchange to one of the Jupyter repositories.\nWith this in mind, the following banner should be used in any source code file\nto indicate the copyright and license terms:\n# Copyright (c) Jupyter Development Team.\n# Distributed under the terms of the Modified BSD License.\n\n\n\n"}, {"name": "jsonschema-specifications", "readme": "\n   \nJSON support files from the JSON Schema Specifications (metaschemas, vocabularies, etc.), packaged for runtime access from Python as a referencing-based Schema Registry.\n"}, {"name": "ipython-genutils", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nWarehouse\nGetting Started\nDiscussion\nTesting\nCode of Conduct\n\n\n\n\n\nREADME.rst\n\n\n\n\nWarehouse\nWarehouse is the software that powers PyPI.\nSee our development roadmap, documentation, and\narchitectural overview.\n\nGetting Started\nYou can run Warehouse locally in a development environment using\ndocker. See Getting started\ndocumentation for instructions on how to set it up.\nThe canonical deployment of Warehouse is in production at pypi.org.\n\nDiscussion\nYou can find help or get involved on:\n\nGithub issue tracker for reporting issues\nIRC: on Libera, channel #pypa for general packaging discussion\nand user support, and #pypa-dev for\ndiscussions about development of packaging tools\nThe PyPA Discord for live discussions\nThe Packaging category on Discourse for discussing\nnew ideas and community initiatives\n\n\nTesting\nRead the running tests and linters section of our documentation to\nlearn how to test your code.  For cross-browser testing, we use an\nopen source account from BrowserStack. If your pull request makes\nany change to the user interface, it will need to be tested to confirm\nit works in our supported browsers.\n\n\nCode of Conduct\nEveryone interacting in the Warehouse project's codebases, issue trackers, chat\nrooms, and mailing lists is expected to follow the PSF Code of Conduct.\n\n\n"}, {"name": "ipykernel", "readme": "\nIPython Kernel for Jupyter\n\n\nThis package provides the IPython kernel for Jupyter.\nInstallation from source\n\ngit clone\ncd ipykernel\npip install -e \".[test]\"\n\nAfter that, all normal ipython commands will use this newly-installed version of the kernel.\nRunning tests\nFollow the instructions from Installation from source.\nand then from the root directory\npytest ipykernel\n\nRunning tests with coverage\nFollow the instructions from Installation from source.\nand then from the root directory\npytest ipykernel -vv -s --cov ipykernel --cov-branch --cov-report term-missing:skip-covered --durations 10\n\nAbout the IPython Development Team\nThe IPython Development Team is the set of all contributors to the IPython project.\nThis includes all of the IPython subprojects.\nThe core team that coordinates development on GitHub can be found here:\nhttps://github.com/ipython/.\nOur Copyright Policy\nIPython uses a shared copyright model. Each contributor maintains copyright\nover their contributions to IPython. But, it is important to note that these\ncontributions are typically only changes to the repositories. Thus, the IPython\nsource code, in its entirety is not the copyright of any single person or\ninstitution. Instead, it is the collective copyright of the entire IPython\nDevelopment Team. If individual contributors want to maintain a record of what\nchanges/contributions they have specific copyright on, they should indicate\ntheir copyright in the commit message of the change, when they commit the\nchange to one of the IPython repositories.\nWith this in mind, the following banner should be used in any source code file\nto indicate the copyright and license terms:\n# Copyright (c) IPython Development Team.\n# Distributed under the terms of the Modified BSD License.\n\n"}, {"name": "importlib-resources", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nCompatibility\nFor Enterprise\n\n\n\n\n\nREADME.rst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimportlib_resources is a backport of Python standard library\nimportlib.resources\nmodule for older Pythons.\nThe key goal of this module is to replace parts of pkg_resources with a\nsolution in Python's stdlib that relies on well-defined APIs.  This makes\nreading resources included in packages easier, with more stable and consistent\nsemantics.\n\nCompatibility\nNew features are introduced in this third-party library and later merged\ninto CPython. The following table indicates which versions of this library\nwere contributed to different versions in the standard library:\n\n\nimportlib_resources\nstdlib\n\n\n\n6.0\n3.13\n\n5.12\n3.12\n\n5.7\n3.11\n\n5.0\n3.10\n\n1.3\n3.9\n\n0.5 (?)\n3.7\n\n\n\n\nFor Enterprise\nAvailable as part of the Tidelift Subscription.\nThis project and the maintainers of thousands of other packages are working with Tidelift to deliver one enterprise subscription that covers all of the open source you use.\nLearn more.\n\n\n"}, {"name": "importlib-metadata", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nCompatibility\nUsage\nCaveats\nProject details\nFor Enterprise\n\n\n\n\n\nREADME.rst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLibrary to access the metadata for a Python package.\nThis package supplies third-party access to the functionality of\nimportlib.metadata\nincluding improvements added to subsequent Python versions.\n\nCompatibility\nNew features are introduced in this third-party library and later merged\ninto CPython. The following table indicates which versions of this library\nwere contributed to different versions in the standard library:\n\n\nimportlib_metadata\nstdlib\n\n\n\n6.5\n3.12\n\n4.13\n3.11\n\n4.6\n3.10\n\n1.4\n3.8\n\n\n\n\nUsage\nSee the online documentation\nfor usage details.\nFinder authors can\nalso add support for custom package installers.  See the above documentation\nfor details.\n\nCaveats\nThis project primarily supports third-party packages installed by PyPA\ntools (or other conforming packages). It does not support:\n\nPackages in the stdlib.\nPackages installed without metadata.\n\n\nProject details\n\n\nProject home: https://github.com/python/importlib_metadata\nReport bugs at: https://github.com/python/importlib_metadata/issues\nCode hosting: https://github.com/python/importlib_metadata\nDocumentation: https://importlib-metadata.readthedocs.io/\n\n\n\nFor Enterprise\nAvailable as part of the Tidelift Subscription.\nThis project and the maintainers of thousands of other packages are working with Tidelift to deliver one enterprise subscription that covers all of the open source you use.\nLearn more.\n\n\n"}, {"name": "imageio-ffmpeg", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimageio-ffmpeg\nPurpose\nInstallation\nExample usage\nHow it works\nimageio-ffmpeg for enterprise\nSecurity contact information\nEnvironment variables\nDevelopers\nAPI\n\n\n\n\n\nREADME.md\n\n\n\n\nimageio-ffmpeg\n\n\nFFMPEG wrapper for Python\nPurpose\nThe purpose of this project is to provide a simple and reliable ffmpeg\nwrapper for working with video files. It implements two simple generator\nfunctions for reading and writing data from/to ffmpeg, which reliably\nterminate the ffmpeg process when done. It also takes care of publishing\nplatform-specific wheels that include the binary ffmpeg executables.\nThis library is used as the basis for the\nimageio\nffmpeg plugin,\nbut it can also be used by itself. Imageio provides a higher level API,\nand adds support for e.g. cameras and seeking.\nInstallation\nThis library works with any version of Python 3.5+ (including Pypy).\nThere are no further dependencies. The wheels on Pypi include the ffmpeg\nexecutable for all common platforms (Windows 7+, Linux kernel 2.6.32+,\nOSX 10.9+). Install using:\n$ pip install --upgrade imageio-ffmpeg\n\n(On Linux you may want to first pip install -U pip, since pip 19 is needed to detect the manylinux2010 wheels.)\nIf you're using a Conda environment: the conda package does not include\nthe ffmpeg executable, but instead depends on the ffmpeg package from\nconda-forge. Install using:\n$ conda install imageio-ffmpeg -c conda-forge\n\nIf you don't want to install the included ffmpeg, you can use pip with\n--no-binary or conda with --no-deps. Then use the\nIMAGEIO_FFMPEG_EXE environment variable if needed.\nExample usage\nThe imageio_ffmpeg library provides low level functionality to read\nand write video data, using Python generators:\n# Read a video file\nreader = read_frames(path)\nmeta = reader.__next__()  # meta data, e.g. meta[\"size\"] -> (width, height)\nfor frame in reader:\n    ... # each frame is a bytes object\n\n# Write a video file\nwriter = write_frames(path, size)  # size is (width, height)\nwriter.send(None)  # seed the generator\nfor frame in frames:\n    writer.send(frame)\nwriter.close()  # don't forget this\n(Also see the API section further down.)\nHow it works\nThis library calls ffmpeg in a subprocess, and video frames are\ncommunicated over pipes. This is certainly not the fastest way to\nuse ffmpeg, but it makes it possible to wrap ffmpeg with pure Python,\nmaking distribution and installation much easier. And probably\nthe code itself too. In contrast, PyAV\nwraps ffmpeg at the C level.\nNote that because of how imageio-ffmpeg works, read_frames() and\nwrite_frames() only accept file names, and not file (like) objects.\nimageio-ffmpeg for enterprise\nAvailable as part of the Tidelift Subscription\nThe maintainers of imageio-ffmpeg and thousands of other packages are working with Tidelift to deliver commercial support and maintenance for the open source dependencies you use to build your applications. Save time, reduce risk, and improve code health, while paying the maintainers of the exact dependencies you use. Learn more.\nSecurity contact information\nTo report a security vulnerability, please use the\nTidelift security contact.\nTidelift will coordinate the fix and disclosure.\nEnvironment variables\nThe library can be configured at runtime by setting the following environment\nvariables:\n\nIMAGEIO_FFMPEG_EXE=[file name] -- override the ffmpeg executable;\nIMAGEIO_FFMPEG_NO_PREVENT_SIGINT=1 -- don't prevent propagation of SIGINT\nto the ffmpeg process.\n\nDevelopers\nDev deps:\npip install invoke black flake8\n\nWe use invoke:\ninvoke autoformat\ninvoke lint\ninvoke -l  # to get a list of all tasks\ninvoke update-readme  # after changes to the docstrings\n\nAPI\ndef read_frames(\n    path,\n    pix_fmt=\"rgb24\",\n    bpp=None,\n    input_params=None,\n    output_params=None,\n    bits_per_pixel=None,\n):\n    \"\"\"\n    Create a generator to iterate over the frames in a video file.\n\n    It first yields a small metadata dictionary that contains:\n\n    * ffmpeg_version: the ffmpeg version in use (as a string).\n    * codec: a hint about the codec used to encode the video, e.g. \"h264\".\n    * source_size: the width and height of the encoded video frames.\n    * size: the width and height of the frames that will be produced.\n    * fps: the frames per second. Can be zero if it could not be detected.\n    * duration: duration in seconds. Can be zero if it could not be detected.\n\n    After that, it yields frames until the end of the video is reached. Each\n    frame is a bytes object.\n\n    This function makes no assumptions about the number of frames in\n    the data. For one because this is hard to predict exactly, but also\n    because it may depend on the provided output_params. If you want\n    to know the number of frames in a video file, use count_frames_and_secs().\n    It is also possible to estimate the number of frames from the fps and\n    duration, but note that even if both numbers are present, the resulting\n    value is not always correct.\n\n    Example:\n\n        gen = read_frames(path)\n        meta = gen.__next__()\n        for frame in gen:\n            print(len(frame))\n\n    Parameters:\n        path (str): the filename of the file to read from.\n        pix_fmt (str): the pixel format of the frames to be read.\n            The default is \"rgb24\" (frames are uint8 RGB images).\n        input_params (list): Additional ffmpeg input command line parameters.\n        output_params (list): Additional ffmpeg output command line parameters.\n        bits_per_pixel (int): The number of bits per pixel in the output frames.\n            This depends on the given pix_fmt. Default is 24 (RGB)\n        bpp (int): DEPRECATED, USE bits_per_pixel INSTEAD. The number of bytes per pixel in the output frames.\n            This depends on the given pix_fmt. Some pixel formats like yuv420p have 12 bits per pixel\n            and cannot be set in bytes as integer. For this reason the bpp argument is deprecated.\n    \"\"\"\ndef write_frames(\n    path,\n    size,\n    pix_fmt_in=\"rgb24\",\n    pix_fmt_out=\"yuv420p\",\n    fps=16,\n    quality=5,\n    bitrate=None,\n    codec=None,\n    macro_block_size=16,\n    ffmpeg_log_level=\"warning\",\n    ffmpeg_timeout=None,\n    input_params=None,\n    output_params=None,\n    audio_path=None,\n    audio_codec=None,\n):\n    \"\"\"\n    Create a generator to write frames (bytes objects) into a video file.\n\n    The frames are written by using the generator's `send()` method. Frames\n    can be anything that can be written to a file. Typically these are\n    bytes objects, but c-contiguous Numpy arrays also work.\n\n    Example:\n\n        gen = write_frames(path, size)\n        gen.send(None)  # seed the generator\n        for frame in frames:\n            gen.send(frame)\n        gen.close()  # don't forget this\n\n    Parameters:\n        path (str): the filename to write to.\n        size (tuple): the width and height of the frames.\n        pix_fmt_in (str): the pixel format of incoming frames.\n            E.g. \"gray\", \"gray8a\", \"rgb24\", or \"rgba\". Default \"rgb24\".\n        pix_fmt_out (str): the pixel format to store frames. Default yuv420p\".\n        fps (float): The frames per second. Default 16.\n        quality (float): A measure for quality between 0 and 10. Default 5.\n            Ignored if bitrate is given.\n        bitrate (str): The bitrate, e.g. \"192k\". The defaults are pretty good.\n        codec (str): The codec. Default \"libx264\" for .mp4 (if available from\n            the ffmpeg executable) or \"msmpeg4\" for .wmv.\n        macro_block_size (int): You probably want to align the size of frames\n            to this value to avoid image resizing. Default 16. Can be set\n            to 1 to avoid block alignment, though this is not recommended.\n        ffmpeg_log_level (str): The ffmpeg logging level. Default \"warning\".\n        ffmpeg_timeout (float): Timeout in seconds to wait for ffmpeg process\n            to finish. Value of 0 or None will wait forever (default). The time that\n            ffmpeg needs depends on CPU speed, compression, and frame size.\n        input_params (list): Additional ffmpeg input command line parameters.\n        output_params (list): Additional ffmpeg output command line parameters.\n        audio_path (str): A input file path for encoding with an audio stream.\n            Default None, no audio.\n        audio_codec (str): The audio codec to use if audio_path is provided.\n            \"copy\" will try to use audio_path's audio codec without re-encoding.\n            Default None, but some formats must have certain codecs specified.\n    \"\"\"\ndef count_frames_and_secs(path):\n    \"\"\"\n    Get the number of frames and number of seconds for the given video\n    file. Note that this operation can be quite slow for large files.\n\n    Disclaimer: I've seen this produce different results from actually reading\n    the frames with older versions of ffmpeg (2.x). Therefore I cannot say\n    with 100% certainty that the returned values are always exact.\n    \"\"\"\ndef get_ffmpeg_exe():\n    \"\"\"\n    Get the ffmpeg executable file. This can be the binary defined by\n    the IMAGEIO_FFMPEG_EXE environment variable, the binary distributed\n    with imageio-ffmpeg, an ffmpeg binary installed with conda, or the\n    system ffmpeg (in that order). A RuntimeError is raised if no valid\n    ffmpeg could be found.\n    \"\"\"\ndef get_ffmpeg_version():\n    \"\"\"\n    Get the version of the used ffmpeg executable (as a string).\n    \"\"\"\n\n\n"}, {"name": "Flask-Login", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nFlask-Login\nInstallation\nUsage\nContributing\n\n\n\n\n\nREADME.md\n\n\n\n\nFlask-Login\n\n\n\nFlask-Login provides user session management for Flask. It handles the common\ntasks of logging in, logging out, and remembering your users' sessions over\nextended periods of time.\nFlask-Login is not bound to any particular database system or permissions\nmodel. The only requirement is that your user objects implement a few methods,\nand that you provide a callback to the extension capable of loading users from\ntheir ID.\nInstallation\nInstall the extension with pip:\n$ pip install flask-login\nUsage\nOnce installed, the Flask-Login is easy to use. Let's walk through setting up\na basic application. Also please note that this is a very basic guide: we will\nbe taking shortcuts here that you should never take in a real application.\nTo begin we'll set up a Flask app:\nimport flask\n\napp = flask.Flask(__name__)\napp.secret_key = 'super secret string'  # Change this!\nFlask-Login works via a login manager. To kick things off, we'll set up the\nlogin manager by instantiating it and telling it about our Flask app:\nimport flask_login\n\nlogin_manager = flask_login.LoginManager()\n\nlogin_manager.init_app(app)\nTo keep things simple we're going to use a dictionary to represent a database\nof users. In a real application, this would be an actual persistence layer.\nHowever it's important to point out this is a feature of Flask-Login: it\ndoesn't care how your data is stored so long as you tell it how to retrieve it!\n# Our mock database.\nusers = {'foo@bar.tld': {'password': 'secret'}}\nWe also need to tell Flask-Login how to load a user from a Flask request and\nfrom its session. To do this we need to define our user object, a\nuser_loader callback, and a request_loader callback.\nclass User(flask_login.UserMixin):\n    pass\n\n\n@login_manager.user_loader\ndef user_loader(email):\n    if email not in users:\n        return\n\n    user = User()\n    user.id = email\n    return user\n\n\n@login_manager.request_loader\ndef request_loader(request):\n    email = request.form.get('email')\n    if email not in users:\n        return\n\n    user = User()\n    user.id = email\n    return user\nNow we're ready to define our views. We can start with a login view, which will\npopulate the session with authentication bits. After that we can define a view\nthat requires authentication.\n@app.route('/login', methods=['GET', 'POST'])\ndef login():\n    if flask.request.method == 'GET':\n        return '''\n               <form action='login' method='POST'>\n                <input type='text' name='email' id='email' placeholder='email'/>\n                <input type='password' name='password' id='password' placeholder='password'/>\n                <input type='submit' name='submit'/>\n               </form>\n               '''\n\n    email = flask.request.form['email']\n    if email in users and flask.request.form['password'] == users[email]['password']:\n        user = User()\n        user.id = email\n        flask_login.login_user(user)\n        return flask.redirect(flask.url_for('protected'))\n\n    return 'Bad login'\n\n\n@app.route('/protected')\n@flask_login.login_required\ndef protected():\n    return 'Logged in as: ' + flask_login.current_user.id\nFinally we can define a view to clear the session and log users out:\n@app.route('/logout')\ndef logout():\n    flask_login.logout_user()\n    return 'Logged out'\nWe now have a basic working application that makes use of session-based\nauthentication. To round things off, we should provide a callback for login\nfailures:\n@login_manager.unauthorized_handler\ndef unauthorized_handler():\n    return 'Unauthorized', 401\nDocumentation for Flask-Login is available on ReadTheDocs.\nFor complete understanding of available configuration, please refer to the source code.\nContributing\nWe welcome contributions! If you would like to hack on Flask-Login, please\nfollow these steps:\n\nFork this repository\nMake your changes\nInstall the dev requirements with pip install -r requirements/dev.txt\nSubmit a pull request after running tox (ensure it does not error!)\n\nPlease give us adequate time to review your submission. Thanks!\n\n\n"}, {"name": "Flask-Cors", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFlask-CORS\nInstallation\nUsage\nSimple Usage\nResource specific CORS\nRoute specific CORS via decorator\nDocumentation\nTroubleshooting\nTests\nContributing\nCredits\n\n\n\n\n\nREADME.rst\n\n\n\n\nFlask-CORS\n\n \n \n\n\n\nA Flask extension for handling Cross Origin Resource Sharing (CORS), making cross-origin AJAX possible.\nThis package has a simple philosophy: when you want to enable CORS, you wish to enable it for all use cases on a domain.\nThis means no mucking around with different allowed headers, methods, etc.\nBy default, submission of cookies across domains is disabled due to the security implications.\nPlease see the documentation for how to enable credential'ed requests, and please make sure you add some sort of CSRF protection before doing so!\n\nInstallation\nInstall the extension with using pip, or easy_install.\n$ pip install -U flask-cors\n\nUsage\nThis package exposes a Flask extension which by default enables CORS support on all routes, for all origins and methods.\nIt allows parameterization of all CORS headers on a per-resource level.\nThe package also contains a decorator, for those who prefer this approach.\n\nSimple Usage\nIn the simplest case, initialize the Flask-Cors extension with default arguments in order to allow CORS for all domains on all routes.\nSee the full list of options in the documentation.\nfrom flask import Flask\nfrom flask_cors import CORS\n\napp = Flask(__name__)\nCORS(app)\n\n@app.route(\"/\")\ndef helloWorld():\n  return \"Hello, cross-origin-world!\"\n\nResource specific CORS\nAlternatively, you can specify CORS options on a resource and origin level of granularity by passing a dictionary as the resources option, mapping paths to a set of options.\nSee the full list of options in the documentation.\napp = Flask(__name__)\ncors = CORS(app, resources={r\"/api/*\": {\"origins\": \"*\"}})\n\n@app.route(\"/api/v1/users\")\ndef list_users():\n  return \"user example\"\n\nRoute specific CORS via decorator\nThis extension also exposes a simple decorator to decorate flask routes with.\nSimply add @cross_origin() below a call to Flask's @app.route(..) to allow CORS on a given route.\nSee the full list of options in the decorator documentation.\n@app.route(\"/\")\n@cross_origin()\ndef helloWorld():\n  return \"Hello, cross-origin-world!\"\n\nDocumentation\nFor a full list of options, please see the full documentation\n\nTroubleshooting\nIf things aren't working as you expect, enable logging to help understand what is going on under the hood, and why.\nlogging.getLogger('flask_cors').level = logging.DEBUG\n\nTests\nA simple set of tests is included in test/.\nTo run, install nose, and simply invoke nosetests or python setup.py test to exercise the tests.\nIf nosetests does not work for you, due to it no longer working with newer python versions.\nYou can use pytest to run the tests instead.\n\nContributing\nQuestions, comments or improvements?\nPlease create an issue on Github, tweet at @corydolphin or send me an email.\nI do my best to include every contribution proposed in any way that I can.\n\nCredits\nThis Flask extension is based upon the Decorator for the HTTP Access Control written by Armin Ronacher.\n\n\n"}, {"name": "Flask-CacheBuster", "readme": "\n\nflask-cachebuster\nFlask-CacheBuster is a lightweight http://flask.pocoo.org/ extension that adds a hash to the URL query parameters of each static file. This lets you safely declare your static resources as indefinitely cacheable because they automatically get new URLs when their contents change.\n\n\nNotes:\nInspired by https://github.com/ChrisTM/Flask-CacheBust, and an updated version of https://github.com/daxlab/Flask-Cache-Buster to work with python 3.+\n\n\nInstallation\nUsing pip:\npip install flask-cachebuster\n\nUsage\nConfiguration:\nfrom flask_cachebuster import CacheBuster\n\nconfig = { 'extensions': ['.js', '.css', '.csv'], 'hash_size': 5 }\n\ncache_buster = CacheBuster(config=config)\n\ncache_buster.init_app(app)\n\n\nConfiguration\nConfiguration:\n* extensions - file extensions to bust\n* hash_size - looks something like this `/static/index.css%3Fq3` where [%3Fq3] is the hash size.\nThe http://flask.pocoo.org/docs/0.12/api/#flask.url_for function will now cache-bust your static files. For example, this template:\n<script src=\"{{ url_for('static', filename='js/main.js') }}\"></script>\nwill render like this:\n<script src=\"/static/js/main.js?%3Fq%3Dc5b5b2fa19\"></script>\n\n\n"}, {"name": "ffmpeg-python", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nffmpeg-python: Python bindings for FFmpeg\nOverview\nQuickstart\nAPI reference\nComplex filter graphs\nInstallation\nInstalling ffmpeg-python\nInstalling FFmpeg\nExamples\nCustom Filters\nFrequently asked questions\nContributing\nRunning tests\nSpecial thanks\nAdditional Resources\n\n\n\n\n\nREADME.md\n\n\n\n\nffmpeg-python: Python bindings for FFmpeg\n\n\nOverview\nThere are tons of Python FFmpeg wrappers out there but they seem to lack complex filter support.  ffmpeg-python works well for simple as well as complex signal graphs.\nQuickstart\nFlip a video horizontally:\nimport ffmpeg\nstream = ffmpeg.input('input.mp4')\nstream = ffmpeg.hflip(stream)\nstream = ffmpeg.output(stream, 'output.mp4')\nffmpeg.run(stream)\nOr if you prefer a fluent interface:\nimport ffmpeg\n(\n    ffmpeg\n    .input('input.mp4')\n    .hflip()\n    .output('output.mp4')\n    .run()\n)\nAPI reference\nComplex filter graphs\nFFmpeg is extremely powerful, but its command-line interface gets really complicated rather quickly - especially when working with signal graphs and doing anything more than trivial.\nTake for example a signal graph that looks like this:\n\nThe corresponding command-line arguments are pretty gnarly:\nffmpeg -i input.mp4 -i overlay.png -filter_complex \"[0]trim=start_frame=10:end_frame=20[v0];\\\n    [0]trim=start_frame=30:end_frame=40[v1];[v0][v1]concat=n=2[v2];[1]hflip[v3];\\\n    [v2][v3]overlay=eof_action=repeat[v4];[v4]drawbox=50:50:120:120:red:t=5[v5]\"\\\n    -map [v5] output.mp4\nMaybe this looks great to you, but if you're not an FFmpeg command-line expert, it probably looks alien.\nIf you're like me and find Python to be powerful and readable, it's easier with ffmpeg-python:\nimport ffmpeg\n\nin_file = ffmpeg.input('input.mp4')\noverlay_file = ffmpeg.input('overlay.png')\n(\n    ffmpeg\n    .concat(\n        in_file.trim(start_frame=10, end_frame=20),\n        in_file.trim(start_frame=30, end_frame=40),\n    )\n    .overlay(overlay_file.hflip())\n    .drawbox(50, 50, 120, 120, color='red', thickness=5)\n    .output('out.mp4')\n    .run()\n)\nffmpeg-python takes care of running ffmpeg with the command-line arguments that correspond to the above filter diagram, in familiar Python terms.\n\nReal-world signal graphs can get a heck of a lot more complex, but ffmpeg-python handles arbitrarily large (directed-acyclic) signal graphs.\nInstallation\nInstalling ffmpeg-python\nThe latest version of ffmpeg-python can be acquired via a typical pip install:\npip install ffmpeg-python\nOr the source can be cloned and installed from locally:\ngit clone git@github.com:kkroening/ffmpeg-python.git\npip install -e ./ffmpeg-python\n\nNote: ffmpeg-python makes no attempt to download/install FFmpeg, as ffmpeg-python is merely a pure-Python wrapper - whereas FFmpeg installation is platform-dependent/environment-specific, and is thus the responsibility of the user, as described below.\n\nInstalling FFmpeg\nBefore using ffmpeg-python, FFmpeg must be installed and accessible via the $PATH environment variable.\nThere are a variety of ways to install FFmpeg, such as the official download links, or using your package manager of choice (e.g. sudo apt install ffmpeg on Debian/Ubuntu, brew install ffmpeg on OS X, etc.).\nRegardless of how FFmpeg is installed, you can check if your environment path is set correctly by running the ffmpeg command from the terminal, in which case the version information should appear, as in the following example (truncated for brevity):\n$ ffmpeg\nffmpeg version 4.2.4-1ubuntu0.1 Copyright (c) 2000-2020 the FFmpeg developers\n  built with gcc 9 (Ubuntu 9.3.0-10ubuntu2)\n\n\nNote: The actual version information displayed here may vary from one system to another; but if a message such as ffmpeg: command not found appears instead of the version information, FFmpeg is not properly installed.\n\nExamples\nWhen in doubt, take a look at the examples to see if there's something that's close to whatever you're trying to do.\nHere are a few:\n\n\nConvert video to numpy array\n\n\nGenerate thumbnail for video\n\n\nRead raw PCM audio via pipe\n\n\nJupyterLab/Notebook stream editor\n\n\n\n\nTensorflow/DeepDream streaming\n\n\nSee the Examples README for additional examples.\nCustom Filters\nDon't see the filter you're looking for?  While ffmpeg-python includes shorthand notation for some of the most commonly used filters (such as concat), all filters can be referenced via the .filter operator:\nstream = ffmpeg.input('dummy.mp4')\nstream = ffmpeg.filter(stream, 'fps', fps=25, round='up')\nstream = ffmpeg.output(stream, 'dummy2.mp4')\nffmpeg.run(stream)\nOr fluently:\n(\n    ffmpeg\n    .input('dummy.mp4')\n    .filter('fps', fps=25, round='up')\n    .output('dummy2.mp4')\n    .run()\n)\nSpecial option names:\nArguments with special names such as -qscale:v (variable bitrate), -b:v (constant bitrate), etc. can be specified as a keyword-args dictionary as follows:\n(\n    ffmpeg\n    .input('in.mp4')\n    .output('out.mp4', **{'qscale:v': 3})\n    .run()\n)\nMultiple inputs:\nFilters that take multiple input streams can be used by passing the input streams as an array to ffmpeg.filter:\nmain = ffmpeg.input('main.mp4')\nlogo = ffmpeg.input('logo.png')\n(\n    ffmpeg\n    .filter([main, logo], 'overlay', 10, 10)\n    .output('out.mp4')\n    .run()\n)\nMultiple outputs:\nFilters that produce multiple outputs can be used with .filter_multi_output:\nsplit = (\n    ffmpeg\n    .input('in.mp4')\n    .filter_multi_output('split')  # or `.split()`\n)\n(\n    ffmpeg\n    .concat(split[0], split[1].reverse())\n    .output('out.mp4')\n    .run()\n)\n(In this particular case, .split() is the equivalent shorthand, but the general approach works for other multi-output filters)\nString expressions:\nExpressions to be interpreted by ffmpeg can be included as string parameters and reference any special ffmpeg variable names:\n(\n    ffmpeg\n    .input('in.mp4')\n    .filter('crop', 'in_w-2*10', 'in_h-2*20')\n    .input('out.mp4')\n)\n\nWhen in doubt, refer to the existing filters, examples, and/or the official ffmpeg documentation.\nFrequently asked questions\nWhy do I get an import/attribute/etc. error from import ffmpeg?\nMake sure you ran pip install ffmpeg-python and not pip install ffmpeg (wrong) or pip install python-ffmpeg (also wrong).\nWhy did my audio stream get dropped?\nSome ffmpeg filters drop audio streams, and care must be taken to preserve the audio in the final output.  The .audio and .video operators can be used to reference the audio/video portions of a stream so that they can be processed separately and then re-combined later in the pipeline.\nThis dilemma is intrinsic to ffmpeg, and ffmpeg-python tries to stay out of the way while users may refer to the official ffmpeg documentation as to why certain filters drop audio.\nAs usual, take a look at the examples (Audio/video pipeline in particular).\nHow can I find out the used command line arguments?\nYou can run stream.get_args() before stream.run() to retrieve the command line arguments that will be passed to ffmpeg. You can also run stream.compile() that also includes the ffmpeg executable as the first argument.\nHow do I do XYZ?\nTake a look at each of the links in the Additional Resources section at the end of this README.  If you look everywhere and can't find what you're looking for and have a question that may be relevant to other users, you may open an issue asking how to do it, while providing a thorough explanation of what you're trying to do and what you've tried so far.\nIssues not directly related to ffmpeg-python or issues asking others to write your code for you or how to do the work of solving a complex signal processing problem for you that's not relevant to other users will be closed.\nThat said, we hope to continue improving our documentation and provide a community of support for people using ffmpeg-python to do cool and exciting things.\nContributing\n\nOne of the best things you can do to help make ffmpeg-python better is to answer open questions in the issue tracker.  The questions that are answered will be tagged and incorporated into the documentation, examples, and other learning resources.\nIf you notice things that could be better in the documentation or overall development experience, please say so in the issue tracker.  And of course, feel free to report any bugs or submit feature requests.\nPull requests are welcome as well, but it wouldn't hurt to touch base in the issue tracker or hop on the Matrix chat channel first.\nAnyone who fixes any of the open bugs or implements requested enhancements is a hero, but changes should include passing tests.\nRunning tests\ngit clone git@github.com:kkroening/ffmpeg-python.git\ncd ffmpeg-python\nvirtualenv venv\n. venv/bin/activate  # (OS X / Linux)\nvenv\\bin\\activate    # (Windows)\npip install -e .[dev]\npytest\n\nSpecial thanks\n\nFabrice Bellard\nThe FFmpeg team\nArne de Laat\nDavide Depau\nDim\nNoah Stier\n\nAdditional Resources\n\nAPI Reference\nExamples\nFilters\nFFmpeg Homepage\nFFmpeg Documentation\nFFmpeg Filters Documentation\nTest cases\nIssue tracker\nMatrix Chat: #ffmpeg-python:matrix.org\n\n\n\n"}, {"name": "extract-msg", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nextract-msg\nNOTICE\nChangelog\nUsage\nError Reporting\nSupporting The Module\nInstallation\nVersioning\nTodo\nCredits\nExtra\n\n\n\n\n\nREADME.rst\n\n\n\n\n\n \n \n \n\nextract-msg\nExtracts emails and attachments saved in Microsoft Outlook's .msg files\nThe python package extract_msg automates the extraction of key email\ndata (from, to, cc, date, subject, body) and the email's attachments.\nDocumentation can be found in the code, on the wiki, and on the\nRead the Docs page.\n\nNOTICE\n0.29.* is the branch that supports both Python 2 and Python 3. It is now only\nreceiving bug fixes and will not be receiving feature updates.\n0.39.* is the last versions that supported Python 3.6 and 3.7. Support for those\nwas dropped to allow the use of new features from 3.8 and because the life spans\nof those versions had ended.\nThis module has a Discord server for general discussion. You can find it here:\nDiscord\n\nChangelog\n\nChangelog\n\n\nUsage\nTo use it as a command-line script:\npython -m extract_msg example.msg\n\nThis will produce a new folder named according to the date, time and\nsubject of the message (for example \"2013-07-24_0915 Example\"). The\nemail itself can be found inside the new folder along with the\nattachments.\nThe script uses Philippe Lagadec's Python module that reads Microsoft\nOLE2 files (also called Structured Storage, Compound File Binary Format\nor Compound Document File Format). This is the underlying format of\nOutlook's .msg files. This library currently supports Python 3.8 and above.\nThe script was originally built using Peter Fiskerstrand's documentation of the\n.msg format. Redemption's discussion of the different property types used within\nExtended MAPI was also useful. For future reference, note that Microsoft have\nopened up their documentation of the file format, which is what is currently\nbeing used for development.\n#########REWRITE COMMAND LINE USAGE#############\nCurrently, the README is in the process of being redone. For now, please\nrefer to the usage information provided from the program's help dialog:\nusage: extract_msg [-h] [--use-content-id] [--json] [--file-logging] [-v] [--log LOG] [--config CONFIGPATH] [--out OUTPATH] [--use-filename] [--dump-stdout] [--html] [--pdf] [--wk-path WKPATH] [--wk-options [WKOPTIONS ...]]\n                   [--prepared-html] [--charset CHARSET] [--raw] [--rtf] [--allow-fallback] [--skip-body-not-found] [--zip ZIP] [--save-header] [--attachments-only] [--skip-hidden] [--no-folders] [--skip-embedded] [--extract-embedded]\n                   [--overwrite-existing] [--skip-not-implemented] [--out-name OUTNAME | --glob] [--ignore-rtfde] [--progress]\n                   msg [msg ...]\n\nextract_msg: Extracts emails and attachments saved in Microsoft Outlook's .msg files. https://github.com/TeamMsgExtractor/msg-extractor\n\npositional arguments:\n  msg                   An MSG file to be parsed.\n\noptions:\n  -h, --help            show this help message and exit\n  --use-content-id, --cid\n                        Save attachments by their Content ID, if they have one. Useful when working with the HTML body.\n  --json                Changes to write output files as json.\n  --file-logging        Enables file logging. Implies --verbose level 1.\n  -v, --verbose         Turns on console logging. Specify more than once for higher verbosity.\n  --log LOG             Set the path to write the file log to.\n  --config CONFIGPATH   Set the path to load the logging config from.\n  --out OUTPATH         Set the folder to use for the program output. (Default: Current directory)\n  --use-filename        Sets whether the name of each output is based on the msg filename.\n  --dump-stdout         Tells the program to dump the message body (plain text) to stdout. Overrides saving arguments.\n  --html                Sets whether the output should be HTML. If this is not possible, will error.\n  --pdf                 Saves the body as a PDF. If this is not possible, will error.\n  --wk-path WKPATH      Overrides the path for finding wkhtmltopdf.\n  --wk-options [WKOPTIONS ...]\n                        Sets additional options to be used in wkhtmltopdf. Should be a series of options and values, replacing the - or -- in the beginning with + or ++, respectively. For example: --wk-options \"+O Landscape\"\n  --prepared-html       When used in conjunction with --html, sets whether the HTML output should be prepared for embedded attachments.\n  --charset CHARSET     Character set to use for the prepared HTML in the added tag. (Default: utf-8)\n  --raw                 Sets whether the output should be raw. If this is not possible, will error.\n  --rtf                 Sets whether the output should be RTF. If this is not possible, will error.\n  --allow-fallback      Tells the program to fallback to a different save type if the selected one is not possible.\n  --skip-body-not-found\n                        Skips saving the body if the body cannot be found, rather than throwing an error.\n  --zip ZIP             Path to use for saving to a zip file.\n  --save-header         Store the header in a separate file.\n  --attachments-only    Specify to only save attachments from an msg file.\n  --skip-hidden         Skips any attachment marked as hidden (usually ones embedded in the body).\n  --no-folders          Stores everything in the location specified by --out. Requires --attachments-only and is incompatible with --out-name.\n  --skip-embedded       Skips all embedded MSG files when saving attachments.\n  --extract-embedded    Extracts the embedded MSG files as MSG files instead of running their save functions.\n  --overwrite-existing  Disables filename conflict resolution code for attachments when saving a file, causing files to be overwriten if two attachments with the same filename are on an MSG file.\n  --skip-not-implemented, --skip-ni\n                        Skips any attachments that are not implemented, allowing saving of the rest of the message.\n  --out-name OUTNAME    Name to be used with saving the file output. Cannot be used if you are saving more than one file.\n  --glob, --wildcard    Interpret all paths as having wildcards. Incompatible with --out-name.\n  --ignore-rtfde        Ignores all errors thrown from RTFDE when trying to save. Useful for allowing fallback to continue when an exception happens.\n  --progress            Shows what file the program is currently working on during it's progress.\n\nTo use this in your own script, start by using:\nimport extract_msg\n\nFrom there, open the MSG file:\nmsg = extract_msg.openMsg(\"path/to/msg/file.msg\")\n\nAlternatively, if you wish to send a msg binary string instead of a file\nto the extract_msg.openMsg Method:\nmsg_raw = b'\\xd0\\xcf\\x11\\xe0\\xa1\\xb1\\x1a\\xe1\\x00 ... \\x00\\x00\\x00'\nmsg = extract_msg.openMsg(msg_raw)\n\nIf you want to override the default attachment class and use one of your\nown, simply change the code to:\nmsg = extract_msg.openMsg(\"path/to/msg/file.msg\", attachmentClass = CustomAttachmentClass)\n\nwhere CustomAttachmentClass is your custom class.\n#TODO: Finish this section\nIf you have any questions feel free to contact Destiny at arceusthe [at]\ngmail [dot] com. She is the co-owner and main developer of the project.\nIf you have issues, it would be best to get help for them by opening a\nnew github issue.\n\nError Reporting\nShould you encounter an error that has not already been reported, please\ndo the following when reporting it: * Make sure you are using the\nlatest version of extract_msg (check the version on PyPi). * State your\nPython version. * Include the code, if any, that you used. * Include a\ncopy of the traceback.\n\nSupporting The Module\nIf you'd like to donate to help support the development of the module, you can\ndonate to Destiny using one of the following services:\n\nBuy Me a Coffee\nKo-fi\nPatreon\n\n\nInstallation\nYou can install using pip:\n\nPypi\n\npip install extract-msg\n\nGithub\n\npip install git+https://github.com/TeamMsgExtractor/msg-extractor\nor you can include this in your list of python dependencies with:\n# setup.py\n\nsetup(\n    ...\n    dependency_links=['https://github.com/TeamMsgExtractor/msg-extractor/zipball/master'],\n)\nAdditionally, this module has the following extras which can be optionally\ninstalled:\n\nall: Installs all of the extras.\nmime: Installs dependency used for mimetype generation when a mimetype is not specified.\n\n\nVersioning\nThis module uses Semantic Versioning, however it has not always done so. All versions greater than or equal to 0.40.* conform successfully. As the package is currently in major version zero (0.*.*), anything MAY change at any time, as per point 4 of the SemVer specification. However, I, Destiny, am aware of the module's usage in other packages and code, and so I have taken efforts to make the versioning more reliable.\nAny change to the minor version MUST be considered a potentially breaking change, and the changelog should be checked before assuming the API will function in the way it did in the previous minor version. I do, however, try to keep the API relatively stable between minor versions, so most typical usage is likely to remain entirely unaffected.\nAny change to a patch version before the 1.0.0 release SHOULD either add functionality or have no visible difference in usage, aside from changes to the typing infomation or from a bug fix correcting the data that a component created.\nIn addition to the above conditions, it must be noted that any class, variable, function, etc., that is preceded by one or more underscores, excluding items preceded by two underscores and also proceeded by two underscores, MUST NOT be considered part of the public api. These methods may change at any time, in any way.\nI am aware of the F.A.Q. question that suggests that I should probably have pushed the module to a 1.0.0 release due to its usage in production, however there are a number of different items on the TODO list that I feel should be completed before that time. While some are simply important features I believe should exist, others are overhauls to sections of the public API that have needed careful fixing for quite a while, fixes that have slowly been happening throughout the versions. An important change was made in the 0.45.0 release which deprecates a large number of commonly used private functions and created more stable versions of them in the public API.\nAdditionally, my focus on versioning info has revealed that some of the dependencies are still in major version 0 or do not necessarily conform to Semantic Versioning. As such, these packages are more tightly constrained on what versions are considered acceptable, and careful consideration should be taken before extending the accepted range of versions.\nDetails on Semantic Versioning can be found at semver.org.\n\nTodo\nHere is a list of things that are currently on our todo list:\n\nTests (ie. unittest)\nFinish writing a usage guide\nImprove the intelligence of the saving functions\nImprove README\nCreate a wiki for advanced usage information\n\n\nCredits\nDestiny Peterson (The Elemental of Destruction) - Co-owner, principle programmer, knows more about msg files than anyone probably should.\nMatthew Walker - Original developer and co-owner.\nJP Bourget - Senior programmer, readability and organization expert, secondary manager.\nPhilippe Lagadec - Python OleFile module developer.\nJoel Kaufman - First implementations of the json and filename flags.\nDean Malmgren - First implementation of the setup.py script.\nSeamus Tuohy - Developer of the Python RTFDE module. Gave first examples of how to use the module and has worked with Destiny to ensure functionality.\nLiam - Significant reorganization and transfer of data.\nAnd thank you to everyone who has opened an issue and helped us track down those pesky bugs.\n\nExtra\nCheck out the new project msg-explorer that allows you to open MSG files and\nexplore their contents in a GUI. It is usually updated within a few days of a\nmajor release to ensure continued support. Because of this, it is recommended to\ninstall it to a separate environment (like a vitural env) to not interfere with\nyour access to the newest major version of extract-msg.\n\n\n"}, {"name": "exchange-calendars", "readme": "\nexchange_calendars\n   \nA Python library for defining and querying calendars for security exchanges.\nCalendars for more than 50 exchanges available out-the-box! If you still can't find the calendar you're looking for, create a new one!\nNotice: market_prices - the new library for prices data!\nMuch of the recent development of exchange_calendars has been driven by the new market_prices library. Check it out if you like the idea of using exchange_calendars to create meaningful OHLCV datasets. Works out-the-box with freely available data!\nNotice: v4 released (June 2022)\nThe earliest stable version of v4 is 4.0.1 (not 4.0).\nWhat's changed?\nVersion 4.0.1 completes the transition to a more consistent interface across the package. The most significant changes are:\n\nSessions are now timezone-naive (previously UTC).\nSchedule columns now have timezone set as UTC (whilst the times have always been defined in terms of UTC, previously the dtype was timezone-naive).\nThe following schedule columns were renamed:\n\n'market_open' renamed as 'open'.\n'market_close' renamed as 'close'.\n\n\nDefault calendar 'side' for all calendars is now \"left\" (previously \"right\" for 24-hour calendars and \"both\" for all others). This changes the minutes that are considered trading minutes by default (see minutes tutorial for an explanation of trading minutes).\nThe 'count' parameter of sessions_window and minutes_window methods now reflects the window length (previously window length + 1).\nNew is_open_at_time calendar method to evaluate if an exchange is open as at a specific instance (as opposed to over an evaluated minute).\nThe minimum Python version supported is now 3.8 (previously 3.7).\nParameters have been renamed for some methods (list here)\nThe following methods have been deprecated:\n\nsessions_opens (use .opens[start:end])\nsessions_closes (use .closes[start:end])\n\n\nMethods deprecated in 3.4 have been removed (lists here and here)\n\nSee the 4.0 release todo for a full list of changes and corresponding PRs.\nPlease offer any feedback at the v4 discussion.\nInstallation\n$ pip install exchange_calendars\n\nQuick Start\nimport exchange_calendars as xcals\n\nGet a list of available calendars:\n>>> xcals.get_calendar_names(include_aliases=False)[5:10]\n['CMES', 'IEPA', 'XAMS', 'XASX', 'XBKK']\n\nGet a calendar:\n>>> xnys = xcals.get_calendar(\"XNYS\")  # New York Stock Exchange\n>>> xhkg = xcals.get_calendar(\"XHKG\")  # Hong Kong Stock Exchange\n\nQuery the schedule:\n>>> xhkg.schedule.loc[\"2021-12-29\":\"2022-01-04\"]\n\n\n\n\n\n\n\n\n\n   open break_start break_end close     2021-12-29 2021-12-29 01:30:00+00:00 2021-12-29 04:00:00+00:00 2021-12-29 05:00:00+00:00 2021-12-29 08:00:00+00:00   2021-12-30 2021-12-30 01:30:00+00:00 2021-12-30 04:00:00+00:00 2021-12-30 05:00:00+00:00 2021-12-30 08:00:00+00:00   2021-12-31 2021-12-31 01:30:00+00:00 NaT NaT 2021-12-31 04:00:00+00:00   2022-01-03 2022-01-03 01:30:00+00:00 2022-01-03 04:00:00+00:00 2022-01-03 05:00:00+00:00 2022-01-03 08:00:00+00:00   2022-01-04 2022-01-04 01:30:00+00:00 2022-01-04 04:00:00+00:00 2022-01-04 05:00:00+00:00 2022-01-04 08:00:00+00:00  \n\nWorking with sessions\n>>> xnys.is_session(\"2022-01-01\")\nFalse\n\n>>> xnys.sessions_in_range(\"2022-01-01\", \"2022-01-11\")\nDatetimeIndex(['2022-01-03', '2022-01-04', '2022-01-05', '2022-01-06',\n               '2022-01-07', '2022-01-10', '2022-01-11'],\n              dtype='datetime64[ns]', freq='C')\n\n>>> xnys.sessions_window(\"2022-01-03\", 7)\nDatetimeIndex(['2022-01-03', '2022-01-04', '2022-01-05', '2022-01-06',\n               '2022-01-07', '2022-01-10', '2022-01-11'],\n              dtype='datetime64[ns]', freq='C')\n\n>>> xnys.date_to_session(\"2022-01-01\", direction=\"next\")\nTimestamp('2022-01-03 00:00:00', freq='C')\n\n>>> xnys.previous_session(\"2022-01-11\")\nTimestamp('2022-01-10 00:00:00', freq='C')\n\n>>> xhkg.trading_index(\n...     \"2021-12-30\", \"2021-12-31\", period=\"90T\", force=True\n... )\nIntervalIndex([[2021-12-30 01:30:00, 2021-12-30 03:00:00), [2021-12-30 03:00:00, 2021-12-30 04:00:00), [2021-12-30 05:00:00, 2021-12-30 06:30:00), [2021-12-30 06:30:00, 2021-12-30 08:00:00), [2021-12-31 01:30:00, 2021-12-31 03:00:00), [2021-12-31 03:00:00, 2021-12-31 04:00:00)], dtype='interval[datetime64[ns, UTC], left]')\n\nSee the sessions tutorial for a deeper dive into sessions.\nWorking with minutes\n>>> xhkg.session_minutes(\"2022-01-03\")\nDatetimeIndex(['2022-01-03 01:30:00+00:00', '2022-01-03 01:31:00+00:00',\n               '2022-01-03 01:32:00+00:00', '2022-01-03 01:33:00+00:00',\n               '2022-01-03 01:34:00+00:00', '2022-01-03 01:35:00+00:00',\n               '2022-01-03 01:36:00+00:00', '2022-01-03 01:37:00+00:00',\n               '2022-01-03 01:38:00+00:00', '2022-01-03 01:39:00+00:00',\n               ...\n               '2022-01-03 07:50:00+00:00', '2022-01-03 07:51:00+00:00',\n               '2022-01-03 07:52:00+00:00', '2022-01-03 07:53:00+00:00',\n               '2022-01-03 07:54:00+00:00', '2022-01-03 07:55:00+00:00',\n               '2022-01-03 07:56:00+00:00', '2022-01-03 07:57:00+00:00',\n               '2022-01-03 07:58:00+00:00', '2022-01-03 07:59:00+00:00'],\n              dtype='datetime64[ns, UTC]', length=330, freq=None)\n\n>>> mins = [ \"2022-01-03 \" + tm for tm in [\"01:29\", \"01:30\", \"04:20\", \"07:59\", \"08:00\"] ]\n>>> [ xhkg.is_trading_minute(minute) for minute in mins ]\n[False, True, False, True, False]  # by default minutes are closed on the left side\n\n>>> xhkg.is_break_minute(\"2022-01-03 04:20\")\nTrue\n\n>>> xhkg.previous_close(\"2022-01-03 08:10\")\nTimestamp('2022-01-03 08:00:00+0000', tz='UTC')\n\n>>> xhkg.previous_minute(\"2022-01-03 08:10\")\nTimestamp('2022-01-03 07:59:00+0000', tz='UTC')\n\nCheck out the minutes tutorial for a deeper dive that includes an explanation of the concept of 'minutes' and how the \"side\" option determines which minutes are treated as trading minutes.\nTutorials\n\nsessions.ipynb - all things sessions.\nminutes.ipynb - all things minutes. Don't miss this one!\ncalendar_properties.ipynb - calendar constrution and a walk through the schedule and all other calendar properties.\ncalendar_methods.ipynb - a walk through all the methods available to interrogate a calendar.\ntrading_index.ipynb - a method that warrants a tutorial all of its own.\n\nHopefully you'll find that exchange_calendars has the method you need to get the information you want. If it doesn't, either PR it or raise an issue and let us know!\nCommand Line Usage\nPrint a unix-cal like calendar straight from the command line (holidays are indicated by brackets)...\necal XNYS 2020\n\n                                        2020\n        January                        February                        March\nSu  Mo  Tu  We  Th  Fr  Sa     Su  Mo  Tu  We  Th  Fr  Sa     Su  Mo  Tu  We  Th  Fr  Sa\n            [ 1]  2   3 [ 4]                           [ 1]\n[ 5]  6   7   8   9  10 [11]   [ 2]  3   4   5   6   7 [ 8]   [ 1]  2   3   4   5   6 [ 7]\n[12] 13  14  15  16  17 [18]   [ 9] 10  11  12  13  14 [15]   [ 8]  9  10  11  12  13 [14]\n[19][20] 21  22  23  24 [25]   [16][17] 18  19  20  21 [22]   [15] 16  17  18  19  20 [21]\n[26] 27  28  29  30  31        [23] 24  25  26  27  28 [29]   [22] 23  24  25  26  27 [28]\n                                                              [29] 30  31\n\n        April                           May                            June\nSu  Mo  Tu  We  Th  Fr  Sa     Su  Mo  Tu  We  Th  Fr  Sa     Su  Mo  Tu  We  Th  Fr  Sa\n              1   2   3 [ 4]                         1 [ 2]         1   2   3   4   5 [ 6]\n[ 5]  6   7   8   9 [10][11]   [ 3]  4   5   6   7   8 [ 9]   [ 7]  8   9  10  11  12 [13]\n[12] 13  14  15  16  17 [18]   [10] 11  12  13  14  15 [16]   [14] 15  16  17  18  19 [20]\n[19] 20  21  22  23  24 [25]   [17] 18  19  20  21  22 [23]   [21] 22  23  24  25  26 [27]\n[26] 27  28  29  30            [24][25] 26  27  28  29 [30]   [28] 29  30\n                               [31]\n\n            July                          August                       September\nSu  Mo  Tu  We  Th  Fr  Sa     Su  Mo  Tu  We  Th  Fr  Sa     Su  Mo  Tu  We  Th  Fr  Sa\n              1   2 [ 3][ 4]                           [ 1]             1   2   3   4 [ 5]\n[ 5]  6   7   8   9  10 [11]   [ 2]  3   4   5   6   7 [ 8]   [ 6][ 7]  8   9  10  11 [12]\n[12] 13  14  15  16  17 [18]   [ 9] 10  11  12  13  14 [15]   [13] 14  15  16  17  18 [19]\n[19] 20  21  22  23  24 [25]   [16] 17  18  19  20  21 [22]   [20] 21  22  23  24  25 [26]\n[26] 27  28  29  30  31        [23] 24  25  26  27  28 [29]   [27] 28  29  30\n                               [30] 31\n\n        October                        November                       December\nSu  Mo  Tu  We  Th  Fr  Sa     Su  Mo  Tu  We  Th  Fr  Sa     Su  Mo  Tu  We  Th  Fr  Sa\n                  1   2 [ 3]                                            1   2   3   4 [ 5]\n[ 4]  5   6   7   8   9 [10]   [ 1]  2   3   4   5   6 [ 7]   [ 6]  7   8   9  10  11 [12]\n[11] 12  13  14  15  16 [17]   [ 8]  9  10  11  12  13 [14]   [13] 14  15  16  17  18 [19]\n[18] 19  20  21  22  23 [24]   [15] 16  17  18  19  20 [21]   [20] 21  22  23  24 [25][26]\n[25] 26  27  28  29  30 [31]   [22] 23  24  25 [26] 27 [28]   [27] 28  29  30  31\n                               [29] 30\n\necal XNYS 1 2020\n\n        January 2020\nSu  Mo  Tu  We  Th  Fr  Sa\n            [ 1]  2   3 [ 4]\n[ 5]  6   7   8   9  10 [11]\n[12] 13  14  15  16  17 [18]\n[19][20] 21  22  23  24 [25]\n[26] 27  28  29  30  31\n\nFrequently Asked Questions\nHow can I create a new calendar?\nFirst off, make sure the calendar you're after hasn't already been defined; exchange calendars comes with over 50 pre-defined calendars, including major security exchanges.\nIf you can't find what you're after, a custom calendar can be created as a subclass of ExchangeCalendar. This workflow describes the process to add a new calendar to exchange_calendars. Just follow the relevant parts.\nTo access the new calendar via get_calendar call either xcals.register_calendar or xcals.register_calendar_type to register, respectively, a specific calendar instance or a calendar factory (i.e. the subclass).\nCan I contribute a new calendar to exchange calendars?\nYes please! The workflow can be found here.\n<calendar> is missing a holiday, has a wrong time, should have a break etc...\nAll of the exchange calendars are maintained by user contributions. If a calendar you care about needs revising, please open a PR - that's how this thing works! (Never contributed to a project before and it all seems a bit daunting? Check this out and don't look back!)\nYou'll find the workflow to modify an existing calendar here.\nWhat times are considered open and closed?\nexchange_calendars attempts to be broadly useful by considering an exchange to be open only during periods of regular trading. During any pre-trading, post-trading or auction period the exchange is treated as closed. An exchange is also treated as closed during any observed lunch break.\nSee the minutes tutorial for a detailed explanation of which minutes an exchange is considered open over. If you previously used trading_calendars, or exchange_calendars prior to release 3.4, then this is the place to look for answers to questions of how the definition of trading minutes has changed over time (and is now stable and flexible!).\nCalendars\n\n\n\nExchange\nISO Code\nCountry\nVersion Added\nExchange Website (English)\n\n\n\n\nNew York Stock Exchange\nXNYS\nUSA\n1.0\nhttps://www.nyse.com/index\n\n\nCBOE Futures\nXCBF\nUSA\n1.0\nhttps://markets.cboe.com/us/futures/overview/\n\n\nChicago Mercantile Exchange\nCMES\nUSA\n1.0\nhttps://www.cmegroup.com/\n\n\nICE US\nIEPA\nUSA\n1.0\nhttps://www.theice.com/index\n\n\nToronto Stock Exchange\nXTSE\nCanada\n1.0\nhttps://www.tsx.com/\n\n\nBMF Bovespa\nBVMF\nBrazil\n1.0\nhttp://www.b3.com.br/en_us/\n\n\nLondon Stock Exchange\nXLON\nEngland\n1.0\nhttps://www.londonstockexchange.com/\n\n\nEuronext Amsterdam\nXAMS\nNetherlands\n1.2\nhttps://www.euronext.com/en/regulation/amsterdam\n\n\nEuronext Brussels\nXBRU\nBelgium\n1.2\nhttps://www.euronext.com/en/regulation/brussels\n\n\nEuronext Lisbon\nXLIS\nPortugal\n1.2\nhttps://www.euronext.com/en/regulation/lisbon\n\n\nEuronext Paris\nXPAR\nFrance\n1.2\nhttps://www.euronext.com/en/regulation/paris\n\n\nFrankfurt Stock Exchange\nXFRA\nGermany\n1.2\nhttp://en.boerse-frankfurt.de/\n\n\nSIX Swiss Exchange\nXSWX\nSwitzerland\n1.2\nhttps://www.six-group.com/en/home.html\n\n\nTokyo Stock Exchange\nXTKS\nJapan\n1.2\nhttps://www.jpx.co.jp/english/\n\n\nAustrialian Securities Exchange\nXASX\nAustralia\n1.3\nhttps://www.asx.com.au/\n\n\nBolsa de Madrid\nXMAD\nSpain\n1.3\nhttps://www.bolsamadrid.es\n\n\nBorsa Italiana\nXMIL\nItaly\n1.3\nhttps://www.borsaitaliana.it\n\n\nNew Zealand Exchange\nXNZE\nNew Zealand\n1.3\nhttps://www.nzx.com/\n\n\nWiener Borse\nXWBO\nAustria\n1.3\nhttps://www.wienerborse.at/en/\n\n\nHong Kong Stock Exchange\nXHKG\nHong Kong\n1.3\nhttps://www.hkex.com.hk/?sc_lang=en\n\n\nCopenhagen Stock Exchange\nXCSE\nDenmark\n1.4\nhttp://www.nasdaqomxnordic.com/\n\n\nHelsinki Stock Exchange\nXHEL\nFinland\n1.4\nhttp://www.nasdaqomxnordic.com/\n\n\nStockholm Stock Exchange\nXSTO\nSweden\n1.4\nhttp://www.nasdaqomxnordic.com/\n\n\nOslo Stock Exchange\nXOSL\nNorway\n1.4\nhttps://www.oslobors.no/ob_eng/\n\n\nIrish Stock Exchange\nXDUB\nIreland\n1.4\nhttp://www.ise.ie/\n\n\nBombay Stock Exchange\nXBOM\nIndia\n1.5\nhttps://www.bseindia.com\n\n\nSingapore Exchange\nXSES\nSingapore\n1.5\nhttps://www.sgx.com\n\n\nShanghai Stock Exchange\nXSHG\nChina\n1.5\nhttp://english.sse.com.cn\n\n\nKorea Exchange\nXKRX\nSouth Korea\n1.6\nhttp://global.krx.co.kr\n\n\nIceland Stock Exchange\nXICE\nIceland\n1.7\nhttp://www.nasdaqomxnordic.com/\n\n\nPoland Stock Exchange\nXWAR\nPoland\n1.9\nhttp://www.gpw.pl\n\n\nSantiago Stock Exchange\nXSGO\nChile\n1.9\nhttps://www.bolsadesantiago.com/\n\n\nColombia Securities Exchange\nXBOG\nColombia\n1.9\nhttps://www.bvc.com.co/nueva/https://www.bvc.com.co/nueva/\n\n\nMexican Stock Exchange\nXMEX\nMexico\n1.9\nhttps://www.bmv.com.mx\n\n\nLima Stock Exchange\nXLIM\nPeru\n1.9\nhttps://www.bvl.com.pe\n\n\nPrague Stock Exchange\nXPRA\nCzech Republic\n1.9\nhttps://www.pse.cz/en/\n\n\nBudapest Stock Exchange\nXBUD\nHungary\n1.10\nhttps://bse.hu/\n\n\nAthens Stock Exchange\nASEX\nGreece\n1.10\nhttp://www.helex.gr/\n\n\nIstanbul Stock Exchange\nXIST\nTurkey\n1.10\nhttps://www.borsaistanbul.com/en/\n\n\nJohannesburg Stock Exchange\nXJSE\nSouth Africa\n1.10\nhttps://www.jse.co.za/z\n\n\nMalaysia Stock Exchange\nXKLS\nMalaysia\n1.11\nhttp://www.bursamalaysia.com/market/\n\n\nMoscow Exchange\nXMOS\nRussia\n1.11\nhttps://www.moex.com/en/\n\n\nPhilippine Stock Exchange\nXPHS\nPhilippines\n1.11\nhttps://www.pse.com.ph/\n\n\nStock Exchange of Thailand\nXBKK\nThailand\n1.11\nhttps://www.set.or.th/set/mainpage.do?language=en&country=US\n\n\nIndonesia Stock Exchange\nXIDX\nIndonesia\n1.11\nhttps://www.idx.co.id/\n\n\nTaiwan Stock Exchange Corp.\nXTAI\nTaiwan\n1.11\nhttps://www.twse.com.tw/en/\n\n\nBuenos Aires Stock Exchange\nXBUE\nArgentina\n1.11\nhttps://www.bcba.sba.com.ar/\n\n\nPakistan Stock Exchange\nXKAR\nPakistan\n1.11\nhttps://www.psx.com.pk/\n\n\nXetra\nXETR\nGermany\n2.1\nhttps://www.xetra.com/\n\n\nTel Aviv Stock Exchange\nXTAE\nIsrael\n2.1\nhttps://www.tase.co.il/\n\n\nAstana International Exchange\nAIXK\nKazakhstan\n3.2\nhttps://www.aix.kz/\n\n\nBucharest Stock Exchange\nXBSE\nRomania\n3.2\nhttps://www.bvb.ro/\n\n\nSaudi Stock Exchange\nXSAU\nSaudi Arabia\n4.2\nhttps://www.saudiexchange.sa/\n\n\n\n\nNote that exchange calendars are defined by their ISO-10383 market identifier code.\n\nDeprecations and Renaming\nMethods deprecated in 4.0\n\n\n\nDeprecated method\nReason\n\n\n\n\nsessions_closes\nuse .closes[start:end]\n\n\nsessions_opens\nuse .opens[start:end]\n\n\n\nMethods with a parameter renamed in 4.0\n\n\n\nMethod\n\n\n\n\nis_session\n\n\nis_open_on_minute\n\n\nminutes_in_range\n\n\nminutes_window\n\n\nnext_close\n\n\nnext_minute\n\n\nnext_open\n\n\nprevious_close\n\n\nprevious_minute\n\n\nprevious_open\n\n\nsession_break_end\n\n\nsession_break_start\n\n\nsession_close\n\n\nsession_open\n\n\nsessions_in_range\n\n\nsessions_window\n\n\n\nMethods renamed in version 3.4 and removed in 4.0\n\n\n\nPrevious name\nNew name\n\n\n\n\nall_minutes\nminutes\n\n\nall_minutes_nanos\nminutes_nanos\n\n\nall_sessions\nsessions\n\n\nbreak_start_and_end_for_session\nsession_break_start_end\n\n\ndate_to_session_label\ndate_to_session\n\n\nfirst_trading_minute\nfirst_minute\n\n\nfirst_trading_session\nfirst_session\n\n\nhas_breaks\nsessions_has_break\n\n\nlast_trading_minute\nlast_minute\n\n\nlast_trading_session\nlast_session\n\n\nnext_session_label\nnext_session\n\n\nopen_and_close_for_session\nsession_open_close\n\n\nprevious_session_label\nprevious_session\n\n\nmarket_break_ends_nanos\nbreak_ends_nanos\n\n\nmarket_break_starts_nanos\nbreak_starts_nanos\n\n\nmarket_closes_nanos\ncloses_nanos\n\n\nmarket_opens_nanos\nopens_nanos\n\n\nminute_index_to_session_labels\nminutes_to_sessions\n\n\nminute_to_session_label\nminute_to_session\n\n\nminutes_count_for_sessions_in_range\nsessions_minutes_count\n\n\nminutes_for_session\nsession_minutes\n\n\nminutes_for_sessions_in_range\nsessions_minutes\n\n\nsession_closes_in_range\nsessions_closes\n\n\nsession_distance\nsessions_distance\n\n\nsession_opens_in_range\nsessions_opens\n\n\n\nOther methods deprecated in 3.4 and removed in 4.0\n\n\n\nRemoved Method\n\n\n\n\nexecution_minute_for_session\n\n\nexecution_minute_for_sessions_in_range\n\n\nexecution_time_from_close\n\n\nexecution_time_from_open\n\n\n\n"}, {"name": "et-xmlfile", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nWarehouse\nGetting Started\nDiscussion\nTesting\nCode of Conduct\n\n\n\n\n\nREADME.rst\n\n\n\n\nWarehouse\nWarehouse is the software that powers PyPI.\nSee our development roadmap, documentation, and\narchitectural overview.\n\nGetting Started\nYou can run Warehouse locally in a development environment using\ndocker. See Getting started\ndocumentation for instructions on how to set it up.\nThe canonical deployment of Warehouse is in production at pypi.org.\n\nDiscussion\nYou can find help or get involved on:\n\nGithub issue tracker for reporting issues\nIRC: on Libera, channel #pypa for general packaging discussion\nand user support, and #pypa-dev for\ndiscussions about development of packaging tools\nThe PyPA Discord for live discussions\nThe Packaging category on Discourse for discussing\nnew ideas and community initiatives\n\n\nTesting\nRead the running tests and linters section of our documentation to\nlearn how to test your code.  For cross-browser testing, we use an\nopen source account from BrowserStack. If your pull request makes\nany change to the user interface, it will need to be tested to confirm\nit works in our supported browsers.\n\n\nCode of Conduct\nEveryone interacting in the Warehouse project's codebases, issue trackers, chat\nrooms, and mailing lists is expected to follow the PSF Code of Conduct.\n\n\n"}, {"name": "email-validator", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nemail-validator: Validate Email Addresses\nInstallation\nQuick Start\nUsage\nOverview\nOptions\nDNS timeout and cache\nTest addresses\nInternationalized email addresses\nInternationalized domain names (IDN)\nInternationalized local parts\nIf you know ahead of time that SMTPUTF8 is not supported by your mail submission stack\nNormalization\nUnicode Normalization\nOther Normalization\nExamples\nReturn value\nAssumptions\nTesting\nFor Project Maintainers\n\n\n\n\n\nREADME.md\n\n\n\n\nemail-validator: Validate Email Addresses\nA robust email address syntax and deliverability validation library for\nPython 3.7+ by Joshua Tauberer.\nThis library validates that a string is of the form name@example.com\nand optionally checks that the domain name is set up to receive email.\nThis is the sort of validation you would want when you are identifying\nusers by their email address like on a registration/login form (but not\nnecessarily for composing an email message, see below).\nKey features:\n\nChecks that an email address has the correct syntax --- good for\nregistration/login forms or other uses related to identifying users.\nGives friendly English error messages when validation fails that you\ncan display to end-users.\nChecks deliverability (optional): Does the domain name resolve?\n(You can override the default DNS resolver to add query caching.)\nSupports internationalized domain names and internationalized local parts.\nRejects addresses with unsafe Unicode characters, obsolete email address\nsyntax that you'd find unexpected, special use domain names like\n@localhost, and domains without a dot by default. This is an\nopinionated library!\nNormalizes email addresses (important for internationalized\nand quoted-string addresses! see below).\nPython type annotations are used.\n\nThis is an opinionated library. You should definitely also consider using\nthe less-opinionated pyIsEmail and\nflanker if they are better for your\nuse case.\n\nView the CHANGELOG / Release Notes for the version history of changes in the library. Occasionally this README is ahead of the latest published package --- see the CHANGELOG for details.\n\nInstallation\nThis package is on PyPI, so:\npip install email-validator\n(You might need to use pip3 depending on your local environment.)\nQuick Start\nIf you're validating a user's email address before creating a user\naccount in your application, you might do this:\nfrom email_validator import validate_email, EmailNotValidError\n\nemail = \"my+address@example.org\"\n\ntry:\n\n  # Check that the email address is valid. Turn on check_deliverability\n  # for first-time validations like on account creation pages (but not\n  # login pages).\n  emailinfo = validate_email(email, check_deliverability=False)\n\n  # After this point, use only the normalized form of the email address,\n  # especially before going to a database query.\n  email = emailinfo.normalized\n\nexcept EmailNotValidError as e:\n\n  # The exception message is human-readable explanation of why it's\n  # not a valid (or deliverable) email address.\n  print(str(e))\nThis validates the address and gives you its normalized form. You should\nput the normalized form in your database and always normalize before\nchecking if an address is in your database. When using this in a login form,\nset check_deliverability to False to avoid unnecessary DNS queries.\nUsage\nOverview\nThe module provides a function validate_email(email_address) which\ntakes an email address and:\n\nRaises a EmailNotValidError with a helpful, human-readable error\nmessage explaining why the email address is not valid, or\nReturns an object with a normalized form of the email address (which\nyou should use!) and other information about it.\n\nWhen an email address is not valid, validate_email raises either an\nEmailSyntaxError if the form of the address is invalid or an\nEmailUndeliverableError if the domain name fails DNS checks. Both\nexception classes are subclasses of EmailNotValidError, which in turn\nis a subclass of ValueError.\nBut when an email address is valid, an object is returned containing\na normalized form of the email address (which you should use!) and\nother information.\nThe validator doesn't, by default, permit obsoleted forms of email addresses\nthat no one uses anymore even though they are still valid and deliverable, since\nthey will probably give you grief if you're using email for login. (See\nlater in the document about how to allow some obsolete forms.)\nThe validator optionally checks that the domain name in the email address has\na DNS MX record indicating that it can receive email. (Except a Null MX record.\nIf there is no MX record, a fallback A/AAAA-record is permitted, unless\na reject-all SPF record is present.) DNS is slow and sometimes unavailable or\nunreliable, so consider whether these checks are useful for your use case and\nturn them off if they aren't.\nThere is nothing to be gained by trying to actually contact an SMTP server, so\nthat's not done here. For privacy, security, and practicality reasons, servers\nare good at not giving away whether an address is\ndeliverable or not: email addresses that appear to accept mail at first\ncan bounce mail after a delay, and bounced mail may indicate a temporary\nfailure of a good email address (sometimes an intentional failure, like\ngreylisting).\nOptions\nThe validate_email function also accepts the following keyword arguments\n(defaults are as shown below):\ncheck_deliverability=True: If true, DNS queries are made to check that the domain name in the email address (the part after the @-sign) can receive mail, as described above. Set to False to skip this DNS-based check. It is recommended to pass False when performing validation for login pages (but not account creation pages) since re-validation of a previously validated domain in your database by querying DNS at every login is probably undesirable. You can also set email_validator.CHECK_DELIVERABILITY to False to turn this off for all calls by default.\ndns_resolver=None: Pass an instance of dns.resolver.Resolver to control the DNS resolver including setting a timeout and a cache. The caching_resolver function shown below is a helper function to construct a dns.resolver.Resolver with a LRUCache. Reuse the same resolver instance across calls to validate_email to make use of the cache.\ntest_environment=False: If True, DNS-based deliverability checks are disabled and  test and **.test domain names are permitted (see below). You can also set email_validator.TEST_ENVIRONMENT to True to turn it on for all calls by default.\nallow_smtputf8=True: Set to False to prohibit internationalized addresses that would\nrequire the\nSMTPUTF8 extension. You can also set email_validator.ALLOW_SMTPUTF8 to False to turn it off for all calls by default.\nallow_quoted_local=False: Set to True to allow obscure and potentially problematic email addresses in which the part of the address before the @-sign contains spaces, @-signs, or other surprising characters when the local part is surrounded in quotes (so-called quoted-string local parts). In the object returned by validate_email, the normalized local part removes any unnecessary backslash-escaping and even removes the surrounding quotes if the address would be valid without them. You can also set email_validator.ALLOW_QUOTED_LOCAL to True to turn this on for all calls by default.\nallow_domain_literal=False: Set to True to allow bracketed IPv4 and \"IPv6:\"-prefixd IPv6 addresses in the domain part of the email address. No deliverability checks are performed for these addresses. In the object returned by validate_email, the normalized domain will use the condensed IPv6 format, if applicable. The object's domain_address attribute will hold the parsed ipaddress.IPv4Address or ipaddress.IPv6Address object if applicable. You can also set email_validator.ALLOW_DOMAIN_LITERAL to True to turn this on for all calls by default.\nallow_empty_local=False: Set to True to allow an empty local part (i.e.\n@example.com), e.g. for validating Postfix aliases.\nDNS timeout and cache\nWhen validating many email addresses or to control the timeout (the default is 15 seconds), create a caching dns.resolver.Resolver to reuse in each call. The caching_resolver function returns one easily for you:\nfrom email_validator import validate_email, caching_resolver\n\nresolver = caching_resolver(timeout=10)\n\nwhile True:\n  validate_email(email, dns_resolver=resolver)\nTest addresses\nThis library rejects email addresess that use the Special Use Domain Names invalid, localhost, test, and some others by raising EmailSyntaxError. This is to protect your system from abuse: You probably don't want a user to be able to cause an email to be sent to localhost (although they might be able to still do so via a malicious MX record). However, in your non-production test environments you may want to use @test or @myname.test email addresses. There are three ways you can allow this:\n\nAdd test_environment=True to the call to validate_email (see above).\nSet email_validator.TEST_ENVIRONMENT to True globally.\nRemove the special-use domain name that you want to use from email_validator.SPECIAL_USE_DOMAIN_NAMES, e.g.:\n\nimport email_validator\nemail_validator.SPECIAL_USE_DOMAIN_NAMES.remove(\"test\")\nIt is tempting to use @example.com/net/org in tests. They are not in this library's SPECIAL_USE_DOMAIN_NAMES list so you can, but shouldn't, use them. These domains are reserved to IANA for use in documentation so there is no risk of accidentally emailing someone at those domains. But beware that this library will nevertheless reject these domain names if DNS-based deliverability checks are not disabled because these domains do not resolve to domains that accept email. In tests, consider using your own domain name or @test or @myname.test instead.\nInternationalized email addresses\nThe email protocol SMTP and the domain name system DNS have historically\nonly allowed English (ASCII) characters in email addresses and domain names,\nrespectively. Each has adapted to internationalization in a separate\nway, creating two separate aspects to email address\ninternationalization.\nInternationalized domain names (IDN)\nThe first is internationalized domain names (RFC\n5891), a.k.a IDNA 2008. The DNS\nsystem has not been updated with Unicode support. Instead, internationalized\ndomain names are converted into a special IDNA ASCII \"Punycode\"\nform starting with xn--. When an email address has non-ASCII\ncharacters in its domain part, the domain part is replaced with its IDNA\nASCII equivalent form in the process of mail transmission. Your mail\nsubmission library probably does this for you transparently. (Compliance\naround the web is not very good though.) This library conforms to IDNA 2008\nusing the idna module by Kim Davies.\nInternationalized local parts\nThe second sort of internationalization is internationalization in the\nlocal part of the address (before the @-sign). In non-internationalized\nemail addresses, only English letters, numbers, and some punctuation\n(._!#$%&'^``*+-=~/?{|}) are allowed. In internationalized email address\nlocal parts, a wider range of Unicode characters are allowed.\nA surprisingly large number of Unicode characters are not safe to display,\nespecially when the email address is concatenated with other text, so this\nlibrary tries to protect you by not permitting resvered, non-, private use,\nformatting (which can be used to alter the display order of characters),\nwhitespace, and control characters, and combining characters\nas the first character of the local part and the domain name (so that they\ncannot combine with something outside of the email address string or with\nthe @-sign). See https://qntm.org/safe and https://trojansource.codes/\nfor relevant prior work. (Other than whitespace, these are checks that\nyou should be applying to nearly all user inputs in a security-sensitive\ncontext.)\nThese character checks are performed after Unicode normalization (see below),\nso you are only fully protected if you replace all user-provided email addresses\nwith the normalized email address string returned by this library. This does not\nguard against the well known problem that many Unicode characters look alike\n(or are identical), which can be used to fool humans reading displayed text.\nEmail addresses with these non-ASCII characters require that your mail\nsubmission library and the mail servers along the route to the destination,\nincluding your own outbound mail server, all support the\nSMTPUTF8 (RFC 6531) extension.\nSupport for SMTPUTF8 varies. See the allow_smtputf8 parameter.\nIf you know ahead of time that SMTPUTF8 is not supported by your mail submission stack\nBy default all internationalized forms are accepted by the validator.\nBut if you know ahead of time that SMTPUTF8 is not supported by your\nmail submission stack, then you must filter out addresses that require\nSMTPUTF8 using the allow_smtputf8=False keyword argument (see above).\nThis will cause the validation function to raise a EmailSyntaxError if\ndelivery would require SMTPUTF8. That's just in those cases where\nnon-ASCII characters appear before the @-sign. If you do not set\nallow_smtputf8=False, you can also check the value of the smtputf8\nfield in the returned object.\nIf your mail submission library doesn't support Unicode at all --- even\nin the domain part of the address --- then immediately prior to mail\nsubmission you must replace the email address with its ASCII-ized form.\nThis library gives you back the ASCII-ized form in the ascii_email\nfield in the returned object, which you can get like this:\nemailinfo = validate_email(email, allow_smtputf8=False)\nemail = emailinfo.ascii_email\nThe local part is left alone (if it has internationalized characters\nallow_smtputf8=False will force validation to fail) and the domain\npart is converted to IDNA ASCII.\n(You probably should not do this at account creation time so you don't\nchange the user's login information without telling them.)\nNormalization\nUnicode Normalization\nThe use of Unicode in email addresses introduced a normalization\nproblem. Different Unicode strings can look identical and have the same\nsemantic meaning to the user. The normalized field returned on successful\nvalidation provides the correctly normalized form of the given email\naddress.\nFor example, the CJK fullwidth Latin letters are considered semantically\nequivalent in domain names to their ASCII counterparts. This library\nnormalizes them to their ASCII counterparts:\nemailinfo = validate_email(\"me@\uff24\uff4f\uff4d\uff41\uff49\uff4e.com\")\nprint(emailinfo.normalized)\nprint(emailinfo.ascii_email)\n# prints \"me@domain.com\" twice\nBecause an end-user might type their email address in different (but\nequivalent) un-normalized forms at different times, you ought to\nreplace what they enter with the normalized form immediately prior to\ngoing into your database (during account creation), querying your database\n(during login), or sending outbound mail. Normalization may also change\nthe length of an email address, and this may affect whether it is valid\nand acceptable by your SMTP provider.\nThe normalizations include lowercasing the domain part of the email\naddress (domain names are case-insensitive), Unicode \"NFC\"\nnormalization of the\nwhole address (which turns characters plus combining\ncharacters into\nprecomposed characters where possible, replacement of fullwidth and\nhalfwidth\ncharacters\nin the domain part, possibly other\nUTS46 mappings on the domain part,\nand conversion from Punycode to Unicode characters.\n(See RFC 6532 (internationalized email) section\n3.1 and RFC 5895\n(IDNA 2008) section 2.)\nOther Normalization\nNormalization is also applied to quoted-string local parts and domain\nliteral IPv6 addresses if you have allowed them by the allow_quoted_local\nand allow_domain_literal options. In quoted-string local parts, unnecessary\nbackslash escaping is removed and even the surrounding quotes are removed if\nthey are unnecessary. For IPv6 domain literals, the IPv6 address is\nnormalized to condensed form. RFC 2142\nalso requires lowercase normalization for some specific mailbox names like postmaster@.\nExamples\nFor the email address test@joshdata.me, the returned object is:\nValidatedEmail(\n  normalized='test@joshdata.me',\n  local_part='test',\n  domain='joshdata.me',\n  ascii_email='test@joshdata.me',\n  ascii_local_part='test',\n  ascii_domain='joshdata.me',\n  smtputf8=False)\nFor the fictitious but valid address example@\u30c4.\u24c1\u24be\u24bb\u24ba, which has an\ninternationalized domain but ASCII local part, the returned object is:\nValidatedEmail(\n  normalized='example@\u30c4.life',\n  local_part='example',\n  domain='\u30c4.life',\n  ascii_email='example@xn--bdk.life',\n  ascii_local_part='example',\n  ascii_domain='xn--bdk.life',\n  smtputf8=False)\nNote that normalized and other fields provide a normalized form of the\nemail address, domain name, and (in other cases) local part (see earlier\ndiscussion of normalization), which you should use in your database.\nCalling validate_email with the ASCII form of the above email address,\nexample@xn--bdk.life, returns the exact same information (i.e., the\nnormalized field always will contain Unicode characters, not Punycode).\nFor the fictitious address \u30c4-test@joshdata.me, which has an\ninternationalized local part, the returned object is:\nValidatedEmail(\n  normalized='\u30c4-test@joshdata.me',\n  local_part='\u30c4-test',\n  domain='joshdata.me',\n  ascii_email=None,\n  ascii_local_part=None,\n  ascii_domain='joshdata.me',\n  smtputf8=True)\nNow smtputf8 is True and ascii_email is None because the local\npart of the address is internationalized. The local_part and normalized fields\nreturn the normalized form of the address.\nReturn value\nWhen an email address passes validation, the fields in the returned object\nare:\n\n\n\nField\nValue\n\n\n\n\nnormalized\nThe normalized form of the email address that you should put in your database. This combines the local_part and domain fields (see below).\n\n\nascii_email\nIf set, an ASCII-only form of the normalized email address by replacing the domain part with IDNA Punycode. This field will be present when an ASCII-only form of the email address exists (including if the email address is already ASCII). If the local part of the email address contains internationalized characters, ascii_email will be None. If set, it merely combines ascii_local_part and ascii_domain.\n\n\nlocal_part\nThe normalized local part of the given email address (before the @-sign). Normalization includes Unicode NFC normalization and removing unnecessary quoted-string quotes and backslashes. If allow_quoted_local is True and the surrounding quotes are necessary, the quotes will be present in this field.\n\n\nascii_local_part\nIf set, the local part, which is composed of ASCII characters only.\n\n\ndomain\nThe canonical internationalized Unicode form of the domain part of the email address. If the returned string contains non-ASCII characters, either the SMTPUTF8 feature of your mail relay will be required to transmit the message or else the email address's domain part must be converted to IDNA ASCII first: Use ascii_domain field instead.\n\n\nascii_domain\nThe IDNA Punycode-encoded form of the domain part of the given email address, as it would be transmitted on the wire.\n\n\ndomain_address\nIf domain literals are allowed and if the email address contains one, an ipaddress.IPv4Address or ipaddress.IPv6Address object.\n\n\nsmtputf8\nA boolean indicating that the SMTPUTF8 feature of your mail relay will be required to transmit messages to this address because the local part of the address has non-ASCII characters (the local part cannot be IDNA-encoded). If allow_smtputf8=False is passed as an argument, this flag will always be false because an exception is raised if it would have been true.\n\n\nmx\nA list of (priority, domain) tuples of MX records specified in the DNS for the domain (see RFC 5321 section 5). May be None if the deliverability check could not be completed because of a temporary issue like a timeout.\n\n\nmx_fallback_type\nNone if an MX record is found. If no MX records are actually specified in DNS and instead are inferred, through an obsolete mechanism, from A or AAAA records, the value is the type of DNS record used instead (A or AAAA). May be None if the deliverability check could not be completed because of a temporary issue like a timeout.\n\n\nspf\nAny SPF record found while checking deliverability. Only set if the SPF record is queried.\n\n\n\nAssumptions\nBy design, this validator does not pass all email addresses that\nstrictly conform to the standards. Many email address forms are obsolete\nor likely to cause trouble:\n\nThe validator assumes the email address is intended to be\nusable on the public Internet. The domain part\nof the email address must be a resolvable domain name\n(see the deliverability checks described above).\nMost Special Use Domain Names\nand their subdomains, as well as\ndomain names without a ., are rejected as a syntax error\n(except see the test_environment parameter above).\nObsolete email syntaxes are rejected:\nThe unusual \"(comment)\" syntax\nis rejected. Extremely old obsolete syntaxes are\nrejected. Quoted-string local parts and domain-literal addresses\nare rejected by default, but there are options to allow them (see above).\nNo one uses these forms anymore, and I can't think of any reason why anyone\nusing this library would need to accept them.\n\nTesting\nTests can be run using\npip install -r test_requirements.txt \nmake test\nTests run with mocked DNS responses. When adding or changing tests, temporarily turn on the BUILD_MOCKED_DNS_RESPONSE_DATA flag in tests/mocked_dns_responses.py to re-build the database of mocked responses from live queries.\nFor Project Maintainers\nThe package is distributed as a universal wheel and as a source package.\nTo release:\n\nUpdate CHANGELOG.md.\nUpdate the version number in email_validator/version.py.\nMake & push a commit with the new version number and make sure tests pass.\nMake & push a tag (see command below).\nMake a release at https://github.com/JoshData/python-email-validator/releases/new.\nPublish a source and wheel distribution to pypi (see command below).\n\ngit tag v$(grep version setup.cfg | sed \"s/.*= //\")\ngit push --tags\n./release_to_pypi.sh\n\n\n"}, {"name": "databricks-sql-connector", "readme": "\nDatabricks SQL Connector for Python\n\n\nThe Databricks SQL Connector for Python allows you to develop Python applications that connect to Databricks clusters and SQL warehouses. It is a Thrift-based client with no dependencies on ODBC or JDBC. It conforms to the Python DB API 2.0 specification and exposes a SQLAlchemy dialect for use with tools like pandas and alembic which use SQLAlchemy to execute DDL.\nThis connector uses Arrow as the data-exchange format, and supports APIs to directly fetch Arrow tables. Arrow tables are wrapped in the ArrowQueue class to provide a natural API to get several rows at a time.\nYou are welcome to file an issue here for general use cases. You can also contact Databricks Support here.\nRequirements\nPython 3.7 or above is required.\nDocumentation\nFor the latest documentation, see\n\nDatabricks\nAzure Databricks\n\nQuickstart\nInstall the library with pip install databricks-sql-connector\nNote: Don't hard-code authentication secrets into your Python. Use environment variables\nexport DATABRICKS_HOST=********.databricks.com\nexport DATABRICKS_HTTP_PATH=/sql/1.0/endpoints/****************\nexport DATABRICKS_TOKEN=dapi********************************\n\nExample usage:\nimport os\nfrom databricks import sql\n\nhost = os.getenv(\"DATABRICKS_HOST\")\nhttp_path = os.getenv(\"DATABRICKS_HTTP_PATH\")\naccess_token = os.getenv(\"DATABRICKS_TOKEN\")\n\nconnection = sql.connect(\n  server_hostname=host,\n  http_path=http_path,\n  access_token=access_token)\n\ncursor = connection.cursor()\n\ncursor.execute('SELECT * FROM RANGE(10)')\nresult = cursor.fetchall()\nfor row in result:\n  print(row)\n\ncursor.close()\nconnection.close()\n\nIn the above example:\n\nserver-hostname is the Databricks instance host name.\nhttp-path is the HTTP Path either to a Databricks SQL endpoint (e.g. /sql/1.0/endpoints/1234567890abcdef),\nor to a Databricks Runtime interactive cluster (e.g. /sql/protocolv1/o/1234567890123456/1234-123456-slid123)\npersonal-access-token is the Databricks Personal Access Token for the account that will execute commands and queries\n\nContributing\nSee CONTRIBUTING.md\nLicense\nApache License 2.0\n"}, {"name": "compressed-rtf", "readme": "\n\n\n\n\n\n\n\n\n\n\n\ncompressed_rtf\nDescription:\nUsage example:\nLicense:\n\n\n\n\n\nREADME.md\n\n\n\n\ncompressed_rtf\n\n\n\n\nCompressed Rich Text Format (RTF) compression worker in Python\nDescription:\nCompressed RTF also known as \"LZFu\" compression format\nBased on Rich Text Format (RTF) Compression Algorithm:\nhttps://msdn.microsoft.com/en-us/library/cc463890(v=exchg.80).aspx\nUsage example:\n>>> from compressed_rtf import compress, decompress\n>>>\n>>> data = '{\\\\rtf1\\\\ansi\\\\ansicpg1252\\\\pard test}'\n>>> comp = compress(data, compressed=True)  # compressed\n>>> comp\n'#\\x00\\x00\\x00\"\\x00\\x00\\x00LZFu3\\\\\\xe8t\\x03\\x00\\n\\x00rcpg125\\x922\\n\\xf3 t\\x07\\x90t}\\x0f\\x10'\n>>>\n>>> raw = compress(data, compressed=False)  # raw/uncompressed\n>>> raw\n'.\\x00\\x00\\x00\"\\x00\\x00\\x00MELA \\xdf\\x12\\xce{\\\\rtf1\\\\ansi\\\\ansicpg1252\\\\pard test}'\n>>>\n>>> decompress(comp)\n'{\\\\rtf1\\\\ansi\\\\ansicpg1252\\\\pard test}'\n>>>\n>>> decompress(raw)\n'{\\\\rtf1\\\\ansi\\\\ansicpg1252\\\\pard test}'\n>>>\nLicense:\nReleased under The MIT License.\n\n\n"}, {"name": "click-plugins", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nclick-plugins\nWhy?\nEnabling Plugins\nDeveloping Plugins\nBroken and Incompatible Plugins\nBest Practices and Extra Credit\nInstallation\nDeveloping\nChangelog\nAuthors\nLicense\n\n\n\n\n\nREADME.rst\n\n\n\n\nclick-plugins\n\n\nAn extension module for click to register\nexternal CLI commands via setuptools entry-points.\n\nWhy?\nLets say you develop a commandline interface and someone requests a new feature\nthat is absolutely related to your project but would have negative consequences\nlike additional dependencies, major refactoring, or maybe its just too domain\nspecific to be supported directly.  Rather than developing a separate standalone\nutility you could offer up a setuptools entry point\nthat allows others to use your commandline utility as a home for their related\nsub-commands.  You get to choose where these sub-commands or sub-groups CAN be\nregistered but the plugin developer gets to choose they ARE registered.  You\ncould have all plugins register alongside the core commands, in a special\nsub-group, across multiple sub-groups, or some combination.\n\nEnabling Plugins\nFor a more detailed example see the examples section.\nThe only requirement is decorating click.group() with click_plugins.with_plugins()\nwhich handles attaching external commands and groups.  In this case the core CLI developer\nregisters CLI plugins from core_package.cli_plugins.\nfrom pkg_resources import iter_entry_points\n\nimport click\nfrom click_plugins import with_plugins\n\n\n@with_plugins(iter_entry_points('core_package.cli_plugins'))\n@click.group()\ndef cli():\n    \"\"\"Commandline interface for yourpackage.\"\"\"\n\n@cli.command()\ndef subcommand():\n    \"\"\"Subcommand that does something.\"\"\"\n\nDeveloping Plugins\nPlugin developers need to register their sub-commands or sub-groups to an\nentry-point in their setup.py that is loaded by the core package.\nfrom setuptools import setup\n\nsetup(\n    name='yourscript',\n    version='0.1',\n    py_modules=['yourscript'],\n    install_requires=[\n        'click',\n    ],\n    entry_points='''\n        [core_package.cli_plugins]\n        cool_subcommand=yourscript.cli:cool_subcommand\n        another_subcommand=yourscript.cli:another_subcommand\n    ''',\n)\n\nBroken and Incompatible Plugins\nAny sub-command or sub-group that cannot be loaded is caught and converted to\na click_plugins.core.BrokenCommand() rather than just crashing the entire\nCLI.  The short-help is converted to a warning message like:\nWarning: could not load plugin. See ``<CLI> <command/group> --help``.\nand if the sub-command or group is executed the entire traceback is printed.\n\nBest Practices and Extra Credit\nOpening a CLI to plugins encourages other developers to independently extend\nfunctionality independently but there is no guarantee these new features will\nbe \"on brand\".  Plugin developers are almost certainly already using features\nin the core package the CLI belongs to so defining commonly used arguments and\noptions in one place lets plugin developers reuse these flags to produce a more\ncohesive CLI.  If the CLI is simple maybe just define them at the top of\nyourpackage/cli.py or for more complex packages something like\nyourpackage/cli/options.py.  These common options need to be easy to find\nand be well documented so that plugin developers know what variable to give to\ntheir sub-command's function and what object they can expect to receive.  Don't\nforget to document non-obvious callbacks.\nKeep in mind that plugin developers also have access to the parent group's\nctx.obj, which is very useful for passing things like verbosity levels or\nconfig values around to sub-commands.\nHere's some code that sub-commands could re-use:\nfrom multiprocessing import cpu_count\n\nimport click\n\njobs_opt = click.option(\n    '-j', '--jobs', metavar='CORES', type=click.IntRange(min=1, max=cpu_count()), default=1,\n    show_default=True, help=\"Process data across N cores.\"\n)\nPlugin developers can access this with:\nimport click\nimport parent_cli_package.cli.options\n\n\n@click.command()\n@parent_cli_package.cli.options.jobs_opt\ndef subcommand(jobs):\n    \"\"\"I do something domain specific.\"\"\"\n\nInstallation\nWith pip:\n$ pip install click-plugins\nFrom source:\n$ git clone https://github.com/click-contrib/click-plugins.git\n$ cd click-plugins\n$ python setup.py install\n\nDeveloping\n$ git clone https://github.com/click-contrib/click-plugins.git\n$ cd click-plugins\n$ pip install -e .\\[dev\\]\n$ pytest tests --cov click_plugins --cov-report term-missing\n\nChangelog\nSee CHANGES.txt\n\nAuthors\nSee AUTHORS.txt\n\nLicense\nSee LICENSE.txt\n\n\n"}, {"name": "charset-normalizer", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharset Detection, for Everyone \ud83d\udc4b\n\u26a1 Performance\n\u2728 Installation\n\ud83d\ude80 Basic Usage\nCLI\nPython\n\ud83d\ude07 Why\n\ud83c\udf70 How\n\u26a1 Known limitations\n\u26a0\ufe0f About Python EOLs\n\ud83d\udc64 Contributing\n\ud83d\udcdd License\n\ud83d\udcbc For Enterprise\n\n\n\n\n\nREADME.md\n\n\n\n\nCharset Detection, for Everyone \ud83d\udc4b\n\nThe Real First Universal Charset Detector\n\n\n\n\n\n\n\n\n\n\n\nFeatured Packages\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA library that helps you read text from an unknown charset encoding. Motivated by chardet,\nI'm trying to resolve the issue by taking a new approach.\nAll IANA character set names for which the Python core library provides codecs are supported.\n\n\n  >>>>> \ud83d\udc49 Try Me Online Now, Then Adopt Me \ud83d\udc48  <<<<<\n\nThis project offers you an alternative to Universal Charset Encoding Detector, also known as Chardet.\n\n\n\nFeature\nChardet\nCharset Normalizer\ncChardet\n\n\n\n\nFast\n\u274c\n\u2705\n\u2705\n\n\nUniversal**\n\u274c\n\u2705\n\u274c\n\n\nReliable without distinguishable standards\n\u274c\n\u2705\n\u2705\n\n\nReliable with distinguishable standards\n\u2705\n\u2705\n\u2705\n\n\nLicense\nLGPL-2.1restrictive\nMIT\nMPL-1.1restrictive\n\n\nNative Python\n\u2705\n\u2705\n\u274c\n\n\nDetect spoken language\n\u274c\n\u2705\nN/A\n\n\nUnicodeDecodeError Safety\n\u274c\n\u2705\n\u274c\n\n\nWhl Size (min)\n193.6 kB\n42 kB\n~200 kB\n\n\nSupported Encoding\n33\n\ud83c\udf89 99\n40\n\n\n\n\n\n** : They are clearly using specific code for a specific encoding even if covering most of used one\nDid you got there because of the logs? See https://charset-normalizer.readthedocs.io/en/latest/user/miscellaneous.html\n\u26a1 Performance\nThis package offer better performance than its counterpart Chardet. Here are some numbers.\n\n\n\nPackage\nAccuracy\nMean per file (ms)\nFile per sec (est)\n\n\n\n\nchardet\n86 %\n200 ms\n5 file/sec\n\n\ncharset-normalizer\n98 %\n10 ms\n100 file/sec\n\n\n\n\n\n\nPackage\n99th percentile\n95th percentile\n50th percentile\n\n\n\n\nchardet\n1200 ms\n287 ms\n23 ms\n\n\ncharset-normalizer\n100 ms\n50 ms\n5 ms\n\n\n\nChardet's performance on larger file (1MB+) are very poor. Expect huge difference on large payload.\n\nStats are generated using 400+ files using default parameters. More details on used files, see GHA workflows.\nAnd yes, these results might change at any time. The dataset can be updated to include more files.\nThe actual delays heavily depends on your CPU capabilities. The factors should remain the same.\nKeep in mind that the stats are generous and that Chardet accuracy vs our is measured using Chardet initial capability\n(eg. Supported Encoding) Challenge-them if you want.\n\n\u2728 Installation\nUsing pip:\npip install charset-normalizer -U\n\ud83d\ude80 Basic Usage\nCLI\nThis package comes with a CLI.\nusage: normalizer [-h] [-v] [-a] [-n] [-m] [-r] [-f] [-t THRESHOLD]\n                  file [file ...]\n\nThe Real First Universal Charset Detector. Discover originating encoding used\non text file. Normalize text to unicode.\n\npositional arguments:\n  files                 File(s) to be analysed\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -v, --verbose         Display complementary information about file if any.\n                        Stdout will contain logs about the detection process.\n  -a, --with-alternative\n                        Output complementary possibilities if any. Top-level\n                        JSON WILL be a list.\n  -n, --normalize       Permit to normalize input file. If not set, program\n                        does not write anything.\n  -m, --minimal         Only output the charset detected to STDOUT. Disabling\n                        JSON output.\n  -r, --replace         Replace file when trying to normalize it instead of\n                        creating a new one.\n  -f, --force           Replace file without asking if you are sure, use this\n                        flag with caution.\n  -t THRESHOLD, --threshold THRESHOLD\n                        Define a custom maximum amount of chaos allowed in\n                        decoded content. 0. <= chaos <= 1.\n  --version             Show version information and exit.\n\nnormalizer ./data/sample.1.fr.srt\nor\npython -m charset_normalizer ./data/sample.1.fr.srt\n\ud83c\udf89 Since version 1.4.0 the CLI produce easily usable stdout result in JSON format.\n{\n    \"path\": \"/home/default/projects/charset_normalizer/data/sample.1.fr.srt\",\n    \"encoding\": \"cp1252\",\n    \"encoding_aliases\": [\n        \"1252\",\n        \"windows_1252\"\n    ],\n    \"alternative_encodings\": [\n        \"cp1254\",\n        \"cp1256\",\n        \"cp1258\",\n        \"iso8859_14\",\n        \"iso8859_15\",\n        \"iso8859_16\",\n        \"iso8859_3\",\n        \"iso8859_9\",\n        \"latin_1\",\n        \"mbcs\"\n    ],\n    \"language\": \"French\",\n    \"alphabets\": [\n        \"Basic Latin\",\n        \"Latin-1 Supplement\"\n    ],\n    \"has_sig_or_bom\": false,\n    \"chaos\": 0.149,\n    \"coherence\": 97.152,\n    \"unicode_path\": null,\n    \"is_preferred\": true\n}\nPython\nJust print out normalized text\nfrom charset_normalizer import from_path\n\nresults = from_path('./my_subtitle.srt')\n\nprint(str(results.best()))\nUpgrade your code without effort\nfrom charset_normalizer import detect\nThe above code will behave the same as chardet. We ensure that we offer the best (reasonable) BC result possible.\nSee the docs for advanced usage : readthedocs.io\n\ud83d\ude07 Why\nWhen I started using Chardet, I noticed that it was not suited to my expectations, and I wanted to propose a\nreliable alternative using a completely different method. Also! I never back down on a good challenge!\nI don't care about the originating charset encoding, because two different tables can\nproduce two identical rendered string.\nWhat I want is to get readable text, the best I can.\nIn a way, I'm brute forcing text decoding. How cool is that ? \ud83d\ude0e\nDon't confuse package ftfy with charset-normalizer or chardet. ftfy goal is to repair unicode string whereas charset-normalizer to convert raw file in unknown encoding to unicode.\n\ud83c\udf70 How\n\nDiscard all charset encoding table that could not fit the binary content.\nMeasure noise, or the mess once opened (by chunks) with a corresponding charset encoding.\nExtract matches with the lowest mess detected.\nAdditionally, we measure coherence / probe for a language.\n\nWait a minute, what is noise/mess and coherence according to YOU ?\nNoise : I opened hundred of text files, written by humans, with the wrong encoding table. I observed, then\nI established some ground rules about what is obvious when it seems like a mess.\nI know that my interpretation of what is noise is probably incomplete, feel free to contribute in order to\nimprove or rewrite it.\nCoherence : For each language there is on earth, we have computed ranked letter appearance occurrences (the best we can). So I thought\nthat intel is worth something here. So I use those records against decoded text to check if I can detect intelligent design.\n\u26a1 Known limitations\n\nLanguage detection is unreliable when text contains two or more languages sharing identical letters. (eg. HTML (english tags) + Turkish content (Sharing Latin characters))\nEvery charset detector heavily depends on sufficient content. In common cases, do not bother run detection on very tiny content.\n\n\u26a0\ufe0f About Python EOLs\nIf you are running:\n\nPython >=2.7,<3.5: Unsupported\nPython 3.5: charset-normalizer < 2.1\nPython 3.6: charset-normalizer < 3.1\nPython 3.7: charset-normalizer < 4.0\n\nUpgrade your Python interpreter as soon as possible.\n\ud83d\udc64 Contributing\nContributions, issues and feature requests are very much welcome.\nFeel free to check issues page if you want to contribute.\n\ud83d\udcdd License\nCopyright \u00a9 Ahmed TAHRI @Ousret.\nThis project is MIT licensed.\nCharacters frequencies used in this project \u00a9 2012 Denny Vrande\u010di\u0107\n\ud83d\udcbc For Enterprise\nProfessional support for charset-normalizer is available as part of the Tidelift\nSubscription. Tidelift gives software development teams a single source for\npurchasing and maintaining their software, with professional grade assurances\nfrom the experts who know it best, while seamlessly integrating with existing\ntools.\n\n\n"}, {"name": "camelot-py", "readme": "\n\n\n\nCamelot: PDF Table Extraction for Humans\n \n\n   \n\nCamelot is a Python library that can help you extract tables from PDFs!\nNote: You can also check out Excalibur, the web interface to Camelot!\n\nHere's how you can extract tables from PDFs. You can check out the PDF used in this example here.\n>>> import camelot\n>>> tables = camelot.read_pdf('foo.pdf')\n>>> tables\n<TableList n=1>\n>>> tables.export('foo.csv', f='csv', compress=True) # json, excel, html, markdown, sqlite\n>>> tables[0]\n<Table shape=(7, 7)>\n>>> tables[0].parsing_report\n{\n    'accuracy': 99.02,\n    'whitespace': 12.24,\n    'order': 1,\n    'page': 1\n}\n>>> tables[0].to_csv('foo.csv') # to_json, to_excel, to_html, to_markdown, to_sqlite\n>>> tables[0].df # get a pandas DataFrame!\n\n\n\n\nCycle Name\nKI (1/km)\nDistance (mi)\nPercent Fuel Savings\n\n\n\n\n\n\n\n\n\n\nImproved Speed\nDecreased Accel\nEliminate Stops\nDecreased Idle\n\n\n2012_2\n3.30\n1.3\n5.9%\n9.5%\n29.2%\n17.4%\n\n\n2145_1\n0.68\n11.2\n2.4%\n0.1%\n9.5%\n2.7%\n\n\n4234_1\n0.59\n58.7\n8.5%\n1.3%\n8.5%\n3.3%\n\n\n2032_2\n0.17\n57.8\n21.7%\n0.3%\n2.7%\n1.2%\n\n\n4171_1\n0.07\n173.9\n58.1%\n1.6%\n2.1%\n0.5%\n\n\n\nCamelot also comes packaged with a command-line interface!\nNote: Camelot only works with text-based PDFs and not scanned documents. (As Tabula explains, \"If you can click and drag to select text in your table in a PDF viewer, then your PDF is text-based\".)\nYou can check out some frequently asked questions here.\nWhy Camelot?\n\nConfigurability: Camelot gives you control over the table extraction process with tweakable settings.\nMetrics: You can discard bad tables based on metrics like accuracy and whitespace, without having to manually look at each table.\nOutput: Each table is extracted into a pandas DataFrame, which seamlessly integrates into ETL and data analysis workflows. You can also export tables to multiple formats, which include CSV, JSON, Excel, HTML, Markdown, and Sqlite.\n\nSee comparison with similar libraries and tools.\nSupport the development\nIf Camelot has helped you, please consider supporting its development with a one-time or monthly donation on OpenCollective.\nInstallation\nUsing conda\nThe easiest way to install Camelot is with conda, which is a package manager and environment management system for the Anaconda distribution.\n$ conda install -c conda-forge camelot-py\n\nUsing pip\nAfter installing the dependencies (tk and ghostscript), you can also just use pip to install Camelot:\n$ pip install \"camelot-py[base]\"\n\nFrom the source code\nAfter installing the dependencies, clone the repo using:\n$ git clone https://www.github.com/camelot-dev/camelot\n\nand install Camelot using pip:\n$ cd camelot\n$ pip install \".[base]\"\n\nDocumentation\nThe documentation is available at http://camelot-py.readthedocs.io/.\nWrappers\n\ncamelot-php provides a PHP wrapper on Camelot.\n\nContributing\nThe Contributor's Guide has detailed information about contributing issues, documentation, code, and tests.\nVersioning\nCamelot uses Semantic Versioning. For the available versions, see the tags on this repository. For the changelog, you can check out HISTORY.md.\nLicense\nThis project is licensed under the MIT License, see the LICENSE file for details.\n"}, {"name": "basemap", "readme": "\nbasemap\nPlot on map projections (with coastlines and political boundaries) using\nmatplotlib.\nThis package depends on the support package basemap-data with the\nbasic basemap data assets, and optionally on the support package\nbasemap-data-hires with high-resolution data assets.\nInstallation\nPrecompiled binary wheels for Windows and GNU/Linux are available in\nPyPI (architectures x86 and x64, Python 2.7 and 3.5+) and can be\ninstalled with pip:\npython -m pip install basemap\n\nIf you need to install from source, please visit the\nGitHub repository for a\nstep-by-step description.\nLicense\nThe library is licensed under the terms of the MIT license (see\nLICENSE). The GEOS dynamic library bundled with the package wheels\nis provided under the terms of the LGPLv2.1 license as given in\nLICENSE.geos.\n"}, {"name": "basemap-data", "readme": "\nbasemap-data\nPlot on map projections (with coastlines and political boundaries) using\nmatplotlib.\nThis is a support package for basemap with the basic data assets\nrequired by basemap to work.\nInstallation\nThe package is available in PyPI and can be installed with pip:\npython -m pip install basemap-data\n\nLicense\nThe land-sea mask, coastline, lake, river and political boundary data\nare extracted from the GSHHG datasets (version 2.3.6) using GMT\n(5.x series) and are included under the terms of the LGPLv3+ license\n(see COPYING and COPYING.LESSER).\nThe other files are included under the terms of the MIT license. See\nLICENSE.epsg for the EPSG file (taken from the PROJ.4 package) and\nLICENSE.mit for the rest.\n"}, {"name": "backports.zoneinfo", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nbackports.zoneinfo: Backport of the standard library module zoneinfo\nInstallation and depending on this library\nUse\nContributing\n\n\n\n\n\nREADME.md\n\n\n\n\nbackports.zoneinfo: Backport of the standard library module zoneinfo\nThis package was originally the reference implementation for PEP 615, which proposes support for the IANA time zone database in the standard library, and now serves as a backport to Python 3.6+ (including PyPy).\nThis exposes the backports.zoneinfo module, which is a backport of the zoneinfo module. The backport's documentation can be found on readthedocs.\nThe module uses the system time zone data if available, and falls back to the tzdata package (available on PyPI) if installed.\nInstallation and depending on this library\nThis module is called backports.zoneinfo on PyPI. To install it in your local environment, use:\npip install backports.zoneinfo\n\nOr (particularly on Windows), you can also use the tzdata extra (which basically just declares a dependency on tzdata, so this doesn't actually save you any typing \ud83d\ude05):\npip install backports.zoneinfo[tzdata]\n\nIf you want to use this in your application, it is best to use PEP 508 environment markers to declare a dependency conditional on the Python version:\nbackports.zoneinfo;python_version<\"3.9\"\n\nSupport for backports.zoneinfo in Python 3.9+ is currently minimal, since it is expected that you would use the standard library zoneinfo module instead.\nUse\nThe backports.zoneinfo module should be a drop-in replacement for the Python 3.9 standard library module zoneinfo. If you do not support anything earlier than Python 3.9, you do not need this library; if you are supporting Python 3.6+, you may want to use this idiom to \"fall back\" to backports.zoneinfo:\ntry:\n    import zoneinfo\nexcept ImportError:\n    from backports import zoneinfo\nTo get access to time zones with this module, construct a ZoneInfo object and attach it to your datetime:\n>>> from backports.zoneinfo import ZoneInfo\n>>> from datetime import datetime, timedelta, timezone\n>>> dt = datetime(1992, 3, 1, tzinfo=ZoneInfo(\"Europe/Minsk\"))\n>>> print(dt)\n1992-03-01 00:00:00+02:00\n>>> print(dt.utcoffset())\n2:00:00\n>>> print(dt.tzname())\nEET\nArithmetic works as expected without the need for a \"normalization\" step:\n>>> dt += timedelta(days=90)\n>>> print(dt)\n1992-05-30 00:00:00+03:00\n>>> dt.utcoffset()\ndatetime.timedelta(seconds=10800)\n>>> dt.tzname()\n'EEST'\nAmbiguous and imaginary times are handled using the fold attribute added in PEP 495:\n>>> dt = datetime(2020, 11, 1, 1, tzinfo=ZoneInfo(\"America/Chicago\"))\n>>> print(dt)\n2020-11-01 01:00:00-05:00\n>>> print(dt.replace(fold=1))\n2020-11-01 01:00:00-06:00\n\n>>> UTC = timezone.utc\n>>> print(dt.astimezone(UTC))\n2020-11-01 06:00:00+00:00\n>>> print(dt.replace(fold=1).astimezone(UTC))\n2020-11-01 07:00:00+00:00\nContributing\nCurrently we are not accepting contributions to this repository because we have not put the CLA in place and we would like to avoid complicating the process of adoption into the standard library. Contributions to CPython will eventually be backported to this repository \u2014 see the Python developer's guide for more information on how to contribute to CPython.\n\n\n"}, {"name": "async-timeout", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nasync-timeout\nUsage example\nInstallation\nAuthors and License\n\n\n\n\n\nREADME.rst\n\n\n\n\nasync-timeout\n\n\n\n\n\n\n\nasyncio-compatible timeout context manager.\n\nUsage example\nThe context manager is useful in cases when you want to apply timeout\nlogic around block of code or in cases when asyncio.wait_for() is\nnot suitable. Also it's much faster than asyncio.wait_for()\nbecause timeout doesn't create a new task.\nThe timeout(delay, *, loop=None) call returns a context manager\nthat cancels a block on timeout expiring:\nfrom async_timeout import timeout\nasync with timeout(1.5):\n    await inner()\n\n\nIf inner() is executed faster than in 1.5 seconds nothing\nhappens.\nOtherwise inner() is cancelled internally by sending\nasyncio.CancelledError into but asyncio.TimeoutError is\nraised outside of context manager scope.\n\ntimeout parameter could be None for skipping timeout functionality.\nAlternatively, timeout_at(when) can be used for scheduling\nat the absolute time:\nloop = asyncio.get_event_loop()\nnow = loop.time()\n\nasync with timeout_at(now + 1.5):\n    await inner()\n\nPlease note: it is not POSIX time but a time with\nundefined starting base, e.g. the time of the system power on.\nContext manager has .expired property for check if timeout happens\nexactly in context manager:\nasync with timeout(1.5) as cm:\n    await inner()\nprint(cm.expired)\n\nThe property is True if inner() execution is cancelled by\ntimeout context manager.\nIf inner() call explicitly raises TimeoutError cm.expired\nis False.\nThe scheduled deadline time is available as .deadline property:\nasync with timeout(1.5) as cm:\n    cm.deadline\n\nNot finished yet timeout can be rescheduled by shift_by()\nor shift_to() methods:\nasync with timeout(1.5) as cm:\n    cm.shift(1)  # add another second on waiting\n    cm.update(loop.time() + 5)  # reschedule to now+5 seconds\n\nRescheduling is forbidden if the timeout is expired or after exit from async with\ncode block.\n\nInstallation\n$ pip install async-timeout\n\nThe library is Python 3 only!\n\nAuthors and License\nThe module is written by Andrew Svetlov.\nIt's Apache 2 licensed and freely available.\n\n\n"}, {"name": "argon2-cffi", "readme": "\nargon2-cffi: Argon2 for Python\nArgon2 won the Password Hashing Competition and argon2-cffi is the simplest way to use it in Python:\n>>> from argon2 import PasswordHasher\n>>> ph = PasswordHasher()\n>>> hash = ph.hash(\"correct horse battery staple\")\n>>> hash  # doctest: +SKIP\n'$argon2id$v=19$m=65536,t=3,p=4$MIIRqgvgQbgj220jfp0MPA$YfwJSVjtjSU0zzV/P3S9nnQ/USre2wvJMjfCIjrTQbg'\n>>> ph.verify(hash, \"correct horse battery staple\")\nTrue\n>>> ph.check_needs_rehash(hash)\nFalse\n>>> ph.verify(hash, \"Tr0ub4dor&3\")\nTraceback (most recent call last):\n  ...\nargon2.exceptions.VerifyMismatchError: The password does not match the supplied hash\n\nProject Links\n\nPyPI\nGitHub\nDocumentation\nChangelog\nFunding\nThe low-level Argon2 CFFI bindings are maintained in the separate argon2-cffi-bindings project.\n\nRelease Information\nRemoved\n\nPython 3.6 is not supported anymore.\n\nDeprecated\n\n\nThe InvalidHash exception is deprecated in favor of InvalidHashError.\nNo plans for removal currently exist and the names can (but shouldn't) be used interchangeably.\n\n\nargon2.hash_password(), argon2.hash_password_raw(), and argon2.verify_password() that have been soft-deprecated since 2016 are now hard-deprecated.\nThey now raise DeprecationWarnings and will be removed in 2024.\n\n\nAdded\n\n\nOfficial support for Python 3.11 and 3.12.\nNo code changes were necessary.\n\n\nargon2.exceptions.InvalidHashError as a replacement for InvalidHash.\n\n\nsalt parameter to argon2.PasswordHasher.hash() to allow for custom salts.\nThis is only useful for specialized use-cases -- leave it on None unless you know exactly what you are doing.\n#153\n\n\n\n\u2192 Full Changelog\nCredits\nargon2-cffi is maintained by Hynek Schlawack.\nThe development is kindly supported by my employer Variomedia AG, argon2-cffi Tidelift subscribers, and my amazing GitHub Sponsors.\nargon2-cffi for Enterprise\nAvailable as part of the Tidelift Subscription.\nThe maintainers of argon2-cffi and thousands of other packages are working with Tidelift to deliver commercial support and maintenance for the open-source packages you use to build your applications.\nSave time, reduce risk, and improve code health, while paying the maintainers of the exact packages you use.\nLearn more.\n"}, {"name": "argon2-cffi-bindings", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLow-level Python CFFI Bindings for Argon2\nUsage\nDisabling Vendored Code\nOverriding Automatic SSE2 Detection\nPython API\nProject Information\nCredits & License\nVendored Code\nargon2-cffi-bindings for Enterprise\n\n\n\n\n\nREADME.md\n\n\n\n\nLow-level Python CFFI Bindings for Argon2\n\n\n\nargon2-cffi-bindings provides low-level CFFI bindings to the official implementation of the Argon2 password hashing algorithm.\nThe currently vendored Argon2 commit ID is f57e61e.\nNote\nIf you want to hash passwords in an application, this package is not for you.\nHave a look at argon2-cffi with its high-level abstractions!\nThese bindings have been extracted from argon2-cffi and it remains its main consumer.\nHowever, they may be used by other packages that want to use the Argon2 library without dealing with C-related complexities.\nUsage\nargon2-cffi-bindings is available from PyPI.\nThe provided CFFI bindings are compiled in API mode.\nBest effort is given to provide binary wheels for as many platforms as possible.\nDisabling Vendored Code\nA copy of Argon2 is vendored and used by default, but can be disabled if argon2-cffi-bindings is installed using:\n$ env ARGON2_CFFI_USE_SYSTEM=1 \\\n  python -Im pip install --no-binary=argon2-cffi-bindings argon2-cffi-bindings\nOverriding Automatic SSE2 Detection\nUsually the build process tries to guess whether or not it should use SSE2-optimized code (see _ffi_build.py for details).\nThis can go wrong and is problematic for cross-compiling.\nTherefore you can use the ARGON2_CFFI_USE_SSE2 environment variable to control the process:\n\nIf you set it to 1, argon2-cffi-bindings will build with SSE2 support.\nIf you set it to 0, argon2-cffi-bindings will build without SSE2 support.\nIf you set it to anything else, it will be ignored and argon2-cffi-bindings will try to guess.\n\nHowever, if our heuristics fail you, we would welcome a bug report.\nPython API\nSince this package is intended to be an implementation detail, it uses a private module name to prevent your users from using it by accident.\nTherefore you have to import the symbols from _argon2_cffi_bindings:\nfrom _argon2_cffi_bindings import ffi, lib\nPlease refer to cffi documentation on how to use the ffi and lib objects.\nThe list of symbols that are provided can be found in the _ffi_build.py file.\nProject Information\n\nChangelog\nDocumentation\nPyPI\nSource Code\n\nCredits & License\nargon2-cffi-bindings is written and maintained by Hynek Schlawack.\nIt is released under the MIT license.\nThe development is kindly supported by Variomedia AG.\nThe authors of Argon2 were very helpful to get the library to compile on ancient versions of Visual Studio for ancient versions of Python.\nThe documentation quotes frequently in verbatim from the Argon2 paper to avoid mistakes by rephrasing.\nVendored Code\nThe original Argon2 repo can be found at https://github.com/P-H-C/phc-winner-argon2/.\nExcept for the components listed below, the Argon2 code in this repository is copyright (c) 2015 Daniel Dinu, Dmitry Khovratovich (main authors), Jean-Philippe Aumasson and Samuel Neves, and under CC0 license.\nThe string encoding routines in src/encoding.c are copyright (c) 2015 Thomas Pornin, and under CC0 license.\nThe BLAKE2 code in src/blake2/ is copyright (c) Samuel Neves, 2013-2015, and under CC0 license.\nargon2-cffi-bindings for Enterprise\nAvailable as part of the Tidelift Subscription.\nThe maintainers of argon2-cffi-bindings and thousands of other packages are working with Tidelift to deliver commercial support and maintenance for the open-source packages you use to build your applications.\nSave time, reduce risk, and improve code health, while paying the maintainers of the exact packages you use.\nLearn more.\n\n\n"}, {"name": "analytics-python", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nanalytics-python\n\ud83d\ude80 How to get started\n\ud83e\udd14 Why?\n\ud83d\udc68\u200d\ud83d\udcbb Getting Started\n\ud83d\ude80 Startup Program\nDocumentation\nLicense\n\n\n\n\n\nREADME.md\n\n\n\n\nanalytics-python\n\nanalytics-python is a python client for Segment\n\n\nYou can't fix what you can't measure\n\nAnalytics helps you measure your users, product, and business. It unlocks insights into your app's funnel, core business metrics, and whether you have a product-market fit.\n\ud83d\ude80 How to get started\n\nCollect analytics data from your app(s).\n\nThe top 200 Segment companies collect data from 5+ source types (web, mobile, server, CRM, etc.).\n\n\nSend the data to analytics tools (for example, Google Analytics, Amplitude, Mixpanel).\n\nOver 250+ Segment companies send data to eight categories of destinations such as analytics tools, warehouses, email marketing, and remarketing systems, session recording, and more.\n\n\nExplore your data by creating metrics (for example, new signups, retention cohorts, and revenue generation).\n\nThe best Segment companies use retention cohorts to measure product-market fit. Netflix has 70% paid retention after 12 months, 30% after 7 years.\n\n\n\nSegment collects analytics data and allows you to send it to more than 250 apps (such as Google Analytics, Mixpanel, Optimizely, Facebook Ads, Slack, Sentry) just by flipping a switch. You only need one Segment code snippet, and you can turn integrations on and off at will, with no additional code. Sign up with Segment today.\n\ud83e\udd14 Why?\n\n\nPower all your analytics apps with the same data. Instead of writing code to integrate all of your tools individually, send data to Segment, once.\n\n\nInstall tracking for the last time. We're the last integration you'll ever need to write. You only need to instrument Segment once. Reduce all of your tracking code and advertising tags into a single set of API calls.\n\n\nSend data from anywhere. Send Segment data from any device, and we'll transform and send it on to any tool.\n\n\nQuery your data in SQL. Slice, dice, and analyze your data in detail with Segment SQL. We'll transform and load your customer behavioral data directly from your apps into Amazon Redshift, Google BigQuery, or Postgres. Save weeks of engineering time by not having to invent your data warehouse and ETL pipeline.\nFor example, you can capture data on any app:\nanalytics.track('Order Completed', { price: 99.84 })\nThen, query the resulting data in SQL:\nselect * from app.order_completed\norder by price desc\n\n\n\ud83d\udc68\u200d\ud83d\udcbb Getting Started\nInstall segment-analytics-python using pip:\npip3 install segment-analytics-python\nor you can clone this repo:\ngit clone https://github.com/segmentio/analytics-python.git\n\ncd analytics-python\n\nsudo python3 setup.py install\nNow inside your app, you'll want to set your write_key before making any analytics calls:\nimport segment.analytics as analytics\n\nanalytics.write_key = 'YOUR_WRITE_KEY'\nNote If you need to send data to multiple Segment sources, you can initialize a new Client for each write_key\n\ud83d\ude80 Startup Program\n\n\n\nIf you are part of a new startup  (<$5M raised, <2 years since founding), we just launched a new startup program for you. You can get a Segment Team plan  (up to $25,000 value in Segment credits) for free up to 2 years \u2014 apply here!\nDocumentation\nDocumentation is available at https://segment.com/libraries/python.\nLicense\nWWWWWW||WWWWWW\n W W W||W W W\n      ||\n    ( OO )__________\n     /  |           \\\n    /o o|    MIT     \\\n    \\___/||_||__||_|| *\n         || ||  || ||\n        _||_|| _||_||\n       (__|__|(__|__|\n\n(The MIT License)\nCopyright (c) 2013 Segment Inc. friends@segment.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the 'Software'), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\n"}, {"name": "absl-py", "readme": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbseil Python Common Libraries\nFeatures\nGetting Started\nInstallation\nRunning Tests\nExample Code\nDocumentation\nFuture Releases\nLicense\n\n\n\n\n\nREADME.md\n\n\n\n\nAbseil Python Common Libraries\nThis repository is a collection of Python library code for building Python\napplications. The code is collected from Google's own Python code base, and has\nbeen extensively tested and used in production.\nFeatures\n\nSimple application startup\nDistributed commandline flags system\nCustom logging module with additional features\nTesting utilities\n\nGetting Started\nInstallation\nTo install the package, simply run:\npip install absl-py\nOr install from source:\npython setup.py install\nRunning Tests\nTo run Abseil tests, you can clone the git repo and run\nbazel:\ngit clone https://github.com/abseil/abseil-py.git\ncd abseil-py\nbazel test absl/...\nExample Code\nPlease refer to\nsmoke_tests/sample_app.py\nas an example to get started.\nDocumentation\nSee the Abseil Python Developer Guide.\nFuture Releases\nThe current repository includes an initial set of libraries for early adoption.\nMore components and interoperability with Abseil C++ Common Libraries\nwill come in future releases.\nLicense\nThe Abseil Python library is licensed under the terms of the Apache\nlicense. See LICENSE for more information.\n\n\n"}, {"name": "unattended-upgrades", "readme": ""}, {"name": "requests-unixsocket", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nrequests-unixsocket\nUsage\nExplicit\nImplicit (monkeypatching)\nAbstract namespace sockets\nSee also\n\n\n\n\n\nREADME.rst\n\n\n\n\nrequests-unixsocket\n\n\n\n\nUse requests to talk HTTP via a UNIX domain socket\n\nUsage\n\nExplicit\nYou can use it by instantiating a special Session object:\nimport json\n\nimport requests_unixsocket\n\nsession = requests_unixsocket.Session()\n\nr = session.get('http+unix://%2Fvar%2Frun%2Fdocker.sock/info')\nregistry_config = r.json()['RegistryConfig']\nprint(json.dumps(registry_config, indent=4))\n\nImplicit (monkeypatching)\nMonkeypatching allows you to use the functionality in this module, while making\nminimal changes to your code. Note that in the above example we had to\ninstantiate a special requests_unixsocket.Session object and call the\nget method on that object. Calling requests.get(url) (the easiest way\nto use requests and probably very common), would not work. But we can make it\nwork by doing monkeypatching.\nYou can monkeypatch globally:\nimport requests_unixsocket\n\nrequests_unixsocket.monkeypatch()\n\nr = requests.get('http+unix://%2Fvar%2Frun%2Fdocker.sock/info')\nassert r.status_code == 200\nor you can do it temporarily using a context manager:\nimport requests_unixsocket\n\nwith requests_unixsocket.monkeypatch():\n    r = requests.get('http+unix://%2Fvar%2Frun%2Fdocker.sock/info')\n    assert r.status_code == 200\n\nAbstract namespace sockets\nTo connect to an abstract namespace\nsocket\n(Linux only), prefix the name with a NULL byte (i.e.: 0) - e.g.:\nimport requests_unixsocket\n\nsession = requests_unixsocket.Session()\nres = session.get('http+unix://\\0test_socket/get')\nprint(res.text)\nFor an example program that illustrates this, see\nexamples/abstract_namespace.py in the git repo. Since abstract namespace\nsockets are specific to Linux, the program will only work on Linux.\n\nSee also\n\nhttps://github.com/httpie/httpie-unixsocket - a plugin for HTTPie that allows you to interact with UNIX domain sockets\n\n\n\n"}, {"name": "python-apt", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nWarehouse\nGetting Started\nDiscussion\nTesting\nCode of Conduct\n\n\n\n\n\nREADME.rst\n\n\n\n\nWarehouse\nWarehouse is the software that powers PyPI.\nSee our development roadmap, documentation, and\narchitectural overview.\n\nGetting Started\nYou can run Warehouse locally in a development environment using\ndocker. See Getting started\ndocumentation for instructions on how to set it up.\nThe canonical deployment of Warehouse is in production at pypi.org.\n\nDiscussion\nYou can find help or get involved on:\n\nGithub issue tracker for reporting issues\nIRC: on Libera, channel #pypa for general packaging discussion\nand user support, and #pypa-dev for\ndiscussions about development of packaging tools\nThe PyPA Discord for live discussions\nThe Packaging category on Discourse for discussing\nnew ideas and community initiatives\n\n\nTesting\nRead the running tests and linters section of our documentation to\nlearn how to test your code.  For cross-browser testing, we use an\nopen source account from BrowserStack. If your pull request makes\nany change to the user interface, it will need to be tested to confirm\nit works in our supported browsers.\n\n\nCode of Conduct\nEveryone interacting in the Warehouse project's codebases, issue trackers, chat\nrooms, and mailing lists is expected to follow the PSF Code of Conduct.\n\n\n"}, {"name": "distro-info", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nWarehouse\nGetting Started\nDiscussion\nTesting\nCode of Conduct\n\n\n\n\n\nREADME.rst\n\n\n\n\nWarehouse\nWarehouse is the software that powers PyPI.\nSee our development roadmap, documentation, and\narchitectural overview.\n\nGetting Started\nYou can run Warehouse locally in a development environment using\ndocker. See Getting started\ndocumentation for instructions on how to set it up.\nThe canonical deployment of Warehouse is in production at pypi.org.\n\nDiscussion\nYou can find help or get involved on:\n\nGithub issue tracker for reporting issues\nIRC: on Libera, channel #pypa for general packaging discussion\nand user support, and #pypa-dev for\ndiscussions about development of packaging tools\nThe PyPA Discord for live discussions\nThe Packaging category on Discourse for discussing\nnew ideas and community initiatives\n\n\nTesting\nRead the running tests and linters section of our documentation to\nlearn how to test your code.  For cross-browser testing, we use an\nopen source account from BrowserStack. If your pull request makes\nany change to the user interface, it will need to be tested to confirm\nit works in our supported browsers.\n\n\nCode of Conduct\nEveryone interacting in the Warehouse project's codebases, issue trackers, chat\nrooms, and mailing lists is expected to follow the PSF Code of Conduct.\n\n\n"}, {"name": "dbus-python", "readme": "\n\n\n\n\n\n\n\n\n\n\n\nWarehouse\nGetting Started\nDiscussion\nTesting\nCode of Conduct\n\n\n\n\n\nREADME.rst\n\n\n\n\nWarehouse\nWarehouse is the software that powers PyPI.\nSee our development roadmap, documentation, and\narchitectural overview.\n\nGetting Started\nYou can run Warehouse locally in a development environment using\ndocker. See Getting started\ndocumentation for instructions on how to set it up.\nThe canonical deployment of Warehouse is in production at pypi.org.\n\nDiscussion\nYou can find help or get involved on:\n\nGithub issue tracker for reporting issues\nIRC: on Libera, channel #pypa for general packaging discussion\nand user support, and #pypa-dev for\ndiscussions about development of packaging tools\nThe PyPA Discord for live discussions\nThe Packaging category on Discourse for discussing\nnew ideas and community initiatives\n\n\nTesting\nRead the running tests and linters section of our documentation to\nlearn how to test your code.  For cross-browser testing, we use an\nopen source account from BrowserStack. If your pull request makes\nany change to the user interface, it will need to be tested to confirm\nit works in our supported browsers.\n\n\nCode of Conduct\nEveryone interacting in the Warehouse project's codebases, issue trackers, chat\nrooms, and mailing lists is expected to follow the PSF Code of Conduct.\n\n\n"}]